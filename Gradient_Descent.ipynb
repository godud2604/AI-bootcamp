{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/godud2604/AI-bootcamp/blob/main/Gradient_Descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "9OoMnObbV3To"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n",
        "* 사이킷런에서 보스턴 주택 가격 데이터 세트를 로드하고 이를 DataFrame으로 생성"
      ],
      "metadata": {
        "id": "vQDcOKxWV3Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "print(sklearn.__version__)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-09T02:29:52.921517Z",
          "iopub.execute_input": "2023-10-09T02:29:52.922025Z",
          "iopub.status.idle": "2023-10-09T02:29:53.286264Z",
          "shell.execute_reply.started": "2023-10-09T02:29:52.921991Z",
          "shell.execute_reply": "2023-10-09T02:29:53.285335Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k4YxDidV3Tt",
        "outputId": "64ed0da8-079c-4f43-ebc1-68190194ef4e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn==1.0.2"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYVklKCYV3Tv",
        "outputId": "ee8f72ec-601d-4069-b773-5f06cdd5a551"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.0.2 in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.11.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.0.2) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "bostonDF['PRICE'] = boston.target\n",
        "print(bostonDF.shape)\n",
        "bostonDF.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-09T02:30:01.222794Z",
          "iopub.execute_input": "2023-10-09T02:30:01.223326Z",
          "iopub.status.idle": "2023-10-09T02:30:01.507044Z",
          "shell.execute_reply.started": "2023-10-09T02:30:01.223293Z",
          "shell.execute_reply": "2023-10-09T02:30:01.505331Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "t8vMsu97V3Ty",
        "outputId": "a1901232-b058-43fb-bd44-45eb873ace0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(506, 14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
              "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
              "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
              "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
              "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
              "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  PRICE  \n",
              "0     15.3  396.90   4.98   24.0  \n",
              "1     17.8  396.90   9.14   21.6  \n",
              "2     17.8  392.83   4.03   34.7  \n",
              "3     18.7  394.63   2.94   33.4  \n",
              "4     18.7  396.90   5.33   36.2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d454acae-56c6-495d-a687-d2cfbf8c8ade\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d454acae-56c6-495d-a687-d2cfbf8c8ade')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d454acae-56c6-495d-a687-d2cfbf8c8ade button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d454acae-56c6-495d-a687-d2cfbf8c8ade');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-266847f9-bb0a-4b4a-97fc-d7fff2c28d34\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-266847f9-bb0a-4b4a-97fc-d7fff2c28d34')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-266847f9-bb0a-4b4a-97fc-d7fff2c28d34 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n",
        "* w1은 RM(방의 계수) 피처의 Weight 값\n",
        "* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n",
        "* bias는 Bias\n",
        "* N은 입력 데이터 건수\n",
        "![](https://raw.githubusercontent.com/chulminkw/CNN_PG/main/utils/images/Weight_update.png)\n"
      ],
      "metadata": {
        "id": "hwpQjUapV3Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수.\n",
        "# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 array가 다 입력됨.\n",
        "# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n",
        "\n",
        "def get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n",
        "  # 데이터 건수\n",
        "  N = len(target)\n",
        "\n",
        "  # 예측 값\n",
        "  predicted = w1 * rm + w2 * lstat + bias\n",
        "\n",
        "  # 실제값과 예측값의 차이\n",
        "  diff = target - predicted\n",
        "\n",
        "  # bias를 array 기반으로 구하기 위해서 설정. (1을 N개만큼 생성)\n",
        "  bias_factors = np.ones((N,))\n",
        "\n",
        "  # weight와 bias를 얼마나 update할 것인지를 계산.\n",
        "  w1_update = -(2/N)*learning_rate*(np.dot(rm.T, diff))\n",
        "  w2_update = -(2/N)*learning_rate*(np.dot(lstat.T, diff))\n",
        "  bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff))\n",
        "\n",
        "  # Mean Squared Error값을 계산.\n",
        "  mse_loss = np.mean(np.square(diff))\n",
        "\n",
        "  # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환.\n",
        "  return bias_update, w1_update, w2_update, mse_loss"
      ],
      "metadata": {
        "trusted": true,
        "id": "v9FRC44qV3Tz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 를 적용하는 함수 생성\n",
        "* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용."
      ],
      "metadata": {
        "id": "umm-4rovV3T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용.\n",
        "def gradient_descent(features, target, iter_epochs=1000, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정.\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "\n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0] # 첫 번째 컬럼\n",
        "    lstat = features[:, 1] # 두 번째 컬럼\n",
        "\n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행.\n",
        "    for i in range(iter_epochs):\n",
        "        # weight/bias update 값 계산\n",
        "        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate)\n",
        "        # weight/bias의 update 적용.\n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if verbose:\n",
        "            print('Epoch:', i+1,'/', iter_epochs)\n",
        "            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', loss)\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "trusted": true,
        "id": "qFhudP8xV3T1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 적용\n",
        "* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함.\n",
        "* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용."
      ],
      "metadata": {
        "id": "_R2HCxaCV3T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n",
        "\n",
        "print(scaled_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70b0WSlzdO5-",
        "outputId": "e6cc1f90-066d-4f0a-9744-c9968e668bb6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.57750527 0.08967991]\n",
            " [0.5479977  0.2044702 ]\n",
            " [0.6943859  0.06346578]\n",
            " ...\n",
            " [0.65433991 0.10789183]\n",
            " [0.61946733 0.13107064]\n",
            " [0.47307913 0.16970199]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n",
        "\n",
        "w1, w2, bias = gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDli2MNmV3T2",
        "outputId": "cfb33cb5-22ec-41bc-8f69-a8d3f2f5f945"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "Epoch: 2502 / 5000\n",
            "w1: [23.9249342] w2: [-21.23815095] bias: [16.45256258] loss: 30.979973242429722\n",
            "Epoch: 2503 / 5000\n",
            "w1: [23.92651912] w2: [-21.24062335] bias: [16.45248123] loss: 30.979108879218803\n",
            "Epoch: 2504 / 5000\n",
            "w1: [23.92810265] w2: [-21.24309331] bias: [16.45239987] loss: 30.97824615628058\n",
            "Epoch: 2505 / 5000\n",
            "w1: [23.92968481] w2: [-21.24556086] bias: [16.45231849] loss: 30.97738507049069\n",
            "Epoch: 2506 / 5000\n",
            "w1: [23.93126558] w2: [-21.24802598] bias: [16.45223711] loss: 30.976525618730737\n",
            "Epoch: 2507 / 5000\n",
            "w1: [23.93284497] w2: [-21.25048868] bias: [16.45215571] loss: 30.975667797888264\n",
            "Epoch: 2508 / 5000\n",
            "w1: [23.93442299] w2: [-21.25294896] bias: [16.4520743] loss: 30.974811604856743\n",
            "Epoch: 2509 / 5000\n",
            "w1: [23.93599963] w2: [-21.25540682] bias: [16.45199288] loss: 30.973957036535577\n",
            "Epoch: 2510 / 5000\n",
            "w1: [23.93757489] w2: [-21.25786227] bias: [16.45191145] loss: 30.973104089830052\n",
            "Epoch: 2511 / 5000\n",
            "w1: [23.93914878] w2: [-21.26031531] bias: [16.45183001] loss: 30.972252761651383\n",
            "Epoch: 2512 / 5000\n",
            "w1: [23.9407213] w2: [-21.26276594] bias: [16.45174855] loss: 30.97140304891665\n",
            "Epoch: 2513 / 5000\n",
            "w1: [23.94229245] w2: [-21.26521416] bias: [16.45166709] loss: 30.97055494854882\n",
            "Epoch: 2514 / 5000\n",
            "w1: [23.94386222] w2: [-21.26765998] bias: [16.45158561] loss: 30.96970845747671\n",
            "Epoch: 2515 / 5000\n",
            "w1: [23.94543063] w2: [-21.2701034] bias: [16.45150413] loss: 30.968863572635023\n",
            "Epoch: 2516 / 5000\n",
            "w1: [23.94699767] w2: [-21.27254441] bias: [16.45142263] loss: 30.968020290964258\n",
            "Epoch: 2517 / 5000\n",
            "w1: [23.94856335] w2: [-21.27498303] bias: [16.45134112] loss: 30.967178609410784\n",
            "Epoch: 2518 / 5000\n",
            "w1: [23.95012766] w2: [-21.27741925] bias: [16.4512596] loss: 30.96633852492676\n",
            "Epoch: 2519 / 5000\n",
            "w1: [23.9516906] w2: [-21.27985308] bias: [16.45117807] loss: 30.965500034470164\n",
            "Epoch: 2520 / 5000\n",
            "w1: [23.95325219] w2: [-21.28228451] bias: [16.45109653] loss: 30.96466313500479\n",
            "Epoch: 2521 / 5000\n",
            "w1: [23.95481241] w2: [-21.28471356] bias: [16.45101498] loss: 30.963827823500203\n",
            "Epoch: 2522 / 5000\n",
            "w1: [23.95637127] w2: [-21.28714022] bias: [16.45093342] loss: 30.962994096931716\n",
            "Epoch: 2523 / 5000\n",
            "w1: [23.95792878] w2: [-21.2895645] bias: [16.45085184] loss: 30.962161952280468\n",
            "Epoch: 2524 / 5000\n",
            "w1: [23.95948493] w2: [-21.2919864] bias: [16.45077026] loss: 30.961331386533296\n",
            "Epoch: 2525 / 5000\n",
            "w1: [23.96103972] w2: [-21.29440591] bias: [16.45068866] loss: 30.960502396682802\n",
            "Epoch: 2526 / 5000\n",
            "w1: [23.96259316] w2: [-21.29682305] bias: [16.45060706] loss: 30.959674979727318\n",
            "Epoch: 2527 / 5000\n",
            "w1: [23.96414525] w2: [-21.29923782] bias: [16.45052544] loss: 30.9588491326709\n",
            "Epoch: 2528 / 5000\n",
            "w1: [23.96569598] w2: [-21.30165021] bias: [16.45044381] loss: 30.9580248525233\n",
            "Epoch: 2529 / 5000\n",
            "w1: [23.96724536] w2: [-21.30406023] bias: [16.45036218] loss: 30.957202136299994\n",
            "Epoch: 2530 / 5000\n",
            "w1: [23.96879339] w2: [-21.30646788] bias: [16.45028053] loss: 30.956380981022114\n",
            "Epoch: 2531 / 5000\n",
            "w1: [23.97034008] w2: [-21.30887317] bias: [16.45019887] loss: 30.9555613837165\n",
            "Epoch: 2532 / 5000\n",
            "w1: [23.97188542] w2: [-21.31127609] bias: [16.4501172] loss: 30.954743341415625\n",
            "Epoch: 2533 / 5000\n",
            "w1: [23.97342941] w2: [-21.31367665] bias: [16.45003552] loss: 30.953926851157654\n",
            "Epoch: 2534 / 5000\n",
            "w1: [23.97497206] w2: [-21.31607486] bias: [16.44995382] loss: 30.953111909986365\n",
            "Epoch: 2535 / 5000\n",
            "w1: [23.97651336] w2: [-21.3184707] bias: [16.44987212] loss: 30.952298514951195\n",
            "Epoch: 2536 / 5000\n",
            "w1: [23.97805333] w2: [-21.32086419] bias: [16.44979041] loss: 30.951486663107175\n",
            "Epoch: 2537 / 5000\n",
            "w1: [23.97959195] w2: [-21.32325533] bias: [16.44970869] loss: 30.95067635151499\n",
            "Epoch: 2538 / 5000\n",
            "w1: [23.98112923] w2: [-21.32564412] bias: [16.44962695] loss: 30.94986757724088\n",
            "Epoch: 2539 / 5000\n",
            "w1: [23.98266518] w2: [-21.32803056] bias: [16.44954521] loss: 30.949060337356713\n",
            "Epoch: 2540 / 5000\n",
            "w1: [23.98419979] w2: [-21.33041465] bias: [16.44946345] loss: 30.948254628939914\n",
            "Epoch: 2541 / 5000\n",
            "w1: [23.98573306] w2: [-21.33279641] bias: [16.44938169] loss: 30.947450449073493\n",
            "Epoch: 2542 / 5000\n",
            "w1: [23.987265] w2: [-21.33517582] bias: [16.44929991] loss: 30.946647794846005\n",
            "Epoch: 2543 / 5000\n",
            "w1: [23.98879561] w2: [-21.33755289] bias: [16.44921813] loss: 30.945846663351567\n",
            "Epoch: 2544 / 5000\n",
            "w1: [23.99032488] w2: [-21.33992762] bias: [16.44913633] loss: 30.945047051689833\n",
            "Epoch: 2545 / 5000\n",
            "w1: [23.99185282] w2: [-21.34230002] bias: [16.44905453] loss: 30.944248956965968\n",
            "Epoch: 2546 / 5000\n",
            "w1: [23.99337944] w2: [-21.34467009] bias: [16.44897271] loss: 30.943452376290676\n",
            "Epoch: 2547 / 5000\n",
            "w1: [23.99490472] w2: [-21.34703783] bias: [16.44889088] loss: 30.94265730678015\n",
            "Epoch: 2548 / 5000\n",
            "w1: [23.99642868] w2: [-21.34940324] bias: [16.44880904] loss: 30.941863745556095\n",
            "Epoch: 2549 / 5000\n",
            "w1: [23.99795132] w2: [-21.35176633] bias: [16.4487272] loss: 30.94107168974568\n",
            "Epoch: 2550 / 5000\n",
            "w1: [23.99947263] w2: [-21.35412709] bias: [16.44864534] loss: 30.940281136481566\n",
            "Epoch: 2551 / 5000\n",
            "w1: [24.00099262] w2: [-21.35648553] bias: [16.44856347] loss: 30.93949208290187\n",
            "Epoch: 2552 / 5000\n",
            "w1: [24.00251128] w2: [-21.35884165] bias: [16.44848159] loss: 30.938704526150186\n",
            "Epoch: 2553 / 5000\n",
            "w1: [24.00402863] w2: [-21.36119546] bias: [16.4483997] loss: 30.937918463375514\n",
            "Epoch: 2554 / 5000\n",
            "w1: [24.00554465] w2: [-21.36354695] bias: [16.4483178] loss: 30.937133891732305\n",
            "Epoch: 2555 / 5000\n",
            "w1: [24.00705936] w2: [-21.36589613] bias: [16.44823589] loss: 30.936350808380435\n",
            "Epoch: 2556 / 5000\n",
            "w1: [24.00857275] w2: [-21.36824299] bias: [16.44815397] loss: 30.93556921048521\n",
            "Epoch: 2557 / 5000\n",
            "w1: [24.01008482] w2: [-21.37058756] bias: [16.44807205] loss: 30.934789095217287\n",
            "Epoch: 2558 / 5000\n",
            "w1: [24.01159559] w2: [-21.37292981] bias: [16.44799011] loss: 30.934010459752777\n",
            "Epoch: 2559 / 5000\n",
            "w1: [24.01310503] w2: [-21.37526976] bias: [16.44790816] loss: 30.93323330127312\n",
            "Epoch: 2560 / 5000\n",
            "w1: [24.01461317] w2: [-21.37760741] bias: [16.4478262] loss: 30.932457616965173\n",
            "Epoch: 2561 / 5000\n",
            "w1: [24.01611999] w2: [-21.37994277] bias: [16.44774423] loss: 30.93168340402112\n",
            "Epoch: 2562 / 5000\n",
            "w1: [24.01762551] w2: [-21.38227582] bias: [16.44766225] loss: 30.930910659638506\n",
            "Epoch: 2563 / 5000\n",
            "w1: [24.01912972] w2: [-21.38460658] bias: [16.44758026] loss: 30.930139381020222\n",
            "Epoch: 2564 / 5000\n",
            "w1: [24.02063262] w2: [-21.38693505] bias: [16.44749826] loss: 30.92936956537449\n",
            "Epoch: 2565 / 5000\n",
            "w1: [24.02213421] w2: [-21.38926123] bias: [16.44741625] loss: 30.928601209914852\n",
            "Epoch: 2566 / 5000\n",
            "w1: [24.0236345] w2: [-21.39158512] bias: [16.44733423] loss: 30.92783431186015\n",
            "Epoch: 2567 / 5000\n",
            "w1: [24.02513349] w2: [-21.39390673] bias: [16.4472522] loss: 30.927068868434528\n",
            "Epoch: 2568 / 5000\n",
            "w1: [24.02663118] w2: [-21.39622605] bias: [16.44717016] loss: 30.926304876867437\n",
            "Epoch: 2569 / 5000\n",
            "w1: [24.02812756] w2: [-21.39854309] bias: [16.44708811] loss: 30.925542334393594\n",
            "Epoch: 2570 / 5000\n",
            "w1: [24.02962265] w2: [-21.40085785] bias: [16.44700606] loss: 30.92478123825299\n",
            "Epoch: 2571 / 5000\n",
            "w1: [24.03111643] w2: [-21.40317033] bias: [16.44692399] loss: 30.92402158569087\n",
            "Epoch: 2572 / 5000\n",
            "w1: [24.03260892] w2: [-21.40548054] bias: [16.44684191] loss: 30.923263373957738\n",
            "Epoch: 2573 / 5000\n",
            "w1: [24.03410012] w2: [-21.40778848] bias: [16.44675982] loss: 30.92250660030933\n",
            "Epoch: 2574 / 5000\n",
            "w1: [24.03559002] w2: [-21.41009414] bias: [16.44667772] loss: 30.921751262006616\n",
            "Epoch: 2575 / 5000\n",
            "w1: [24.03707862] w2: [-21.41239754] bias: [16.44659562] loss: 30.920997356315794\n",
            "Epoch: 2576 / 5000\n",
            "w1: [24.03856594] w2: [-21.41469867] bias: [16.4465135] loss: 30.920244880508253\n",
            "Epoch: 2577 / 5000\n",
            "w1: [24.04005196] w2: [-21.41699754] bias: [16.44643137] loss: 30.919493831860596\n",
            "Epoch: 2578 / 5000\n",
            "w1: [24.04153669] w2: [-21.41929415] bias: [16.44634924] loss: 30.918744207654616\n",
            "Epoch: 2579 / 5000\n",
            "w1: [24.04302013] w2: [-21.42158849] bias: [16.44626709] loss: 30.917996005177276\n",
            "Epoch: 2580 / 5000\n",
            "w1: [24.04450229] w2: [-21.42388058] bias: [16.44618493] loss: 30.91724922172072\n",
            "Epoch: 2581 / 5000\n",
            "w1: [24.04598316] w2: [-21.42617042] bias: [16.44610277] loss: 30.916503854582253\n",
            "Epoch: 2582 / 5000\n",
            "w1: [24.04746275] w2: [-21.428458] bias: [16.44602059] loss: 30.91575990106432\n",
            "Epoch: 2583 / 5000\n",
            "w1: [24.04894105] w2: [-21.43074333] bias: [16.44593841] loss: 30.915017358474515\n",
            "Epoch: 2584 / 5000\n",
            "w1: [24.05041807] w2: [-21.43302641] bias: [16.44585621] loss: 30.914276224125558\n",
            "Epoch: 2585 / 5000\n",
            "w1: [24.05189381] w2: [-21.43530724] bias: [16.44577401] loss: 30.913536495335304\n",
            "Epoch: 2586 / 5000\n",
            "w1: [24.05336826] w2: [-21.43758583] bias: [16.4456918] loss: 30.9127981694267\n",
            "Epoch: 2587 / 5000\n",
            "w1: [24.05484144] w2: [-21.43986218] bias: [16.44560958] loss: 30.912061243727816\n",
            "Epoch: 2588 / 5000\n",
            "w1: [24.05631334] w2: [-21.44213628] bias: [16.44552734] loss: 30.911325715571785\n",
            "Epoch: 2589 / 5000\n",
            "w1: [24.05778397] w2: [-21.44440815] bias: [16.4454451] loss: 30.910591582296856\n",
            "Epoch: 2590 / 5000\n",
            "w1: [24.05925332] w2: [-21.44667779] bias: [16.44536285] loss: 30.909858841246322\n",
            "Epoch: 2591 / 5000\n",
            "w1: [24.06072139] w2: [-21.44894518] bias: [16.44528059] loss: 30.909127489768558\n",
            "Epoch: 2592 / 5000\n",
            "w1: [24.06218819] w2: [-21.45121035] bias: [16.44519832] loss: 30.90839752521698\n",
            "Epoch: 2593 / 5000\n",
            "w1: [24.06365372] w2: [-21.45347329] bias: [16.44511604] loss: 30.907668944950064\n",
            "Epoch: 2594 / 5000\n",
            "w1: [24.06511798] w2: [-21.455734] bias: [16.44503376] loss: 30.9069417463313\n",
            "Epoch: 2595 / 5000\n",
            "w1: [24.06658097] w2: [-21.45799249] bias: [16.44495146] loss: 30.90621592672922\n",
            "Epoch: 2596 / 5000\n",
            "w1: [24.06804269] w2: [-21.46024875] bias: [16.44486915] loss: 30.90549148351736\n",
            "Epoch: 2597 / 5000\n",
            "w1: [24.06950315] w2: [-21.46250279] bias: [16.44478684] loss: 30.90476841407426\n",
            "Epoch: 2598 / 5000\n",
            "w1: [24.07096234] w2: [-21.46475461] bias: [16.44470451] loss: 30.904046715783462\n",
            "Epoch: 2599 / 5000\n",
            "w1: [24.07242026] w2: [-21.46700422] bias: [16.44462218] loss: 30.90332638603349\n",
            "Epoch: 2600 / 5000\n",
            "w1: [24.07387693] w2: [-21.46925161] bias: [16.44453983] loss: 30.902607422217855\n",
            "Epoch: 2601 / 5000\n",
            "w1: [24.07533232] w2: [-21.47149679] bias: [16.44445748] loss: 30.90188982173503\n",
            "Epoch: 2602 / 5000\n",
            "w1: [24.07678646] w2: [-21.47373976] bias: [16.44437512] loss: 30.90117358198843\n",
            "Epoch: 2603 / 5000\n",
            "w1: [24.07823934] w2: [-21.47598052] bias: [16.44429275] loss: 30.900458700386448\n",
            "Epoch: 2604 / 5000\n",
            "w1: [24.07969096] w2: [-21.47821907] bias: [16.44421037] loss: 30.899745174342385\n",
            "Epoch: 2605 / 5000\n",
            "w1: [24.08114132] w2: [-21.48045542] bias: [16.44412798] loss: 30.899033001274503\n",
            "Epoch: 2606 / 5000\n",
            "w1: [24.08259043] w2: [-21.48268957] bias: [16.44404558] loss: 30.898322178605955\n",
            "Epoch: 2607 / 5000\n",
            "w1: [24.08403828] w2: [-21.48492152] bias: [16.44396317] loss: 30.897612703764832\n",
            "Epoch: 2608 / 5000\n",
            "w1: [24.08548488] w2: [-21.48715127] bias: [16.44388075] loss: 30.896904574184088\n",
            "Epoch: 2609 / 5000\n",
            "w1: [24.08693022] w2: [-21.48937882] bias: [16.44379833] loss: 30.896197787301613\n",
            "Epoch: 2610 / 5000\n",
            "w1: [24.08837431] w2: [-21.49160418] bias: [16.44371589] loss: 30.895492340560157\n",
            "Epoch: 2611 / 5000\n",
            "w1: [24.08981716] w2: [-21.49382735] bias: [16.44363345] loss: 30.894788231407343\n",
            "Epoch: 2612 / 5000\n",
            "w1: [24.09125875] w2: [-21.49604833] bias: [16.443551] loss: 30.894085457295667\n",
            "Epoch: 2613 / 5000\n",
            "w1: [24.09269909] w2: [-21.49826713] bias: [16.44346854] loss: 30.893384015682464\n",
            "Epoch: 2614 / 5000\n",
            "w1: [24.09413819] w2: [-21.50048374] bias: [16.44338607] loss: 30.892683904029937\n",
            "Epoch: 2615 / 5000\n",
            "w1: [24.09557604] w2: [-21.50269816] bias: [16.44330359] loss: 30.891985119805106\n",
            "Epoch: 2616 / 5000\n",
            "w1: [24.09701265] w2: [-21.50491041] bias: [16.4432211] loss: 30.891287660479836\n",
            "Epoch: 2617 / 5000\n",
            "w1: [24.09844802] w2: [-21.50712047] bias: [16.4431386] loss: 30.890591523530794\n",
            "Epoch: 2618 / 5000\n",
            "w1: [24.09988214] w2: [-21.50932836] bias: [16.4430561] loss: 30.889896706439448\n",
            "Epoch: 2619 / 5000\n",
            "w1: [24.10131502] w2: [-21.51153408] bias: [16.44297358] loss: 30.889203206692105\n",
            "Epoch: 2620 / 5000\n",
            "w1: [24.10274666] w2: [-21.51373762] bias: [16.44289106] loss: 30.888511021779824\n",
            "Epoch: 2621 / 5000\n",
            "w1: [24.10417706] w2: [-21.51593899] bias: [16.44280853] loss: 30.887820149198458\n",
            "Epoch: 2622 / 5000\n",
            "w1: [24.10560623] w2: [-21.51813819] bias: [16.44272599] loss: 30.887130586448635\n",
            "Epoch: 2623 / 5000\n",
            "w1: [24.10703416] w2: [-21.52033523] bias: [16.44264344] loss: 30.886442331035745\n",
            "Epoch: 2624 / 5000\n",
            "w1: [24.10846085] w2: [-21.5225301] bias: [16.44256088] loss: 30.885755380469927\n",
            "Epoch: 2625 / 5000\n",
            "w1: [24.10988631] w2: [-21.52472281] bias: [16.44247831] loss: 30.88506973226607\n",
            "Epoch: 2626 / 5000\n",
            "w1: [24.11131054] w2: [-21.52691336] bias: [16.44239574] loss: 30.8843853839438\n",
            "Epoch: 2627 / 5000\n",
            "w1: [24.11273353] w2: [-21.52910175] bias: [16.44231315] loss: 30.88370233302747\n",
            "Epoch: 2628 / 5000\n",
            "w1: [24.11415529] w2: [-21.53128799] bias: [16.44223056] loss: 30.88302057704615\n",
            "Epoch: 2629 / 5000\n",
            "w1: [24.11557583] w2: [-21.53347207] bias: [16.44214796] loss: 30.88234011353361\n",
            "Epoch: 2630 / 5000\n",
            "w1: [24.11699514] w2: [-21.535654] bias: [16.44206535] loss: 30.88166094002833\n",
            "Epoch: 2631 / 5000\n",
            "w1: [24.11841322] w2: [-21.53783378] bias: [16.44198273] loss: 30.880983054073486\n",
            "Epoch: 2632 / 5000\n",
            "w1: [24.11983007] w2: [-21.54001142] bias: [16.4419001] loss: 30.880306453216924\n",
            "Epoch: 2633 / 5000\n",
            "w1: [24.1212457] w2: [-21.5421869] bias: [16.44181747] loss: 30.87963113501117\n",
            "Epoch: 2634 / 5000\n",
            "w1: [24.1226601] w2: [-21.54436025] bias: [16.44173482] loss: 30.87895709701341\n",
            "Epoch: 2635 / 5000\n",
            "w1: [24.12407328] w2: [-21.54653145] bias: [16.44165217] loss: 30.878284336785494\n",
            "Epoch: 2636 / 5000\n",
            "w1: [24.12548525] w2: [-21.54870051] bias: [16.44156951] loss: 30.8776128518939\n",
            "Epoch: 2637 / 5000\n",
            "w1: [24.12689599] w2: [-21.55086744] bias: [16.44148684] loss: 30.87694263990977\n",
            "Epoch: 2638 / 5000\n",
            "w1: [24.12830551] w2: [-21.55303223] bias: [16.44140416] loss: 30.876273698408855\n",
            "Epoch: 2639 / 5000\n",
            "w1: [24.12971381] w2: [-21.55519488] bias: [16.44132147] loss: 30.875606024971532\n",
            "Epoch: 2640 / 5000\n",
            "w1: [24.1311209] w2: [-21.55735541] bias: [16.44123878] loss: 30.874939617182786\n",
            "Epoch: 2641 / 5000\n",
            "w1: [24.13252677] w2: [-21.5595138] bias: [16.44115608] loss: 30.874274472632212\n",
            "Epoch: 2642 / 5000\n",
            "w1: [24.13393143] w2: [-21.56167007] bias: [16.44107336] loss: 30.873610588913998\n",
            "Epoch: 2643 / 5000\n",
            "w1: [24.13533487] w2: [-21.56382421] bias: [16.44099064] loss: 30.872947963626896\n",
            "Epoch: 2644 / 5000\n",
            "w1: [24.1367371] w2: [-21.56597623] bias: [16.44090792] loss: 30.87228659437427\n",
            "Epoch: 2645 / 5000\n",
            "w1: [24.13813812] w2: [-21.56812613] bias: [16.44082518] loss: 30.87162647876402\n",
            "Epoch: 2646 / 5000\n",
            "w1: [24.13953794] w2: [-21.57027391] bias: [16.44074244] loss: 30.870967614408627\n",
            "Epoch: 2647 / 5000\n",
            "w1: [24.14093654] w2: [-21.57241957] bias: [16.44065968] loss: 30.870309998925112\n",
            "Epoch: 2648 / 5000\n",
            "w1: [24.14233393] w2: [-21.57456312] bias: [16.44057692] loss: 30.869653629935023\n",
            "Epoch: 2649 / 5000\n",
            "w1: [24.14373012] w2: [-21.57670456] bias: [16.44049415] loss: 30.868998505064468\n",
            "Epoch: 2650 / 5000\n",
            "w1: [24.1451251] w2: [-21.57884388] bias: [16.44041137] loss: 30.86834462194407\n",
            "Epoch: 2651 / 5000\n",
            "w1: [24.14651888] w2: [-21.58098109] bias: [16.44032859] loss: 30.867691978208953\n",
            "Epoch: 2652 / 5000\n",
            "w1: [24.14791145] w2: [-21.5831162] bias: [16.44024579] loss: 30.867040571498762\n",
            "Epoch: 2653 / 5000\n",
            "w1: [24.14930283] w2: [-21.58524921] bias: [16.44016299] loss: 30.866390399457643\n",
            "Epoch: 2654 / 5000\n",
            "w1: [24.150693] w2: [-21.5873801] bias: [16.44008018] loss: 30.86574145973421\n",
            "Epoch: 2655 / 5000\n",
            "w1: [24.15208197] w2: [-21.5895089] bias: [16.43999736] loss: 30.865093749981586\n",
            "Epoch: 2656 / 5000\n",
            "w1: [24.15346974] w2: [-21.5916356] bias: [16.43991454] loss: 30.864447267857354\n",
            "Epoch: 2657 / 5000\n",
            "w1: [24.15485632] w2: [-21.59376021] bias: [16.4398317] loss: 30.86380201102356\n",
            "Epoch: 2658 / 5000\n",
            "w1: [24.1562417] w2: [-21.59588271] bias: [16.43974886] loss: 30.863157977146702\n",
            "Epoch: 2659 / 5000\n",
            "w1: [24.15762588] w2: [-21.59800313] bias: [16.43966601] loss: 30.86251516389774\n",
            "Epoch: 2660 / 5000\n",
            "w1: [24.15900888] w2: [-21.60012145] bias: [16.43958315] loss: 30.861873568952046\n",
            "Epoch: 2661 / 5000\n",
            "w1: [24.16039067] w2: [-21.60223769] bias: [16.43950028] loss: 30.861233189989452\n",
            "Epoch: 2662 / 5000\n",
            "w1: [24.16177128] w2: [-21.60435184] bias: [16.43941741] loss: 30.86059402469419\n",
            "Epoch: 2663 / 5000\n",
            "w1: [24.16315069] w2: [-21.6064639] bias: [16.43933453] loss: 30.859956070754922\n",
            "Epoch: 2664 / 5000\n",
            "w1: [24.16452892] w2: [-21.60857388] bias: [16.43925164] loss: 30.8593193258647\n",
            "Epoch: 2665 / 5000\n",
            "w1: [24.16590596] w2: [-21.61068178] bias: [16.43916874] loss: 30.85868378772097\n",
            "Epoch: 2666 / 5000\n",
            "w1: [24.1672818] w2: [-21.61278761] bias: [16.43908583] loss: 30.858049454025597\n",
            "Epoch: 2667 / 5000\n",
            "w1: [24.16865647] w2: [-21.61489135] bias: [16.43900292] loss: 30.857416322484777\n",
            "Epoch: 2668 / 5000\n",
            "w1: [24.17002995] w2: [-21.61699302] bias: [16.43892] loss: 30.85678439080912\n",
            "Epoch: 2669 / 5000\n",
            "w1: [24.17140224] w2: [-21.61909262] bias: [16.43883707] loss: 30.856153656713584\n",
            "Epoch: 2670 / 5000\n",
            "w1: [24.17277335] w2: [-21.62119014] bias: [16.43875413] loss: 30.855524117917472\n",
            "Epoch: 2671 / 5000\n",
            "w1: [24.17414328] w2: [-21.6232856] bias: [16.43867118] loss: 30.854895772144445\n",
            "Epoch: 2672 / 5000\n",
            "w1: [24.17551203] w2: [-21.62537899] bias: [16.43858823] loss: 30.854268617122493\n",
            "Epoch: 2673 / 5000\n",
            "w1: [24.1768796] w2: [-21.62747031] bias: [16.43850527] loss: 30.85364265058395\n",
            "Epoch: 2674 / 5000\n",
            "w1: [24.17824599] w2: [-21.62955957] bias: [16.4384223] loss: 30.85301787026546\n",
            "Epoch: 2675 / 5000\n",
            "w1: [24.1796112] w2: [-21.63164677] bias: [16.43833932] loss: 30.85239427390798\n",
            "Epoch: 2676 / 5000\n",
            "w1: [24.18097524] w2: [-21.63373192] bias: [16.43825634] loss: 30.851771859256782\n",
            "Epoch: 2677 / 5000\n",
            "w1: [24.1823381] w2: [-21.635815] bias: [16.43817335] loss: 30.85115062406142\n",
            "Epoch: 2678 / 5000\n",
            "w1: [24.18369978] w2: [-21.63789603] bias: [16.43809035] loss: 30.850530566075754\n",
            "Epoch: 2679 / 5000\n",
            "w1: [24.1850603] w2: [-21.63997501] bias: [16.43800734] loss: 30.849911683057908\n",
            "Epoch: 2680 / 5000\n",
            "w1: [24.18641964] w2: [-21.64205193] bias: [16.43792433] loss: 30.849293972770294\n",
            "Epoch: 2681 / 5000\n",
            "w1: [24.18777781] w2: [-21.64412681] bias: [16.4378413] loss: 30.84867743297957\n",
            "Epoch: 2682 / 5000\n",
            "w1: [24.18913481] w2: [-21.64619964] bias: [16.43775827] loss: 30.848062061456677\n",
            "Epoch: 2683 / 5000\n",
            "w1: [24.19049065] w2: [-21.64827042] bias: [16.43767524] loss: 30.84744785597677\n",
            "Epoch: 2684 / 5000\n",
            "w1: [24.19184531] w2: [-21.65033916] bias: [16.43759219] loss: 30.846834814319276\n",
            "Epoch: 2685 / 5000\n",
            "w1: [24.19319881] w2: [-21.65240586] bias: [16.43750914] loss: 30.846222934267825\n",
            "Epoch: 2686 / 5000\n",
            "w1: [24.19455115] w2: [-21.65447052] bias: [16.43742608] loss: 30.845612213610288\n",
            "Epoch: 2687 / 5000\n",
            "w1: [24.19590232] w2: [-21.65653314] bias: [16.43734301] loss: 30.84500265013876\n",
            "Epoch: 2688 / 5000\n",
            "w1: [24.19725232] w2: [-21.65859373] bias: [16.43725994] loss: 30.844394241649518\n",
            "Epoch: 2689 / 5000\n",
            "w1: [24.19860117] w2: [-21.66065228] bias: [16.43717686] loss: 30.843786985943073\n",
            "Epoch: 2690 / 5000\n",
            "w1: [24.19994885] w2: [-21.6627088] bias: [16.43709377] loss: 30.84318088082408\n",
            "Epoch: 2691 / 5000\n",
            "w1: [24.20129538] w2: [-21.66476329] bias: [16.43701067] loss: 30.84257592410143\n",
            "Epoch: 2692 / 5000\n",
            "w1: [24.20264074] w2: [-21.66681576] bias: [16.43692756] loss: 30.841972113588156\n",
            "Epoch: 2693 / 5000\n",
            "w1: [24.20398495] w2: [-21.66886619] bias: [16.43684445] loss: 30.841369447101467\n",
            "Epoch: 2694 / 5000\n",
            "w1: [24.205328] w2: [-21.67091461] bias: [16.43676133] loss: 30.840767922462735\n",
            "Epoch: 2695 / 5000\n",
            "w1: [24.2066699] w2: [-21.672961] bias: [16.43667821] loss: 30.840167537497468\n",
            "Epoch: 2696 / 5000\n",
            "w1: [24.20801064] w2: [-21.67500537] bias: [16.43659507] loss: 30.839568290035356\n",
            "Epoch: 2697 / 5000\n",
            "w1: [24.20935023] w2: [-21.67704773] bias: [16.43651193] loss: 30.838970177910184\n",
            "Epoch: 2698 / 5000\n",
            "w1: [24.21068866] w2: [-21.67908807] bias: [16.43642878] loss: 30.83837319895988\n",
            "Epoch: 2699 / 5000\n",
            "w1: [24.21202595] w2: [-21.68112639] bias: [16.43634563] loss: 30.837777351026507\n",
            "Epoch: 2700 / 5000\n",
            "w1: [24.21336208] w2: [-21.6831627] bias: [16.43626246] loss: 30.837182631956217\n",
            "Epoch: 2701 / 5000\n",
            "w1: [24.21469707] w2: [-21.685197] bias: [16.43617929] loss: 30.83658903959928\n",
            "Epoch: 2702 / 5000\n",
            "w1: [24.2160309] w2: [-21.6872293] bias: [16.43609612] loss: 30.83599657181006\n",
            "Epoch: 2703 / 5000\n",
            "w1: [24.2173636] w2: [-21.68925958] bias: [16.43601293] loss: 30.83540522644701\n",
            "Epoch: 2704 / 5000\n",
            "w1: [24.21869514] w2: [-21.69128787] bias: [16.43592974] loss: 30.834815001372665\n",
            "Epoch: 2705 / 5000\n",
            "w1: [24.22002554] w2: [-21.69331415] bias: [16.43584654] loss: 30.834225894453635\n",
            "Epoch: 2706 / 5000\n",
            "w1: [24.2213548] w2: [-21.69533842] bias: [16.43576333] loss: 30.833637903560582\n",
            "Epoch: 2707 / 5000\n",
            "w1: [24.22268291] w2: [-21.6973607] bias: [16.43568012] loss: 30.833051026568256\n",
            "Epoch: 2708 / 5000\n",
            "w1: [24.22400988] w2: [-21.69938099] bias: [16.4355969] loss: 30.83246526135542\n",
            "Epoch: 2709 / 5000\n",
            "w1: [24.22533572] w2: [-21.70139928] bias: [16.43551367] loss: 30.831880605804905\n",
            "Epoch: 2710 / 5000\n",
            "w1: [24.22666041] w2: [-21.70341557] bias: [16.43543044] loss: 30.831297057803564\n",
            "Epoch: 2711 / 5000\n",
            "w1: [24.22798396] w2: [-21.70542988] bias: [16.4353472] loss: 30.830714615242293\n",
            "Epoch: 2712 / 5000\n",
            "w1: [24.22930638] w2: [-21.70744219] bias: [16.43526395] loss: 30.830133276015992\n",
            "Epoch: 2713 / 5000\n",
            "w1: [24.23062766] w2: [-21.70945252] bias: [16.43518069] loss: 30.829553038023576\n",
            "Epoch: 2714 / 5000\n",
            "w1: [24.23194781] w2: [-21.71146086] bias: [16.43509743] loss: 30.82897389916797\n",
            "Epoch: 2715 / 5000\n",
            "w1: [24.23326682] w2: [-21.71346722] bias: [16.43501416] loss: 30.828395857356078\n",
            "Epoch: 2716 / 5000\n",
            "w1: [24.2345847] w2: [-21.7154716] bias: [16.43493088] loss: 30.827818910498827\n",
            "Epoch: 2717 / 5000\n",
            "w1: [24.23590145] w2: [-21.71747399] bias: [16.4348476] loss: 30.82724305651109\n",
            "Epoch: 2718 / 5000\n",
            "w1: [24.23721707] w2: [-21.71947441] bias: [16.43476431] loss: 30.826668293311727\n",
            "Epoch: 2719 / 5000\n",
            "w1: [24.23853155] w2: [-21.72147286] bias: [16.43468101] loss: 30.82609461882357\n",
            "Epoch: 2720 / 5000\n",
            "w1: [24.23984491] w2: [-21.72346933] bias: [16.43459771] loss: 30.825522030973403\n",
            "Epoch: 2721 / 5000\n",
            "w1: [24.24115714] w2: [-21.72546382] bias: [16.4345144] loss: 30.82495052769196\n",
            "Epoch: 2722 / 5000\n",
            "w1: [24.24246825] w2: [-21.72745635] bias: [16.43443108] loss: 30.824380106913924\n",
            "Epoch: 2723 / 5000\n",
            "w1: [24.24377823] w2: [-21.72944691] bias: [16.43434775] loss: 30.823810766577907\n",
            "Epoch: 2724 / 5000\n",
            "w1: [24.24508708] w2: [-21.7314355] bias: [16.43426442] loss: 30.82324250462646\n",
            "Epoch: 2725 / 5000\n",
            "w1: [24.24639481] w2: [-21.73342213] bias: [16.43418108] loss: 30.822675319006034\n",
            "Epoch: 2726 / 5000\n",
            "w1: [24.24770142] w2: [-21.7354068] bias: [16.43409774] loss: 30.82210920766703\n",
            "Epoch: 2727 / 5000\n",
            "w1: [24.24900691] w2: [-21.7373895] bias: [16.43401439] loss: 30.821544168563705\n",
            "Epoch: 2728 / 5000\n",
            "w1: [24.25031128] w2: [-21.73937025] bias: [16.43393103] loss: 30.820980199654265\n",
            "Epoch: 2729 / 5000\n",
            "w1: [24.25161452] w2: [-21.74134904] bias: [16.43384766] loss: 30.82041729890077\n",
            "Epoch: 2730 / 5000\n",
            "w1: [24.25291665] w2: [-21.74332587] bias: [16.43376429] loss: 30.819855464269185\n",
            "Epoch: 2731 / 5000\n",
            "w1: [24.25421767] w2: [-21.74530075] bias: [16.43368091] loss: 30.81929469372934\n",
            "Epoch: 2732 / 5000\n",
            "w1: [24.25551756] w2: [-21.74727368] bias: [16.43359753] loss: 30.81873498525494\n",
            "Epoch: 2733 / 5000\n",
            "w1: [24.25681635] w2: [-21.74924466] bias: [16.43351414] loss: 30.818176336823548\n",
            "Epoch: 2734 / 5000\n",
            "w1: [24.25811401] w2: [-21.75121369] bias: [16.43343074] loss: 30.817618746416585\n",
            "Epoch: 2735 / 5000\n",
            "w1: [24.25941057] w2: [-21.75318077] bias: [16.43334733] loss: 30.817062212019316\n",
            "Epoch: 2736 / 5000\n",
            "w1: [24.26070601] w2: [-21.75514591] bias: [16.43326392] loss: 30.81650673162084\n",
            "Epoch: 2737 / 5000\n",
            "w1: [24.26200035] w2: [-21.75710911] bias: [16.4331805] loss: 30.815952303214104\n",
            "Epoch: 2738 / 5000\n",
            "w1: [24.26329357] w2: [-21.75907037] bias: [16.43309708] loss: 30.81539892479586\n",
            "Epoch: 2739 / 5000\n",
            "w1: [24.26458568] w2: [-21.76102969] bias: [16.43301365] loss: 30.8148465943667\n",
            "Epoch: 2740 / 5000\n",
            "w1: [24.26587669] w2: [-21.76298708] bias: [16.43293021] loss: 30.814295309931005\n",
            "Epoch: 2741 / 5000\n",
            "w1: [24.26716659] w2: [-21.76494253] bias: [16.43284676] loss: 30.813745069496967\n",
            "Epoch: 2742 / 5000\n",
            "w1: [24.26845538] w2: [-21.76689604] bias: [16.43276331] loss: 30.81319587107658\n",
            "Epoch: 2743 / 5000\n",
            "w1: [24.26974307] w2: [-21.76884763] bias: [16.43267986] loss: 30.81264771268561\n",
            "Epoch: 2744 / 5000\n",
            "w1: [24.27102966] w2: [-21.77079729] bias: [16.43259639] loss: 30.81210059234364\n",
            "Epoch: 2745 / 5000\n",
            "w1: [24.27231514] w2: [-21.77274501] bias: [16.43251292] loss: 30.811554508073968\n",
            "Epoch: 2746 / 5000\n",
            "w1: [24.27359952] w2: [-21.77469082] bias: [16.43242945] loss: 30.811009457903722\n",
            "Epoch: 2747 / 5000\n",
            "w1: [24.27488281] w2: [-21.7766347] bias: [16.43234596] loss: 30.810465439863755\n",
            "Epoch: 2748 / 5000\n",
            "w1: [24.27616499] w2: [-21.77857666] bias: [16.43226247] loss: 30.80992245198868\n",
            "Epoch: 2749 / 5000\n",
            "w1: [24.27744607] w2: [-21.7805167] bias: [16.43217898] loss: 30.80938049231684\n",
            "Epoch: 2750 / 5000\n",
            "w1: [24.27872606] w2: [-21.78245482] bias: [16.43209548] loss: 30.80883955889035\n",
            "Epoch: 2751 / 5000\n",
            "w1: [24.28000495] w2: [-21.78439102] bias: [16.43201197] loss: 30.80829964975503\n",
            "Epoch: 2752 / 5000\n",
            "w1: [24.28128274] w2: [-21.78632531] bias: [16.43192845] loss: 30.807760762960427\n",
            "Epoch: 2753 / 5000\n",
            "w1: [24.28255945] w2: [-21.78825769] bias: [16.43184493] loss: 30.80722289655981\n",
            "Epoch: 2754 / 5000\n",
            "w1: [24.28383505] w2: [-21.79018815] bias: [16.4317614] loss: 30.806686048610164\n",
            "Epoch: 2755 / 5000\n",
            "w1: [24.28510957] w2: [-21.79211671] bias: [16.43167787] loss: 30.806150217172164\n",
            "Epoch: 2756 / 5000\n",
            "w1: [24.28638299] w2: [-21.79404336] bias: [16.43159433] loss: 30.805615400310185\n",
            "Epoch: 2757 / 5000\n",
            "w1: [24.28765533] w2: [-21.79596811] bias: [16.43151078] loss: 30.8050815960923\n",
            "Epoch: 2758 / 5000\n",
            "w1: [24.28892657] w2: [-21.79789095] bias: [16.43142723] loss: 30.804548802590244\n",
            "Epoch: 2759 / 5000\n",
            "w1: [24.29019673] w2: [-21.79981189] bias: [16.43134367] loss: 30.80401701787945\n",
            "Epoch: 2760 / 5000\n",
            "w1: [24.2914658] w2: [-21.80173093] bias: [16.43126011] loss: 30.803486240039003\n",
            "Epoch: 2761 / 5000\n",
            "w1: [24.29273378] w2: [-21.80364807] bias: [16.43117654] loss: 30.802956467151652\n",
            "Epoch: 2762 / 5000\n",
            "w1: [24.29400068] w2: [-21.80556332] bias: [16.43109296] loss: 30.802427697303795\n",
            "Epoch: 2763 / 5000\n",
            "w1: [24.2952665] w2: [-21.80747667] bias: [16.43100938] loss: 30.801899928585502\n",
            "Epoch: 2764 / 5000\n",
            "w1: [24.29653123] w2: [-21.80938813] bias: [16.43092579] loss: 30.801373159090446\n",
            "Epoch: 2765 / 5000\n",
            "w1: [24.29779488] w2: [-21.8112977] bias: [16.43084219] loss: 30.800847386915947\n",
            "Epoch: 2766 / 5000\n",
            "w1: [24.29905745] w2: [-21.81320538] bias: [16.43075859] loss: 30.80032261016297\n",
            "Epoch: 2767 / 5000\n",
            "w1: [24.30031894] w2: [-21.81511118] bias: [16.43067499] loss: 30.799798826936065\n",
            "Epoch: 2768 / 5000\n",
            "w1: [24.30157935] w2: [-21.81701508] bias: [16.43059137] loss: 30.79927603534343\n",
            "Epoch: 2769 / 5000\n",
            "w1: [24.30283868] w2: [-21.81891711] bias: [16.43050775] loss: 30.79875423349684\n",
            "Epoch: 2770 / 5000\n",
            "w1: [24.30409693] w2: [-21.82081725] bias: [16.43042413] loss: 30.79823341951168\n",
            "Epoch: 2771 / 5000\n",
            "w1: [24.30535411] w2: [-21.82271551] bias: [16.4303405] loss: 30.79771359150692\n",
            "Epoch: 2772 / 5000\n",
            "w1: [24.30661022] w2: [-21.8246119] bias: [16.43025686] loss: 30.797194747605136\n",
            "Epoch: 2773 / 5000\n",
            "w1: [24.30786525] w2: [-21.82650641] bias: [16.43017322] loss: 30.796676885932452\n",
            "Epoch: 2774 / 5000\n",
            "w1: [24.30911921] w2: [-21.82839904] bias: [16.43008957] loss: 30.79616000461858\n",
            "Epoch: 2775 / 5000\n",
            "w1: [24.31037209] w2: [-21.8302898] bias: [16.43000591] loss: 30.795644101796793\n",
            "Epoch: 2776 / 5000\n",
            "w1: [24.31162391] w2: [-21.8321787] bias: [16.42992225] loss: 30.795129175603915\n",
            "Epoch: 2777 / 5000\n",
            "w1: [24.31287465] w2: [-21.83406572] bias: [16.42983858] loss: 30.794615224180344\n",
            "Epoch: 2778 / 5000\n",
            "w1: [24.31412433] w2: [-21.83595087] bias: [16.42975491] loss: 30.79410224566999\n",
            "Epoch: 2779 / 5000\n",
            "w1: [24.31537294] w2: [-21.83783416] bias: [16.42967123] loss: 30.793590238220315\n",
            "Epoch: 2780 / 5000\n",
            "w1: [24.31662048] w2: [-21.83971559] bias: [16.42958755] loss: 30.793079199982316\n",
            "Epoch: 2781 / 5000\n",
            "w1: [24.31786695] w2: [-21.84159515] bias: [16.42950386] loss: 30.79256912911051\n",
            "Epoch: 2782 / 5000\n",
            "w1: [24.31911236] w2: [-21.84347285] bias: [16.42942016] loss: 30.79206002376293\n",
            "Epoch: 2783 / 5000\n",
            "w1: [24.32035671] w2: [-21.8453487] bias: [16.42933646] loss: 30.79155188210111\n",
            "Epoch: 2784 / 5000\n",
            "w1: [24.32159999] w2: [-21.84722269] bias: [16.42925275] loss: 30.791044702290115\n",
            "Epoch: 2785 / 5000\n",
            "w1: [24.32284222] w2: [-21.84909482] bias: [16.42916904] loss: 30.79053848249847\n",
            "Epoch: 2786 / 5000\n",
            "w1: [24.32408338] w2: [-21.8509651] bias: [16.42908532] loss: 30.79003322089822\n",
            "Epoch: 2787 / 5000\n",
            "w1: [24.32532348] w2: [-21.85283353] bias: [16.4290016] loss: 30.789528915664878\n",
            "Epoch: 2788 / 5000\n",
            "w1: [24.32656252] w2: [-21.85470011] bias: [16.42891787] loss: 30.789025564977432\n",
            "Epoch: 2789 / 5000\n",
            "w1: [24.32780051] w2: [-21.85656485] bias: [16.42883413] loss: 30.788523167018354\n",
            "Epoch: 2790 / 5000\n",
            "w1: [24.32903743] w2: [-21.85842773] bias: [16.42875039] loss: 30.788021719973575\n",
            "Epoch: 2791 / 5000\n",
            "w1: [24.33027331] w2: [-21.86028877] bias: [16.42866664] loss: 30.787521222032467\n",
            "Epoch: 2792 / 5000\n",
            "w1: [24.33150812] w2: [-21.86214798] bias: [16.42858289] loss: 30.787021671387883\n",
            "Epoch: 2793 / 5000\n",
            "w1: [24.33274189] w2: [-21.86400533] bias: [16.42849913] loss: 30.786523066236086\n",
            "Epoch: 2794 / 5000\n",
            "w1: [24.3339746] w2: [-21.86586086] bias: [16.42841537] loss: 30.786025404776804\n",
            "Epoch: 2795 / 5000\n",
            "w1: [24.33520625] w2: [-21.86771454] bias: [16.4283316] loss: 30.78552868521318\n",
            "Epoch: 2796 / 5000\n",
            "w1: [24.33643686] w2: [-21.86956639] bias: [16.42824782] loss: 30.785032905751788\n",
            "Epoch: 2797 / 5000\n",
            "w1: [24.33766642] w2: [-21.8714164] bias: [16.42816404] loss: 30.784538064602614\n",
            "Epoch: 2798 / 5000\n",
            "w1: [24.33889493] w2: [-21.87326459] bias: [16.42808026] loss: 30.78404415997906\n",
            "Epoch: 2799 / 5000\n",
            "w1: [24.34012239] w2: [-21.87511094] bias: [16.42799646] loss: 30.783551190097928\n",
            "Epoch: 2800 / 5000\n",
            "w1: [24.3413488] w2: [-21.87695546] bias: [16.42791267] loss: 30.78305915317943\n",
            "Epoch: 2801 / 5000\n",
            "w1: [24.34257417] w2: [-21.87879816] bias: [16.42782886] loss: 30.78256804744715\n",
            "Epoch: 2802 / 5000\n",
            "w1: [24.34379849] w2: [-21.88063904] bias: [16.42774506] loss: 30.78207787112808\n",
            "Epoch: 2803 / 5000\n",
            "w1: [24.34502177] w2: [-21.88247809] bias: [16.42766124] loss: 30.781588622452574\n",
            "Epoch: 2804 / 5000\n",
            "w1: [24.346244] w2: [-21.88431532] bias: [16.42757742] loss: 30.78110029965436\n",
            "Epoch: 2805 / 5000\n",
            "w1: [24.3474652] w2: [-21.88615072] bias: [16.4274936] loss: 30.78061290097054\n",
            "Epoch: 2806 / 5000\n",
            "w1: [24.34868535] w2: [-21.88798432] bias: [16.42740977] loss: 30.780126424641573\n",
            "Epoch: 2807 / 5000\n",
            "w1: [24.34990446] w2: [-21.88981609] bias: [16.42732593] loss: 30.77964086891126\n",
            "Epoch: 2808 / 5000\n",
            "w1: [24.35112253] w2: [-21.89164605] bias: [16.42724209] loss: 30.77915623202677\n",
            "Epoch: 2809 / 5000\n",
            "w1: [24.35233957] w2: [-21.8934742] bias: [16.42715825] loss: 30.778672512238597\n",
            "Epoch: 2810 / 5000\n",
            "w1: [24.35355556] w2: [-21.89530054] bias: [16.4270744] loss: 30.778189707800564\n",
            "Epoch: 2811 / 5000\n",
            "w1: [24.35477052] w2: [-21.89712507] bias: [16.42699054] loss: 30.77770781696984\n",
            "Epoch: 2812 / 5000\n",
            "w1: [24.35598445] w2: [-21.89894779] bias: [16.42690668] loss: 30.777226838006897\n",
            "Epoch: 2813 / 5000\n",
            "w1: [24.35719734] w2: [-21.90076871] bias: [16.42682281] loss: 30.776746769175542\n",
            "Epoch: 2814 / 5000\n",
            "w1: [24.3584092] w2: [-21.90258782] bias: [16.42673894] loss: 30.776267608742867\n",
            "Epoch: 2815 / 5000\n",
            "w1: [24.35962002] w2: [-21.90440513] bias: [16.42665506] loss: 30.775789354979285\n",
            "Epoch: 2816 / 5000\n",
            "w1: [24.36082982] w2: [-21.90622064] bias: [16.42657118] loss: 30.775312006158494\n",
            "Epoch: 2817 / 5000\n",
            "w1: [24.36203858] w2: [-21.90803435] bias: [16.42648729] loss: 30.77483556055749\n",
            "Epoch: 2818 / 5000\n",
            "w1: [24.36324631] w2: [-21.90984626] bias: [16.4264034] loss: 30.774360016456544\n",
            "Epoch: 2819 / 5000\n",
            "w1: [24.36445302] w2: [-21.91165638] bias: [16.4263195] loss: 30.77388537213921\n",
            "Epoch: 2820 / 5000\n",
            "w1: [24.3656587] w2: [-21.91346471] bias: [16.4262356] loss: 30.773411625892315\n",
            "Epoch: 2821 / 5000\n",
            "w1: [24.36686335] w2: [-21.91527124] bias: [16.42615169] loss: 30.77293877600594\n",
            "Epoch: 2822 / 5000\n",
            "w1: [24.36806697] w2: [-21.91707598] bias: [16.42606777] loss: 30.772466820773452\n",
            "Epoch: 2823 / 5000\n",
            "w1: [24.36926957] w2: [-21.91887894] bias: [16.42598385] loss: 30.771995758491425\n",
            "Epoch: 2824 / 5000\n",
            "w1: [24.37047115] w2: [-21.92068011] bias: [16.42589993] loss: 30.771525587459717\n",
            "Epoch: 2825 / 5000\n",
            "w1: [24.3716717] w2: [-21.92247949] bias: [16.425816] loss: 30.77105630598141\n",
            "Epoch: 2826 / 5000\n",
            "w1: [24.37287124] w2: [-21.92427709] bias: [16.42573207] loss: 30.770587912362828\n",
            "Epoch: 2827 / 5000\n",
            "w1: [24.37406975] w2: [-21.92607291] bias: [16.42564813] loss: 30.77012040491351\n",
            "Epoch: 2828 / 5000\n",
            "w1: [24.37526724] w2: [-21.92786694] bias: [16.42556418] loss: 30.769653781946236\n",
            "Epoch: 2829 / 5000\n",
            "w1: [24.37646371] w2: [-21.92965921] bias: [16.42548024] loss: 30.76918804177697\n",
            "Epoch: 2830 / 5000\n",
            "w1: [24.37765917] w2: [-21.93144969] bias: [16.42539628] loss: 30.768723182724926\n",
            "Epoch: 2831 / 5000\n",
            "w1: [24.37885361] w2: [-21.9332384] bias: [16.42531232] loss: 30.768259203112486\n",
            "Epoch: 2832 / 5000\n",
            "w1: [24.38004703] w2: [-21.93502533] bias: [16.42522836] loss: 30.767796101265255\n",
            "Epoch: 2833 / 5000\n",
            "w1: [24.38123943] w2: [-21.9368105] bias: [16.42514439] loss: 30.767333875512005\n",
            "Epoch: 2834 / 5000\n",
            "w1: [24.38243083] w2: [-21.93859389] bias: [16.42506042] loss: 30.766872524184706\n",
            "Epoch: 2835 / 5000\n",
            "w1: [24.38362121] w2: [-21.94037552] bias: [16.42497644] loss: 30.766412045618505\n",
            "Epoch: 2836 / 5000\n",
            "w1: [24.38481057] w2: [-21.94215538] bias: [16.42489245] loss: 30.76595243815173\n",
            "Epoch: 2837 / 5000\n",
            "w1: [24.38599893] w2: [-21.94393348] bias: [16.42480846] loss: 30.76549370012586\n",
            "Epoch: 2838 / 5000\n",
            "w1: [24.38718627] w2: [-21.94570981] bias: [16.42472447] loss: 30.765035829885534\n",
            "Epoch: 2839 / 5000\n",
            "w1: [24.38837261] w2: [-21.94748438] bias: [16.42464047] loss: 30.764578825778575\n",
            "Epoch: 2840 / 5000\n",
            "w1: [24.38955794] w2: [-21.9492572] bias: [16.42455647] loss: 30.76412268615592\n",
            "Epoch: 2841 / 5000\n",
            "w1: [24.39074226] w2: [-21.95102825] bias: [16.42447246] loss: 30.76366740937165\n",
            "Epoch: 2842 / 5000\n",
            "w1: [24.39192557] w2: [-21.95279755] bias: [16.42438845] loss: 30.763212993783014\n",
            "Epoch: 2843 / 5000\n",
            "w1: [24.39310788] w2: [-21.95456509] bias: [16.42430443] loss: 30.76275943775036\n",
            "Epoch: 2844 / 5000\n",
            "w1: [24.39428918] w2: [-21.95633089] bias: [16.42422041] loss: 30.76230673963718\n",
            "Epoch: 2845 / 5000\n",
            "w1: [24.39546948] w2: [-21.95809493] bias: [16.42413638] loss: 30.76185489781007\n",
            "Epoch: 2846 / 5000\n",
            "w1: [24.39664878] w2: [-21.95985722] bias: [16.42405235] loss: 30.761403910638755\n",
            "Epoch: 2847 / 5000\n",
            "w1: [24.39782707] w2: [-21.96161776] bias: [16.42396831] loss: 30.760953776496045\n",
            "Epoch: 2848 / 5000\n",
            "w1: [24.39900436] w2: [-21.96337656] bias: [16.42388427] loss: 30.760504493757868\n",
            "Epoch: 2849 / 5000\n",
            "w1: [24.40018066] w2: [-21.96513361] bias: [16.42380023] loss: 30.76005606080324\n",
            "Epoch: 2850 / 5000\n",
            "w1: [24.40135595] w2: [-21.96688892] bias: [16.42371618] loss: 30.75960847601428\n",
            "Epoch: 2851 / 5000\n",
            "w1: [24.40253025] w2: [-21.96864249] bias: [16.42363212] loss: 30.759161737776168\n",
            "Epoch: 2852 / 5000\n",
            "w1: [24.40370355] w2: [-21.97039432] bias: [16.42354806] loss: 30.758715844477166\n",
            "Epoch: 2853 / 5000\n",
            "w1: [24.40487585] w2: [-21.97214442] bias: [16.423464] loss: 30.758270794508622\n",
            "Epoch: 2854 / 5000\n",
            "w1: [24.40604716] w2: [-21.97389277] bias: [16.42337993] loss: 30.75782658626494\n",
            "Epoch: 2855 / 5000\n",
            "w1: [24.40721747] w2: [-21.97563939] bias: [16.42329585] loss: 30.75738321814358\n",
            "Epoch: 2856 / 5000\n",
            "w1: [24.40838679] w2: [-21.97738428] bias: [16.42321178] loss: 30.75694068854507\n",
            "Epoch: 2857 / 5000\n",
            "w1: [24.40955512] w2: [-21.97912744] bias: [16.42312769] loss: 30.756498995872967\n",
            "Epoch: 2858 / 5000\n",
            "w1: [24.41072246] w2: [-21.98086887] bias: [16.42304361] loss: 30.75605813853387\n",
            "Epoch: 2859 / 5000\n",
            "w1: [24.4118888] w2: [-21.98260857] bias: [16.42295951] loss: 30.75561811493745\n",
            "Epoch: 2860 / 5000\n",
            "w1: [24.41305416] w2: [-21.98434655] bias: [16.42287542] loss: 30.755178923496363\n",
            "Epoch: 2861 / 5000\n",
            "w1: [24.41421852] w2: [-21.9860828] bias: [16.42279132] loss: 30.754740562626317\n",
            "Epoch: 2862 / 5000\n",
            "w1: [24.4153819] w2: [-21.98781733] bias: [16.42270721] loss: 30.754303030746023\n",
            "Epoch: 2863 / 5000\n",
            "w1: [24.41654429] w2: [-21.98955013] bias: [16.4226231] loss: 30.753866326277222\n",
            "Epoch: 2864 / 5000\n",
            "w1: [24.4177057] w2: [-21.99128122] bias: [16.42253899] loss: 30.753430447644654\n",
            "Epoch: 2865 / 5000\n",
            "w1: [24.41886612] w2: [-21.99301059] bias: [16.42245487] loss: 30.75299539327606\n",
            "Epoch: 2866 / 5000\n",
            "w1: [24.42002555] w2: [-21.99473824] bias: [16.42237075] loss: 30.75256116160218\n",
            "Epoch: 2867 / 5000\n",
            "w1: [24.42118401] w2: [-21.99646418] bias: [16.42228662] loss: 30.752127751056744\n",
            "Epoch: 2868 / 5000\n",
            "w1: [24.42234148] w2: [-21.99818841] bias: [16.42220249] loss: 30.75169516007646\n",
            "Epoch: 2869 / 5000\n",
            "w1: [24.42349797] w2: [-21.99991093] bias: [16.42211835] loss: 30.751263387101027\n",
            "Epoch: 2870 / 5000\n",
            "w1: [24.42465348] w2: [-22.00163173] bias: [16.42203421] loss: 30.75083243057311\n",
            "Epoch: 2871 / 5000\n",
            "w1: [24.425808] w2: [-22.00335083] bias: [16.42195006] loss: 30.750402288938336\n",
            "Epoch: 2872 / 5000\n",
            "w1: [24.42696155] w2: [-22.00506822] bias: [16.42186591] loss: 30.749972960645316\n",
            "Epoch: 2873 / 5000\n",
            "w1: [24.42811413] w2: [-22.00678391] bias: [16.42178176] loss: 30.749544444145595\n",
            "Epoch: 2874 / 5000\n",
            "w1: [24.42926572] w2: [-22.00849789] bias: [16.4216976] loss: 30.74911673789367\n",
            "Epoch: 2875 / 5000\n",
            "w1: [24.43041634] w2: [-22.01021017] bias: [16.42161344] loss: 30.748689840346998\n",
            "Epoch: 2876 / 5000\n",
            "w1: [24.43156599] w2: [-22.01192075] bias: [16.42152927] loss: 30.748263749965965\n",
            "Epoch: 2877 / 5000\n",
            "w1: [24.43271466] w2: [-22.01362964] bias: [16.4214451] loss: 30.74783846521388\n",
            "Epoch: 2878 / 5000\n",
            "w1: [24.43386236] w2: [-22.01533683] bias: [16.42136093] loss: 30.747413984557028\n",
            "Epoch: 2879 / 5000\n",
            "w1: [24.43500908] w2: [-22.01704232] bias: [16.42127675] loss: 30.746990306464543\n",
            "Epoch: 2880 / 5000\n",
            "w1: [24.43615484] w2: [-22.01874612] bias: [16.42119257] loss: 30.74656742940854\n",
            "Epoch: 2881 / 5000\n",
            "w1: [24.43729962] w2: [-22.02044823] bias: [16.42110838] loss: 30.746145351864012\n",
            "Epoch: 2882 / 5000\n",
            "w1: [24.43844343] w2: [-22.02214865] bias: [16.42102419] loss: 30.745724072308874\n",
            "Epoch: 2883 / 5000\n",
            "w1: [24.43958628] w2: [-22.02384738] bias: [16.42093999] loss: 30.745303589223923\n",
            "Epoch: 2884 / 5000\n",
            "w1: [24.44072816] w2: [-22.02554442] bias: [16.42085579] loss: 30.74488390109287\n",
            "Epoch: 2885 / 5000\n",
            "w1: [24.44186907] w2: [-22.02723978] bias: [16.42077159] loss: 30.744465006402304\n",
            "Epoch: 2886 / 5000\n",
            "w1: [24.44300901] w2: [-22.02893345] bias: [16.42068738] loss: 30.74404690364171\n",
            "Epoch: 2887 / 5000\n",
            "w1: [24.44414799] w2: [-22.03062544] bias: [16.42060316] loss: 30.743629591303446\n",
            "Epoch: 2888 / 5000\n",
            "w1: [24.44528601] w2: [-22.03231575] bias: [16.42051895] loss: 30.74321306788273\n",
            "Epoch: 2889 / 5000\n",
            "w1: [24.44642306] w2: [-22.03400439] bias: [16.42043473] loss: 30.742797331877664\n",
            "Epoch: 2890 / 5000\n",
            "w1: [24.44755915] w2: [-22.03569134] bias: [16.4203505] loss: 30.742382381789213\n",
            "Epoch: 2891 / 5000\n",
            "w1: [24.44869428] w2: [-22.03737662] bias: [16.42026627] loss: 30.74196821612118\n",
            "Epoch: 2892 / 5000\n",
            "w1: [24.44982845] w2: [-22.03906023] bias: [16.42018204] loss: 30.74155483338023\n",
            "Epoch: 2893 / 5000\n",
            "w1: [24.45096166] w2: [-22.04074217] bias: [16.42009781] loss: 30.741142232075887\n",
            "Epoch: 2894 / 5000\n",
            "w1: [24.45209391] w2: [-22.04242243] bias: [16.42001356] loss: 30.7407304107205\n",
            "Epoch: 2895 / 5000\n",
            "w1: [24.4532252] w2: [-22.04410102] bias: [16.41992932] loss: 30.740319367829244\n",
            "Epoch: 2896 / 5000\n",
            "w1: [24.45435553] w2: [-22.04577795] bias: [16.41984507] loss: 30.73990910192016\n",
            "Epoch: 2897 / 5000\n",
            "w1: [24.45548491] w2: [-22.04745321] bias: [16.41976082] loss: 30.739499611514066\n",
            "Epoch: 2898 / 5000\n",
            "w1: [24.45661334] w2: [-22.04912681] bias: [16.41967656] loss: 30.739090895134638\n",
            "Epoch: 2899 / 5000\n",
            "w1: [24.45774081] w2: [-22.05079875] bias: [16.4195923] loss: 30.738682951308334\n",
            "Epoch: 2900 / 5000\n",
            "w1: [24.45886732] w2: [-22.05246902] bias: [16.41950804] loss: 30.73827577856445\n",
            "Epoch: 2901 / 5000\n",
            "w1: [24.45999289] w2: [-22.05413763] bias: [16.41942377] loss: 30.737869375435054\n",
            "Epoch: 2902 / 5000\n",
            "w1: [24.4611175] w2: [-22.05580459] bias: [16.4193395] loss: 30.737463740455045\n",
            "Epoch: 2903 / 5000\n",
            "w1: [24.46224116] w2: [-22.05746989] bias: [16.41925522] loss: 30.737058872162084\n",
            "Epoch: 2904 / 5000\n",
            "w1: [24.46336387] w2: [-22.05913353] bias: [16.41917094] loss: 30.736654769096628\n",
            "Epoch: 2905 / 5000\n",
            "w1: [24.46448564] w2: [-22.06079553] bias: [16.41908666] loss: 30.736251429801932\n",
            "Epoch: 2906 / 5000\n",
            "w1: [24.46560645] w2: [-22.06245587] bias: [16.41900237] loss: 30.735848852824\n",
            "Epoch: 2907 / 5000\n",
            "w1: [24.46672632] w2: [-22.06411456] bias: [16.41891808] loss: 30.735447036711626\n",
            "Epoch: 2908 / 5000\n",
            "w1: [24.46784524] w2: [-22.0657716] bias: [16.41883378] loss: 30.735045980016363\n",
            "Epoch: 2909 / 5000\n",
            "w1: [24.46896322] w2: [-22.06742699] bias: [16.41874949] loss: 30.73464568129252\n",
            "Epoch: 2910 / 5000\n",
            "w1: [24.47008025] w2: [-22.06908074] bias: [16.41866518] loss: 30.73424613909717\n",
            "Epoch: 2911 / 5000\n",
            "w1: [24.47119634] w2: [-22.07073285] bias: [16.41858088] loss: 30.73384735199013\n",
            "Epoch: 2912 / 5000\n",
            "w1: [24.47231148] w2: [-22.07238331] bias: [16.41849657] loss: 30.733449318533957\n",
            "Epoch: 2913 / 5000\n",
            "w1: [24.47342569] w2: [-22.07403214] bias: [16.41841225] loss: 30.73305203729397\n",
            "Epoch: 2914 / 5000\n",
            "w1: [24.47453895] w2: [-22.07567932] bias: [16.41832794] loss: 30.732655506838185\n",
            "Epoch: 2915 / 5000\n",
            "w1: [24.47565128] w2: [-22.07732487] bias: [16.41824362] loss: 30.732259725737375\n",
            "Epoch: 2916 / 5000\n",
            "w1: [24.47676266] w2: [-22.07896878] bias: [16.41815929] loss: 30.73186469256503\n",
            "Epoch: 2917 / 5000\n",
            "w1: [24.47787311] w2: [-22.08061106] bias: [16.41807496] loss: 30.731470405897355\n",
            "Epoch: 2918 / 5000\n",
            "w1: [24.47898261] w2: [-22.08225171] bias: [16.41799063] loss: 30.73107686431327\n",
            "Epoch: 2919 / 5000\n",
            "w1: [24.48009119] w2: [-22.08389072] bias: [16.41790629] loss: 30.730684066394396\n",
            "Epoch: 2920 / 5000\n",
            "w1: [24.48119882] w2: [-22.0855281] bias: [16.41782196] loss: 30.730292010725073\n",
            "Epoch: 2921 / 5000\n",
            "w1: [24.48230552] w2: [-22.08716386] bias: [16.41773761] loss: 30.729900695892333\n",
            "Epoch: 2922 / 5000\n",
            "w1: [24.48341129] w2: [-22.08879799] bias: [16.41765327] loss: 30.729510120485887\n",
            "Epoch: 2923 / 5000\n",
            "w1: [24.48451612] w2: [-22.0904305] bias: [16.41756892] loss: 30.729120283098144\n",
            "Epoch: 2924 / 5000\n",
            "w1: [24.48562003] w2: [-22.09206138] bias: [16.41748456] loss: 30.728731182324196\n",
            "Epoch: 2925 / 5000\n",
            "w1: [24.486723] w2: [-22.09369064] bias: [16.41740021] loss: 30.728342816761817\n",
            "Epoch: 2926 / 5000\n",
            "w1: [24.48782504] w2: [-22.09531828] bias: [16.41731585] loss: 30.727955185011446\n",
            "Epoch: 2927 / 5000\n",
            "w1: [24.48892615] w2: [-22.0969443] bias: [16.41723148] loss: 30.727568285676185\n",
            "Epoch: 2928 / 5000\n",
            "w1: [24.49002633] w2: [-22.0985687] bias: [16.41714711] loss: 30.727182117361803\n",
            "Epoch: 2929 / 5000\n",
            "w1: [24.49112559] w2: [-22.10019149] bias: [16.41706274] loss: 30.726796678676727\n",
            "Epoch: 2930 / 5000\n",
            "w1: [24.49222392] w2: [-22.10181267] bias: [16.41697837] loss: 30.726411968232046\n",
            "Epoch: 2931 / 5000\n",
            "w1: [24.49332132] w2: [-22.10343223] bias: [16.41689399] loss: 30.72602798464147\n",
            "Epoch: 2932 / 5000\n",
            "w1: [24.49441779] w2: [-22.10505018] bias: [16.41680961] loss: 30.725644726521367\n",
            "Epoch: 2933 / 5000\n",
            "w1: [24.49551335] w2: [-22.10666652] bias: [16.41672523] loss: 30.72526219249075\n",
            "Epoch: 2934 / 5000\n",
            "w1: [24.49660797] w2: [-22.10828126] bias: [16.41664084] loss: 30.724880381171236\n",
            "Epoch: 2935 / 5000\n",
            "w1: [24.49770168] w2: [-22.10989439] bias: [16.41655645] loss: 30.724499291187104\n",
            "Epoch: 2936 / 5000\n",
            "w1: [24.49879447] w2: [-22.11150591] bias: [16.41647205] loss: 30.724118921165235\n",
            "Epoch: 2937 / 5000\n",
            "w1: [24.49988633] w2: [-22.11311583] bias: [16.41638765] loss: 30.72373926973512\n",
            "Epoch: 2938 / 5000\n",
            "w1: [24.50097727] w2: [-22.11472415] bias: [16.41630325] loss: 30.723360335528874\n",
            "Epoch: 2939 / 5000\n",
            "w1: [24.5020673] w2: [-22.11633087] bias: [16.41621885] loss: 30.72298211718122\n",
            "Epoch: 2940 / 5000\n",
            "w1: [24.5031564] w2: [-22.11793599] bias: [16.41613444] loss: 30.72260461332947\n",
            "Epoch: 2941 / 5000\n",
            "w1: [24.50424459] w2: [-22.11953951] bias: [16.41605003] loss: 30.72222782261354\n",
            "Epoch: 2942 / 5000\n",
            "w1: [24.50533186] w2: [-22.12114144] bias: [16.41596561] loss: 30.721851743675934\n",
            "Epoch: 2943 / 5000\n",
            "w1: [24.50641822] w2: [-22.12274177] bias: [16.41588119] loss: 30.721476375161767\n",
            "Epoch: 2944 / 5000\n",
            "w1: [24.50750366] w2: [-22.12434052] bias: [16.41579677] loss: 30.721101715718696\n",
            "Epoch: 2945 / 5000\n",
            "w1: [24.50858819] w2: [-22.12593767] bias: [16.41571235] loss: 30.720727763996983\n",
            "Epoch: 2946 / 5000\n",
            "w1: [24.5096718] w2: [-22.12753323] bias: [16.41562792] loss: 30.72035451864945\n",
            "Epoch: 2947 / 5000\n",
            "w1: [24.51075451] w2: [-22.1291272] bias: [16.41554349] loss: 30.719981978331496\n",
            "Epoch: 2948 / 5000\n",
            "w1: [24.5118363] w2: [-22.13071959] bias: [16.41545906] loss: 30.71961014170108\n",
            "Epoch: 2949 / 5000\n",
            "w1: [24.51291718] w2: [-22.13231039] bias: [16.41537462] loss: 30.719239007418697\n",
            "Epoch: 2950 / 5000\n",
            "w1: [24.51399715] w2: [-22.13389961] bias: [16.41529018] loss: 30.718868574147432\n",
            "Epoch: 2951 / 5000\n",
            "w1: [24.51507621] w2: [-22.13548725] bias: [16.41520573] loss: 30.718498840552883\n",
            "Epoch: 2952 / 5000\n",
            "w1: [24.51615436] w2: [-22.1370733] bias: [16.41512129] loss: 30.71812980530321\n",
            "Epoch: 2953 / 5000\n",
            "w1: [24.51723161] w2: [-22.13865778] bias: [16.41503684] loss: 30.717761467069117\n",
            "Epoch: 2954 / 5000\n",
            "w1: [24.51830795] w2: [-22.14024068] bias: [16.41495238] loss: 30.71739382452382\n",
            "Epoch: 2955 / 5000\n",
            "w1: [24.51938338] w2: [-22.14182201] bias: [16.41486793] loss: 30.71702687634308\n",
            "Epoch: 2956 / 5000\n",
            "w1: [24.52045791] w2: [-22.14340176] bias: [16.41478347] loss: 30.716660621205172\n",
            "Epoch: 2957 / 5000\n",
            "w1: [24.52153154] w2: [-22.14497994] bias: [16.41469901] loss: 30.716295057790898\n",
            "Epoch: 2958 / 5000\n",
            "w1: [24.52260426] w2: [-22.14655655] bias: [16.41461454] loss: 30.71593018478356\n",
            "Epoch: 2959 / 5000\n",
            "w1: [24.52367609] w2: [-22.14813158] bias: [16.41453007] loss: 30.71556600086899\n",
            "Epoch: 2960 / 5000\n",
            "w1: [24.52474701] w2: [-22.14970505] bias: [16.4144456] loss: 30.7152025047355\n",
            "Epoch: 2961 / 5000\n",
            "w1: [24.52581702] w2: [-22.15127696] bias: [16.41436113] loss: 30.71483969507393\n",
            "Epoch: 2962 / 5000\n",
            "w1: [24.52688614] w2: [-22.15284729] bias: [16.41427665] loss: 30.71447757057759\n",
            "Epoch: 2963 / 5000\n",
            "w1: [24.52795437] w2: [-22.15441607] bias: [16.41419217] loss: 30.714116129942283\n",
            "Epoch: 2964 / 5000\n",
            "w1: [24.52902169] w2: [-22.15598328] bias: [16.41410769] loss: 30.71375537186632\n",
            "Epoch: 2965 / 5000\n",
            "w1: [24.53008812] w2: [-22.15754893] bias: [16.4140232] loss: 30.713395295050454\n",
            "Epoch: 2966 / 5000\n",
            "w1: [24.53115365] w2: [-22.15911302] bias: [16.41393871] loss: 30.713035898197955\n",
            "Epoch: 2967 / 5000\n",
            "w1: [24.53221828] w2: [-22.16067556] bias: [16.41385422] loss: 30.712677180014538\n",
            "Epoch: 2968 / 5000\n",
            "w1: [24.53328202] w2: [-22.16223654] bias: [16.41376972] loss: 30.712319139208393\n",
            "Epoch: 2969 / 5000\n",
            "w1: [24.53434487] w2: [-22.16379596] bias: [16.41368522] loss: 30.71196177449016\n",
            "Epoch: 2970 / 5000\n",
            "w1: [24.53540682] w2: [-22.16535383] bias: [16.41360072] loss: 30.711605084572952\n",
            "Epoch: 2971 / 5000\n",
            "w1: [24.53646788] w2: [-22.16691015] bias: [16.41351622] loss: 30.711249068172332\n",
            "Epoch: 2972 / 5000\n",
            "w1: [24.53752805] w2: [-22.16846492] bias: [16.41343171] loss: 30.710893724006308\n",
            "Epoch: 2973 / 5000\n",
            "w1: [24.53858733] w2: [-22.17001814] bias: [16.4133472] loss: 30.71053905079532\n",
            "Epoch: 2974 / 5000\n",
            "w1: [24.53964572] w2: [-22.17156981] bias: [16.41326269] loss: 30.710185047262264\n",
            "Epoch: 2975 / 5000\n",
            "w1: [24.54070323] w2: [-22.17311994] bias: [16.41317818] loss: 30.709831712132456\n",
            "Epoch: 2976 / 5000\n",
            "w1: [24.54175984] w2: [-22.17466852] bias: [16.41309366] loss: 30.709479044133648\n",
            "Epoch: 2977 / 5000\n",
            "w1: [24.54281557] w2: [-22.17621556] bias: [16.41300914] loss: 30.709127041996016\n",
            "Epoch: 2978 / 5000\n",
            "w1: [24.54387041] w2: [-22.17776106] bias: [16.41292461] loss: 30.70877570445216\n",
            "Epoch: 2979 / 5000\n",
            "w1: [24.54492436] w2: [-22.17930502] bias: [16.41284009] loss: 30.708425030237088\n",
            "Epoch: 2980 / 5000\n",
            "w1: [24.54597743] w2: [-22.18084744] bias: [16.41275556] loss: 30.708075018088206\n",
            "Epoch: 2981 / 5000\n",
            "w1: [24.54702962] w2: [-22.18238832] bias: [16.41267103] loss: 30.70772566674536\n",
            "Epoch: 2982 / 5000\n",
            "w1: [24.54808092] w2: [-22.18392767] bias: [16.41258649] loss: 30.707376974950762\n",
            "Epoch: 2983 / 5000\n",
            "w1: [24.54913135] w2: [-22.18546548] bias: [16.41250196] loss: 30.707028941449053\n",
            "Epoch: 2984 / 5000\n",
            "w1: [24.55018089] w2: [-22.18700176] bias: [16.41241742] loss: 30.70668156498724\n",
            "Epoch: 2985 / 5000\n",
            "w1: [24.55122955] w2: [-22.18853651] bias: [16.41233287] loss: 30.706334844314725\n",
            "Epoch: 2986 / 5000\n",
            "w1: [24.55227733] w2: [-22.19006974] bias: [16.41224833] loss: 30.705988778183304\n",
            "Epoch: 2987 / 5000\n",
            "w1: [24.55332423] w2: [-22.19160143] bias: [16.41216378] loss: 30.70564336534713\n",
            "Epoch: 2988 / 5000\n",
            "w1: [24.55437026] w2: [-22.19313159] bias: [16.41207923] loss: 30.70529860456276\n",
            "Epoch: 2989 / 5000\n",
            "w1: [24.5554154] w2: [-22.19466024] bias: [16.41199468] loss: 30.704954494589092\n",
            "Epoch: 2990 / 5000\n",
            "w1: [24.55645967] w2: [-22.19618735] bias: [16.41191012] loss: 30.704611034187405\n",
            "Epoch: 2991 / 5000\n",
            "w1: [24.55750307] w2: [-22.19771295] bias: [16.41182556] loss: 30.704268222121332\n",
            "Epoch: 2992 / 5000\n",
            "w1: [24.55854559] w2: [-22.19923702] bias: [16.411741] loss: 30.70392605715686\n",
            "Epoch: 2993 / 5000\n",
            "w1: [24.55958724] w2: [-22.20075958] bias: [16.41165644] loss: 30.70358453806235\n",
            "Epoch: 2994 / 5000\n",
            "w1: [24.56062801] w2: [-22.20228062] bias: [16.41157187] loss: 30.70324366360847\n",
            "Epoch: 2995 / 5000\n",
            "w1: [24.56166791] w2: [-22.20380014] bias: [16.4114873] loss: 30.702903432568267\n",
            "Epoch: 2996 / 5000\n",
            "w1: [24.56270694] w2: [-22.20531815] bias: [16.41140273] loss: 30.702563843717105\n",
            "Epoch: 2997 / 5000\n",
            "w1: [24.5637451] w2: [-22.20683464] bias: [16.41131816] loss: 30.702224895832686\n",
            "Epoch: 2998 / 5000\n",
            "w1: [24.56478239] w2: [-22.20834962] bias: [16.41123358] loss: 30.701886587695046\n",
            "Epoch: 2999 / 5000\n",
            "w1: [24.56581881] w2: [-22.20986309] bias: [16.41114901] loss: 30.70154891808654\n",
            "Epoch: 3000 / 5000\n",
            "w1: [24.56685437] w2: [-22.21137505] bias: [16.41106442] loss: 30.70121188579186\n",
            "Epoch: 3001 / 5000\n",
            "w1: [24.56788905] w2: [-22.21288551] bias: [16.41097984] loss: 30.700875489597983\n",
            "Epoch: 3002 / 5000\n",
            "w1: [24.56892287] w2: [-22.21439445] bias: [16.41089525] loss: 30.700539728294224\n",
            "Epoch: 3003 / 5000\n",
            "w1: [24.56995582] w2: [-22.2159019] bias: [16.41081067] loss: 30.700204600672187\n",
            "Epoch: 3004 / 5000\n",
            "w1: [24.57098791] w2: [-22.21740784] bias: [16.41072608] loss: 30.6998701055258\n",
            "Epoch: 3005 / 5000\n",
            "w1: [24.57201914] w2: [-22.21891227] bias: [16.41064148] loss: 30.699536241651266\n",
            "Epoch: 3006 / 5000\n",
            "w1: [24.5730495] w2: [-22.22041521] bias: [16.41055689] loss: 30.699203007847096\n",
            "Epoch: 3007 / 5000\n",
            "w1: [24.57407899] w2: [-22.22191665] bias: [16.41047229] loss: 30.698870402914096\n",
            "Epoch: 3008 / 5000\n",
            "w1: [24.57510763] w2: [-22.22341659] bias: [16.41038769] loss: 30.698538425655325\n",
            "Epoch: 3009 / 5000\n",
            "w1: [24.57613541] w2: [-22.22491503] bias: [16.41030309] loss: 30.698207074876176\n",
            "Epoch: 3010 / 5000\n",
            "w1: [24.57716232] w2: [-22.22641198] bias: [16.41021848] loss: 30.697876349384273\n",
            "Epoch: 3011 / 5000\n",
            "w1: [24.57818838] w2: [-22.22790744] bias: [16.41013387] loss: 30.697546247989518\n",
            "Epoch: 3012 / 5000\n",
            "w1: [24.57921358] w2: [-22.2294014] bias: [16.41004926] loss: 30.697216769504106\n",
            "Epoch: 3013 / 5000\n",
            "w1: [24.58023792] w2: [-22.23089388] bias: [16.40996465] loss: 30.69688791274248\n",
            "Epoch: 3014 / 5000\n",
            "w1: [24.5812614] w2: [-22.23238486] bias: [16.40988004] loss: 30.696559676521325\n",
            "Epoch: 3015 / 5000\n",
            "w1: [24.58228403] w2: [-22.23387436] bias: [16.40979542] loss: 30.69623205965962\n",
            "Epoch: 3016 / 5000\n",
            "w1: [24.5833058] w2: [-22.23536237] bias: [16.4097108] loss: 30.69590506097856\n",
            "Epoch: 3017 / 5000\n",
            "w1: [24.58432672] w2: [-22.2368489] bias: [16.40962618] loss: 30.6955786793016\n",
            "Epoch: 3018 / 5000\n",
            "w1: [24.58534678] w2: [-22.23833394] bias: [16.40954156] loss: 30.69525291345444\n",
            "Epoch: 3019 / 5000\n",
            "w1: [24.58636599] w2: [-22.23981751] bias: [16.40945693] loss: 30.69492776226501\n",
            "Epoch: 3020 / 5000\n",
            "w1: [24.58738435] w2: [-22.24129959] bias: [16.4093723] loss: 30.694603224563476\n",
            "Epoch: 3021 / 5000\n",
            "w1: [24.58840186] w2: [-22.24278019] bias: [16.40928767] loss: 30.694279299182238\n",
            "Epoch: 3022 / 5000\n",
            "w1: [24.58941852] w2: [-22.24425932] bias: [16.40920304] loss: 30.69395598495591\n",
            "Epoch: 3023 / 5000\n",
            "w1: [24.59043433] w2: [-22.24573696] bias: [16.40911841] loss: 30.693633280721347\n",
            "Epoch: 3024 / 5000\n",
            "w1: [24.59144929] w2: [-22.24721314] bias: [16.40903377] loss: 30.693311185317594\n",
            "Epoch: 3025 / 5000\n",
            "w1: [24.5924634] w2: [-22.24868784] bias: [16.40894913] loss: 30.692989697585922\n",
            "Epoch: 3026 / 5000\n",
            "w1: [24.59347666] w2: [-22.25016107] bias: [16.40886449] loss: 30.69266881636982\n",
            "Epoch: 3027 / 5000\n",
            "w1: [24.59448908] w2: [-22.25163283] bias: [16.40877985] loss: 30.69234854051497\n",
            "Epoch: 3028 / 5000\n",
            "w1: [24.59550065] w2: [-22.25310312] bias: [16.40869521] loss: 30.692028868869237\n",
            "Epoch: 3029 / 5000\n",
            "w1: [24.59651137] w2: [-22.25457194] bias: [16.40861056] loss: 30.691709800282716\n",
            "Epoch: 3030 / 5000\n",
            "w1: [24.59752126] w2: [-22.25603929] bias: [16.40852591] loss: 30.691391333607676\n",
            "Epoch: 3031 / 5000\n",
            "w1: [24.5985303] w2: [-22.25750518] bias: [16.40844126] loss: 30.69107346769856\n",
            "Epoch: 3032 / 5000\n",
            "w1: [24.59953849] w2: [-22.25896961] bias: [16.40835661] loss: 30.690756201412015\n",
            "Epoch: 3033 / 5000\n",
            "w1: [24.60054585] w2: [-22.26043258] bias: [16.40827195] loss: 30.69043953360685\n",
            "Epoch: 3034 / 5000\n",
            "w1: [24.60155236] w2: [-22.26189408] bias: [16.40818729] loss: 30.69012346314407\n",
            "Epoch: 3035 / 5000\n",
            "w1: [24.60255803] w2: [-22.26335413] bias: [16.40810263] loss: 30.689807988886827\n",
            "Epoch: 3036 / 5000\n",
            "w1: [24.60356287] w2: [-22.26481271] bias: [16.40801797] loss: 30.689493109700457\n",
            "Epoch: 3037 / 5000\n",
            "w1: [24.60456686] w2: [-22.26626984] bias: [16.40793331] loss: 30.689178824452444\n",
            "Epoch: 3038 / 5000\n",
            "w1: [24.60557002] w2: [-22.26772552] bias: [16.40784864] loss: 30.688865132012445\n",
            "Epoch: 3039 / 5000\n",
            "w1: [24.60657234] w2: [-22.26917974] bias: [16.40776398] loss: 30.688552031252257\n",
            "Epoch: 3040 / 5000\n",
            "w1: [24.60757383] w2: [-22.27063251] bias: [16.40767931] loss: 30.688239521045833\n",
            "Epoch: 3041 / 5000\n",
            "w1: [24.60857447] w2: [-22.27208383] bias: [16.40759464] loss: 30.68792760026927\n",
            "Epoch: 3042 / 5000\n",
            "w1: [24.60957429] w2: [-22.2735337] bias: [16.40750996] loss: 30.687616267800824\n",
            "Epoch: 3043 / 5000\n",
            "w1: [24.61057327] w2: [-22.27498212] bias: [16.40742529] loss: 30.68730552252084\n",
            "Epoch: 3044 / 5000\n",
            "w1: [24.61157142] w2: [-22.2764291] bias: [16.40734061] loss: 30.686995363311862\n",
            "Epoch: 3045 / 5000\n",
            "w1: [24.61256873] w2: [-22.27787463] bias: [16.40725593] loss: 30.686685789058515\n",
            "Epoch: 3046 / 5000\n",
            "w1: [24.61356521] w2: [-22.27931871] bias: [16.40717125] loss: 30.686376798647565\n",
            "Epoch: 3047 / 5000\n",
            "w1: [24.61456087] w2: [-22.28076135] bias: [16.40708657] loss: 30.68606839096791\n",
            "Epoch: 3048 / 5000\n",
            "w1: [24.61555569] w2: [-22.28220256] bias: [16.40700189] loss: 30.685760564910545\n",
            "Epoch: 3049 / 5000\n",
            "w1: [24.61654968] w2: [-22.28364232] bias: [16.4069172] loss: 30.685453319368577\n",
            "Epoch: 3050 / 5000\n",
            "w1: [24.61754284] w2: [-22.28508064] bias: [16.40683251] loss: 30.685146653237258\n",
            "Epoch: 3051 / 5000\n",
            "w1: [24.61853518] w2: [-22.28651753] bias: [16.40674782] loss: 30.6848405654139\n",
            "Epoch: 3052 / 5000\n",
            "w1: [24.61952669] w2: [-22.28795298] bias: [16.40666313] loss: 30.68453505479795\n",
            "Epoch: 3053 / 5000\n",
            "w1: [24.62051737] w2: [-22.289387] bias: [16.40657844] loss: 30.684230120290927\n",
            "Epoch: 3054 / 5000\n",
            "w1: [24.62150723] w2: [-22.29081958] bias: [16.40649374] loss: 30.68392576079647\n",
            "Epoch: 3055 / 5000\n",
            "w1: [24.62249627] w2: [-22.29225073] bias: [16.40640905] loss: 30.68362197522028\n",
            "Epoch: 3056 / 5000\n",
            "w1: [24.62348448] w2: [-22.29368046] bias: [16.40632435] loss: 30.68331876247015\n",
            "Epoch: 3057 / 5000\n",
            "w1: [24.62447186] w2: [-22.29510875] bias: [16.40623965] loss: 30.683016121455978\n",
            "Epoch: 3058 / 5000\n",
            "w1: [24.62545843] w2: [-22.29653562] bias: [16.40615495] loss: 30.682714051089707\n",
            "Epoch: 3059 / 5000\n",
            "w1: [24.62644417] w2: [-22.29796106] bias: [16.40607024] loss: 30.682412550285367\n",
            "Epoch: 3060 / 5000\n",
            "w1: [24.62742909] w2: [-22.29938507] bias: [16.40598554] loss: 30.682111617959062\n",
            "Epoch: 3061 / 5000\n",
            "w1: [24.62841319] w2: [-22.30080766] bias: [16.40590083] loss: 30.681811253028958\n",
            "Epoch: 3062 / 5000\n",
            "w1: [24.62939648] w2: [-22.30222883] bias: [16.40581612] loss: 30.681511454415276\n",
            "Epoch: 3063 / 5000\n",
            "w1: [24.63037894] w2: [-22.30364858] bias: [16.40573141] loss: 30.6812122210403\n",
            "Epoch: 3064 / 5000\n",
            "w1: [24.63136059] w2: [-22.30506691] bias: [16.4056467] loss: 30.68091355182838\n",
            "Epoch: 3065 / 5000\n",
            "w1: [24.63234142] w2: [-22.30648383] bias: [16.40556199] loss: 30.680615445705886\n",
            "Epoch: 3066 / 5000\n",
            "w1: [24.63332143] w2: [-22.30789932] bias: [16.40547727] loss: 30.68031790160126\n",
            "Epoch: 3067 / 5000\n",
            "w1: [24.63430063] w2: [-22.30931341] bias: [16.40539256] loss: 30.680020918444974\n",
            "Epoch: 3068 / 5000\n",
            "w1: [24.63527901] w2: [-22.31072607] bias: [16.40530784] loss: 30.679724495169545\n",
            "Epoch: 3069 / 5000\n",
            "w1: [24.63625658] w2: [-22.31213733] bias: [16.40522312] loss: 30.679428630709513\n",
            "Epoch: 3070 / 5000\n",
            "w1: [24.63723333] w2: [-22.31354717] bias: [16.4051384] loss: 30.67913332400146\n",
            "Epoch: 3071 / 5000\n",
            "w1: [24.63820928] w2: [-22.31495561] bias: [16.40505367] loss: 30.67883857398399\n",
            "Epoch: 3072 / 5000\n",
            "w1: [24.63918441] w2: [-22.31636263] bias: [16.40496895] loss: 30.67854437959773\n",
            "Epoch: 3073 / 5000\n",
            "w1: [24.64015873] w2: [-22.31776825] bias: [16.40488422] loss: 30.67825073978533\n",
            "Epoch: 3074 / 5000\n",
            "w1: [24.64113224] w2: [-22.31917247] bias: [16.40479949] loss: 30.677957653491436\n",
            "Epoch: 3075 / 5000\n",
            "w1: [24.64210494] w2: [-22.32057528] bias: [16.40471477] loss: 30.677665119662727\n",
            "Epoch: 3076 / 5000\n",
            "w1: [24.64307684] w2: [-22.32197668] bias: [16.40463003] loss: 30.677373137247873\n",
            "Epoch: 3077 / 5000\n",
            "w1: [24.64404792] w2: [-22.32337669] bias: [16.4045453] loss: 30.677081705197576\n",
            "Epoch: 3078 / 5000\n",
            "w1: [24.6450182] w2: [-22.32477529] bias: [16.40446057] loss: 30.676790822464497\n",
            "Epoch: 3079 / 5000\n",
            "w1: [24.64598767] w2: [-22.3261725] bias: [16.40437583] loss: 30.676500488003317\n",
            "Epoch: 3080 / 5000\n",
            "w1: [24.64695634] w2: [-22.32756831] bias: [16.4042911] loss: 30.676210700770703\n",
            "Epoch: 3081 / 5000\n",
            "w1: [24.6479242] w2: [-22.32896272] bias: [16.40420636] loss: 30.675921459725316\n",
            "Epoch: 3082 / 5000\n",
            "w1: [24.64889126] w2: [-22.33035573] bias: [16.40412162] loss: 30.675632763827792\n",
            "Epoch: 3083 / 5000\n",
            "w1: [24.64985751] w2: [-22.33174736] bias: [16.40403688] loss: 30.67534461204076\n",
            "Epoch: 3084 / 5000\n",
            "w1: [24.65082296] w2: [-22.33313759] bias: [16.40395214] loss: 30.675057003328803\n",
            "Epoch: 3085 / 5000\n",
            "w1: [24.65178761] w2: [-22.33452643] bias: [16.4038674] loss: 30.67476993665851\n",
            "Epoch: 3086 / 5000\n",
            "w1: [24.65275146] w2: [-22.33591388] bias: [16.40378265] loss: 30.674483410998413\n",
            "Epoch: 3087 / 5000\n",
            "w1: [24.65371451] w2: [-22.33729994] bias: [16.4036979] loss: 30.674197425319015\n",
            "Epoch: 3088 / 5000\n",
            "w1: [24.65467675] w2: [-22.33868462] bias: [16.40361316] loss: 30.673911978592784\n",
            "Epoch: 3089 / 5000\n",
            "w1: [24.6556382] w2: [-22.34006791] bias: [16.40352841] loss: 30.673627069794147\n",
            "Epoch: 3090 / 5000\n",
            "w1: [24.65659885] w2: [-22.34144982] bias: [16.40344366] loss: 30.673342697899486\n",
            "Epoch: 3091 / 5000\n",
            "w1: [24.65755871] w2: [-22.34283034] bias: [16.40335891] loss: 30.673058861887128\n",
            "Epoch: 3092 / 5000\n",
            "w1: [24.65851777] w2: [-22.34420948] bias: [16.40327415] loss: 30.672775560737353\n",
            "Epoch: 3093 / 5000\n",
            "w1: [24.65947603] w2: [-22.34558724] bias: [16.4031894] loss: 30.672492793432372\n",
            "Epoch: 3094 / 5000\n",
            "w1: [24.66043349] w2: [-22.34696363] bias: [16.40310464] loss: 30.67221055895635\n",
            "Epoch: 3095 / 5000\n",
            "w1: [24.66139017] w2: [-22.34833863] bias: [16.40301989] loss: 30.67192885629539\n",
            "Epoch: 3096 / 5000\n",
            "w1: [24.66234604] w2: [-22.34971226] bias: [16.40293513] loss: 30.671647684437517\n",
            "Epoch: 3097 / 5000\n",
            "w1: [24.66330113] w2: [-22.35108451] bias: [16.40285037] loss: 30.67136704237267\n",
            "Epoch: 3098 / 5000\n",
            "w1: [24.66425542] w2: [-22.3524554] bias: [16.40276561] loss: 30.671086929092745\n",
            "Epoch: 3099 / 5000\n",
            "w1: [24.66520892] w2: [-22.3538249] bias: [16.40268085] loss: 30.670807343591537\n",
            "Epoch: 3100 / 5000\n",
            "w1: [24.66616164] w2: [-22.35519304] bias: [16.40259609] loss: 30.670528284864776\n",
            "Epoch: 3101 / 5000\n",
            "w1: [24.66711356] w2: [-22.35655981] bias: [16.40251132] loss: 30.670249751910074\n",
            "Epoch: 3102 / 5000\n",
            "w1: [24.66806469] w2: [-22.35792521] bias: [16.40242656] loss: 30.66997174372699\n",
            "Epoch: 3103 / 5000\n",
            "w1: [24.66901503] w2: [-22.35928924] bias: [16.40234179] loss: 30.669694259316962\n",
            "Epoch: 3104 / 5000\n",
            "w1: [24.66996459] w2: [-22.36065191] bias: [16.40225703] loss: 30.66941729768335\n",
            "Epoch: 3105 / 5000\n",
            "w1: [24.67091336] w2: [-22.36201321] bias: [16.40217226] loss: 30.669140857831394\n",
            "Epoch: 3106 / 5000\n",
            "w1: [24.67186134] w2: [-22.36337315] bias: [16.40208749] loss: 30.668864938768245\n",
            "Epoch: 3107 / 5000\n",
            "w1: [24.67280854] w2: [-22.36473172] bias: [16.40200272] loss: 30.668589539502953\n",
            "Epoch: 3108 / 5000\n",
            "w1: [24.67375495] w2: [-22.36608894] bias: [16.40191795] loss: 30.66831465904642\n",
            "Epoch: 3109 / 5000\n",
            "w1: [24.67470058] w2: [-22.3674448] bias: [16.40183317] loss: 30.668040296411476\n",
            "Epoch: 3110 / 5000\n",
            "w1: [24.67564542] w2: [-22.3687993] bias: [16.4017484] loss: 30.667766450612802\n",
            "Epoch: 3111 / 5000\n",
            "w1: [24.67658948] w2: [-22.37015244] bias: [16.40166362] loss: 30.667493120666972\n",
            "Epoch: 3112 / 5000\n",
            "w1: [24.67753276] w2: [-22.37150423] bias: [16.40157885] loss: 30.667220305592434\n",
            "Epoch: 3113 / 5000\n",
            "w1: [24.67847526] w2: [-22.37285466] bias: [16.40149407] loss: 30.66694800440949\n",
            "Epoch: 3114 / 5000\n",
            "w1: [24.67941698] w2: [-22.37420375] bias: [16.40140929] loss: 30.66667621614033\n",
            "Epoch: 3115 / 5000\n",
            "w1: [24.68035792] w2: [-22.37555148] bias: [16.40132452] loss: 30.66640493980899\n",
            "Epoch: 3116 / 5000\n",
            "w1: [24.68129808] w2: [-22.37689786] bias: [16.40123974] loss: 30.66613417444139\n",
            "Epoch: 3117 / 5000\n",
            "w1: [24.68223746] w2: [-22.37824289] bias: [16.40115495] loss: 30.665863919065266\n",
            "Epoch: 3118 / 5000\n",
            "w1: [24.68317607] w2: [-22.37958657] bias: [16.40107017] loss: 30.665594172710247\n",
            "Epoch: 3119 / 5000\n",
            "w1: [24.68411389] w2: [-22.38092891] bias: [16.40098539] loss: 30.665324934407778\n",
            "Epoch: 3120 / 5000\n",
            "w1: [24.68505095] w2: [-22.3822699] bias: [16.40090061] loss: 30.66505620319119\n",
            "Epoch: 3121 / 5000\n",
            "w1: [24.68598722] w2: [-22.38360955] bias: [16.40081582] loss: 30.66478797809561\n",
            "Epoch: 3122 / 5000\n",
            "w1: [24.68692272] w2: [-22.38494786] bias: [16.40073104] loss: 30.664520258158035\n",
            "Epoch: 3123 / 5000\n",
            "w1: [24.68785745] w2: [-22.38628482] bias: [16.40064625] loss: 30.664253042417286\n",
            "Epoch: 3124 / 5000\n",
            "w1: [24.6887914] w2: [-22.38762045] bias: [16.40056146] loss: 30.663986329914017\n",
            "Epoch: 3125 / 5000\n",
            "w1: [24.68972459] w2: [-22.38895474] bias: [16.40047667] loss: 30.66372011969071\n",
            "Epoch: 3126 / 5000\n",
            "w1: [24.690657] w2: [-22.39028769] bias: [16.40039188] loss: 30.66345441079167\n",
            "Epoch: 3127 / 5000\n",
            "w1: [24.69158864] w2: [-22.3916193] bias: [16.40030709] loss: 30.663189202263027\n",
            "Epoch: 3128 / 5000\n",
            "w1: [24.6925195] w2: [-22.39294958] bias: [16.4002223] loss: 30.662924493152726\n",
            "Epoch: 3129 / 5000\n",
            "w1: [24.6934496] w2: [-22.39427853] bias: [16.40013751] loss: 30.662660282510515\n",
            "Epoch: 3130 / 5000\n",
            "w1: [24.69437893] w2: [-22.39560614] bias: [16.40005272] loss: 30.66239656938798\n",
            "Epoch: 3131 / 5000\n",
            "w1: [24.6953075] w2: [-22.39693243] bias: [16.39996792] loss: 30.66213335283849\n",
            "Epoch: 3132 / 5000\n",
            "w1: [24.69623529] w2: [-22.39825738] bias: [16.39988313] loss: 30.66187063191723\n",
            "Epoch: 3133 / 5000\n",
            "w1: [24.69716232] w2: [-22.39958101] bias: [16.39979833] loss: 30.661608405681175\n",
            "Epoch: 3134 / 5000\n",
            "w1: [24.69808858] w2: [-22.40090331] bias: [16.39971354] loss: 30.6613466731891\n",
            "Epoch: 3135 / 5000\n",
            "w1: [24.69901408] w2: [-22.40222428] bias: [16.39962874] loss: 30.6610854335016\n",
            "Epoch: 3136 / 5000\n",
            "w1: [24.69993881] w2: [-22.40354393] bias: [16.39954394] loss: 30.660824685681007\n",
            "Epoch: 3137 / 5000\n",
            "w1: [24.70086278] w2: [-22.40486226] bias: [16.39945915] loss: 30.66056442879148\n",
            "Epoch: 3138 / 5000\n",
            "w1: [24.70178599] w2: [-22.40617926] bias: [16.39937435] loss: 30.660304661898962\n",
            "Epoch: 3139 / 5000\n",
            "w1: [24.70270843] w2: [-22.40749494] bias: [16.39928955] loss: 30.66004538407115\n",
            "Epoch: 3140 / 5000\n",
            "w1: [24.70363011] w2: [-22.40880931] bias: [16.39920475] loss: 30.659786594377536\n",
            "Epoch: 3141 / 5000\n",
            "w1: [24.70455103] w2: [-22.41012236] bias: [16.39911995] loss: 30.659528291889387\n",
            "Epoch: 3142 / 5000\n",
            "w1: [24.70547119] w2: [-22.41143409] bias: [16.39903514] loss: 30.659270475679726\n",
            "Epoch: 3143 / 5000\n",
            "w1: [24.70639059] w2: [-22.4127445] bias: [16.39895034] loss: 30.659013144823344\n",
            "Epoch: 3144 / 5000\n",
            "w1: [24.70730923] w2: [-22.4140536] bias: [16.39886554] loss: 30.658756298396824\n",
            "Epoch: 3145 / 5000\n",
            "w1: [24.70822712] w2: [-22.41536139] bias: [16.39878074] loss: 30.65849993547846\n",
            "Epoch: 3146 / 5000\n",
            "w1: [24.70914424] w2: [-22.41666786] bias: [16.39869593] loss: 30.658244055148344\n",
            "Epoch: 3147 / 5000\n",
            "w1: [24.71006061] w2: [-22.41797303] bias: [16.39861113] loss: 30.657988656488296\n",
            "Epoch: 3148 / 5000\n",
            "w1: [24.71097622] w2: [-22.41927688] bias: [16.39852632] loss: 30.6577337385819\n",
            "Epoch: 3149 / 5000\n",
            "w1: [24.71189108] w2: [-22.42057943] bias: [16.39844151] loss: 30.657479300514478\n",
            "Epoch: 3150 / 5000\n",
            "w1: [24.71280519] w2: [-22.42188067] bias: [16.39835671] loss: 30.6572253413731\n",
            "Epoch: 3151 / 5000\n",
            "w1: [24.71371853] w2: [-22.42318061] bias: [16.3982719] loss: 30.656971860246564\n",
            "Epoch: 3152 / 5000\n",
            "w1: [24.71463113] w2: [-22.42447924] bias: [16.39818709] loss: 30.65671885622542\n",
            "Epoch: 3153 / 5000\n",
            "w1: [24.71554297] w2: [-22.42577657] bias: [16.39810228] loss: 30.656466328401947\n",
            "Epoch: 3154 / 5000\n",
            "w1: [24.71645407] w2: [-22.42707259] bias: [16.39801747] loss: 30.65621427587015\n",
            "Epoch: 3155 / 5000\n",
            "w1: [24.71736441] w2: [-22.42836732] bias: [16.39793266] loss: 30.655962697725755\n",
            "Epoch: 3156 / 5000\n",
            "w1: [24.718274] w2: [-22.42966075] bias: [16.39784785] loss: 30.655711593066215\n",
            "Epoch: 3157 / 5000\n",
            "w1: [24.71918284] w2: [-22.43095288] bias: [16.39776304] loss: 30.655460960990723\n",
            "Epoch: 3158 / 5000\n",
            "w1: [24.72009093] w2: [-22.43224371] bias: [16.39767823] loss: 30.65521080060014\n",
            "Epoch: 3159 / 5000\n",
            "w1: [24.72099827] w2: [-22.43353325] bias: [16.39759342] loss: 30.654961110997103\n",
            "Epoch: 3160 / 5000\n",
            "w1: [24.72190487] w2: [-22.43482149] bias: [16.39750861] loss: 30.65471189128591\n",
            "Epoch: 3161 / 5000\n",
            "w1: [24.72281072] w2: [-22.43610844] bias: [16.3974238] loss: 30.654463140572584\n",
            "Epoch: 3162 / 5000\n",
            "w1: [24.72371582] w2: [-22.4373941] bias: [16.39733898] loss: 30.654214857964845\n",
            "Epoch: 3163 / 5000\n",
            "w1: [24.72462017] w2: [-22.43867847] bias: [16.39725417] loss: 30.65396704257213\n",
            "Epoch: 3164 / 5000\n",
            "w1: [24.72552379] w2: [-22.43996154] bias: [16.39716935] loss: 30.653719693505547\n",
            "Epoch: 3165 / 5000\n",
            "w1: [24.72642665] w2: [-22.44124334] bias: [16.39708454] loss: 30.65347280987793\n",
            "Epoch: 3166 / 5000\n",
            "w1: [24.72732878] w2: [-22.44252384] bias: [16.39699972] loss: 30.65322639080376\n",
            "Epoch: 3167 / 5000\n",
            "w1: [24.72823016] w2: [-22.44380306] bias: [16.39691491] loss: 30.65298043539925\n",
            "Epoch: 3168 / 5000\n",
            "w1: [24.7291308] w2: [-22.44508099] bias: [16.39683009] loss: 30.652734942782267\n",
            "Epoch: 3169 / 5000\n",
            "w1: [24.7300307] w2: [-22.44635764] bias: [16.39674528] loss: 30.65248991207238\n",
            "Epoch: 3170 / 5000\n",
            "w1: [24.73092985] w2: [-22.44763301] bias: [16.39666046] loss: 30.652245342390813\n",
            "Epoch: 3171 / 5000\n",
            "w1: [24.73182827] w2: [-22.4489071] bias: [16.39657564] loss: 30.652001232860485\n",
            "Epoch: 3172 / 5000\n",
            "w1: [24.73272595] w2: [-22.45017991] bias: [16.39649083] loss: 30.651757582605974\n",
            "Epoch: 3173 / 5000\n",
            "w1: [24.73362289] w2: [-22.45145144] bias: [16.39640601] loss: 30.65151439075354\n",
            "Epoch: 3174 / 5000\n",
            "w1: [24.73451909] w2: [-22.4527217] bias: [16.39632119] loss: 30.651271656431074\n",
            "Epoch: 3175 / 5000\n",
            "w1: [24.73541456] w2: [-22.45399067] bias: [16.39623637] loss: 30.651029378768172\n",
            "Epoch: 3176 / 5000\n",
            "w1: [24.73630928] w2: [-22.45525838] bias: [16.39615155] loss: 30.650787556896063\n",
            "Epoch: 3177 / 5000\n",
            "w1: [24.73720327] w2: [-22.45652481] bias: [16.39606674] loss: 30.650546189947633\n",
            "Epoch: 3178 / 5000\n",
            "w1: [24.73809653] w2: [-22.45778997] bias: [16.39598192] loss: 30.650305277057438\n",
            "Epoch: 3179 / 5000\n",
            "w1: [24.73898905] w2: [-22.45905386] bias: [16.3958971] loss: 30.65006481736166\n",
            "Epoch: 3180 / 5000\n",
            "w1: [24.73988084] w2: [-22.46031648] bias: [16.39581228] loss: 30.64982480999813\n",
            "Epoch: 3181 / 5000\n",
            "w1: [24.7407719] w2: [-22.46157783] bias: [16.39572746] loss: 30.649585254106338\n",
            "Epoch: 3182 / 5000\n",
            "w1: [24.74166222] w2: [-22.46283791] bias: [16.39564264] loss: 30.6493461488274\n",
            "Epoch: 3183 / 5000\n",
            "w1: [24.74255181] w2: [-22.46409673] bias: [16.39555782] loss: 30.649107493304076\n",
            "Epoch: 3184 / 5000\n",
            "w1: [24.74344067] w2: [-22.46535428] bias: [16.395473] loss: 30.648869286680757\n",
            "Epoch: 3185 / 5000\n",
            "w1: [24.7443288] w2: [-22.46661057] bias: [16.39538818] loss: 30.64863152810346\n",
            "Epoch: 3186 / 5000\n",
            "w1: [24.7452162] w2: [-22.46786559] bias: [16.39530336] loss: 30.64839421671983\n",
            "Epoch: 3187 / 5000\n",
            "w1: [24.74610287] w2: [-22.46911936] bias: [16.39521853] loss: 30.648157351679146\n",
            "Epoch: 3188 / 5000\n",
            "w1: [24.74698881] w2: [-22.47037187] bias: [16.39513371] loss: 30.64792093213229\n",
            "Epoch: 3189 / 5000\n",
            "w1: [24.74787403] w2: [-22.47162311] bias: [16.39504889] loss: 30.64768495723179\n",
            "Epoch: 3190 / 5000\n",
            "w1: [24.74875852] w2: [-22.4728731] bias: [16.39496407] loss: 30.647449426131747\n",
            "Epoch: 3191 / 5000\n",
            "w1: [24.74964228] w2: [-22.47412184] bias: [16.39487925] loss: 30.64721433798792\n",
            "Epoch: 3192 / 5000\n",
            "w1: [24.75052531] w2: [-22.47536932] bias: [16.39479443] loss: 30.646979691957643\n",
            "Epoch: 3193 / 5000\n",
            "w1: [24.75140762] w2: [-22.47661554] bias: [16.3947096] loss: 30.646745487199873\n",
            "Epoch: 3194 / 5000\n",
            "w1: [24.75228921] w2: [-22.47786051] bias: [16.39462478] loss: 30.646511722875154\n",
            "Epoch: 3195 / 5000\n",
            "w1: [24.75317007] w2: [-22.47910423] bias: [16.39453996] loss: 30.64627839814566\n",
            "Epoch: 3196 / 5000\n",
            "w1: [24.75405021] w2: [-22.4803467] bias: [16.39445514] loss: 30.646045512175125\n",
            "Epoch: 3197 / 5000\n",
            "w1: [24.75492963] w2: [-22.48158793] bias: [16.39437032] loss: 30.645813064128895\n",
            "Epoch: 3198 / 5000\n",
            "w1: [24.75580833] w2: [-22.4828279] bias: [16.39428549] loss: 30.645581053173906\n",
            "Epoch: 3199 / 5000\n",
            "w1: [24.7566863] w2: [-22.48406663] bias: [16.39420067] loss: 30.64534947847868\n",
            "Epoch: 3200 / 5000\n",
            "w1: [24.75756356] w2: [-22.48530411] bias: [16.39411585] loss: 30.64511833921332\n",
            "Epoch: 3201 / 5000\n",
            "w1: [24.75844009] w2: [-22.48654035] bias: [16.39403103] loss: 30.64488763454951\n",
            "Epoch: 3202 / 5000\n",
            "w1: [24.75931591] w2: [-22.48777534] bias: [16.3939462] loss: 30.644657363660517\n",
            "Epoch: 3203 / 5000\n",
            "w1: [24.76019101] w2: [-22.48900909] bias: [16.39386138] loss: 30.64442752572119\n",
            "Epoch: 3204 / 5000\n",
            "w1: [24.76106539] w2: [-22.4902416] bias: [16.39377656] loss: 30.644198119907937\n",
            "Epoch: 3205 / 5000\n",
            "w1: [24.76193905] w2: [-22.49147288] bias: [16.39369174] loss: 30.64396914539874\n",
            "Epoch: 3206 / 5000\n",
            "w1: [24.762812] w2: [-22.49270291] bias: [16.39360691] loss: 30.643740601373135\n",
            "Epoch: 3207 / 5000\n",
            "w1: [24.76368423] w2: [-22.49393171] bias: [16.39352209] loss: 30.64351248701225\n",
            "Epoch: 3208 / 5000\n",
            "w1: [24.76455575] w2: [-22.49515927] bias: [16.39343727] loss: 30.64328480149875\n",
            "Epoch: 3209 / 5000\n",
            "w1: [24.76542655] w2: [-22.4963856] bias: [16.39335245] loss: 30.64305754401687\n",
            "Epoch: 3210 / 5000\n",
            "w1: [24.76629664] w2: [-22.49761069] bias: [16.39326762] loss: 30.642830713752385\n",
            "Epoch: 3211 / 5000\n",
            "w1: [24.76716601] w2: [-22.49883455] bias: [16.3931828] loss: 30.64260430989264\n",
            "Epoch: 3212 / 5000\n",
            "w1: [24.76803468] w2: [-22.50005718] bias: [16.39309798] loss: 30.64237833162651\n",
            "Epoch: 3213 / 5000\n",
            "w1: [24.76890263] w2: [-22.50127858] bias: [16.39301316] loss: 30.642152778144418\n",
            "Epoch: 3214 / 5000\n",
            "w1: [24.76976987] w2: [-22.50249875] bias: [16.39292834] loss: 30.641927648638358\n",
            "Epoch: 3215 / 5000\n",
            "w1: [24.7706364] w2: [-22.5037177] bias: [16.39284351] loss: 30.641702942301823\n",
            "Epoch: 3216 / 5000\n",
            "w1: [24.77150223] w2: [-22.50493541] bias: [16.39275869] loss: 30.64147865832986\n",
            "Epoch: 3217 / 5000\n",
            "w1: [24.77236734] w2: [-22.50615191] bias: [16.39267387] loss: 30.641254795919068\n",
            "Epoch: 3218 / 5000\n",
            "w1: [24.77323174] w2: [-22.50736717] bias: [16.39258905] loss: 30.64103135426754\n",
            "Epoch: 3219 / 5000\n",
            "w1: [24.77409544] w2: [-22.50858122] bias: [16.39250423] loss: 30.640808332574935\n",
            "Epoch: 3220 / 5000\n",
            "w1: [24.77495843] w2: [-22.50979404] bias: [16.39241941] loss: 30.640585730042407\n",
            "Epoch: 3221 / 5000\n",
            "w1: [24.77582071] w2: [-22.51100565] bias: [16.39233459] loss: 30.64036354587263\n",
            "Epoch: 3222 / 5000\n",
            "w1: [24.77668229] w2: [-22.51221603] bias: [16.39224977] loss: 30.640141779269843\n",
            "Epoch: 3223 / 5000\n",
            "w1: [24.77754316] w2: [-22.51342519] bias: [16.39216495] loss: 30.639920429439748\n",
            "Epoch: 3224 / 5000\n",
            "w1: [24.77840333] w2: [-22.51463314] bias: [16.39208012] loss: 30.63969949558959\n",
            "Epoch: 3225 / 5000\n",
            "w1: [24.7792628] w2: [-22.51583988] bias: [16.3919953] loss: 30.63947897692811\n",
            "Epoch: 3226 / 5000\n",
            "w1: [24.78012156] w2: [-22.51704539] bias: [16.39191049] loss: 30.63925887266556\n",
            "Epoch: 3227 / 5000\n",
            "w1: [24.78097962] w2: [-22.5182497] bias: [16.39182567] loss: 30.63903918201372\n",
            "Epoch: 3228 / 5000\n",
            "w1: [24.78183698] w2: [-22.51945279] bias: [16.39174085] loss: 30.638819904185826\n",
            "Epoch: 3229 / 5000\n",
            "w1: [24.78269363] w2: [-22.52065467] bias: [16.39165603] loss: 30.63860103839665\n",
            "Epoch: 3230 / 5000\n",
            "w1: [24.78354959] w2: [-22.52185534] bias: [16.39157121] loss: 30.638382583862448\n",
            "Epoch: 3231 / 5000\n",
            "w1: [24.78440484] w2: [-22.5230548] bias: [16.39148639] loss: 30.638164539800975\n",
            "Epoch: 3232 / 5000\n",
            "w1: [24.7852594] w2: [-22.52425306] bias: [16.39140157] loss: 30.63794690543147\n",
            "Epoch: 3233 / 5000\n",
            "w1: [24.78611326] w2: [-22.52545011] bias: [16.39131675] loss: 30.637729679974647\n",
            "Epoch: 3234 / 5000\n",
            "w1: [24.78696642] w2: [-22.52664595] bias: [16.39123194] loss: 30.63751286265274\n",
            "Epoch: 3235 / 5000\n",
            "w1: [24.78781889] w2: [-22.52784059] bias: [16.39114712] loss: 30.63729645268943\n",
            "Epoch: 3236 / 5000\n",
            "w1: [24.78867065] w2: [-22.52903402] bias: [16.3910623] loss: 30.637080449309906\n",
            "Epoch: 3237 / 5000\n",
            "w1: [24.78952173] w2: [-22.53022625] bias: [16.39097749] loss: 30.636864851740803\n",
            "Epoch: 3238 / 5000\n",
            "w1: [24.7903721] w2: [-22.53141729] bias: [16.39089267] loss: 30.63664965921025\n",
            "Epoch: 3239 / 5000\n",
            "w1: [24.79122178] w2: [-22.53260712] bias: [16.39080786] loss: 30.636434870947845\n",
            "Epoch: 3240 / 5000\n",
            "w1: [24.79207077] w2: [-22.53379575] bias: [16.39072304] loss: 30.636220486184648\n",
            "Epoch: 3241 / 5000\n",
            "w1: [24.79291907] w2: [-22.53498319] bias: [16.39063823] loss: 30.63600650415319\n",
            "Epoch: 3242 / 5000\n",
            "w1: [24.79376667] w2: [-22.53616943] bias: [16.39055341] loss: 30.635792924087465\n",
            "Epoch: 3243 / 5000\n",
            "w1: [24.79461358] w2: [-22.53735448] bias: [16.3904686] loss: 30.635579745222916\n",
            "Epoch: 3244 / 5000\n",
            "w1: [24.7954598] w2: [-22.53853833] bias: [16.39038378] loss: 30.635366966796447\n",
            "Epoch: 3245 / 5000\n",
            "w1: [24.79630533] w2: [-22.53972099] bias: [16.39029897] loss: 30.635154588046426\n",
            "Epoch: 3246 / 5000\n",
            "w1: [24.79715017] w2: [-22.54090246] bias: [16.39021416] loss: 30.63494260821266\n",
            "Epoch: 3247 / 5000\n",
            "w1: [24.79799431] w2: [-22.54208274] bias: [16.39012934] loss: 30.63473102653641\n",
            "Epoch: 3248 / 5000\n",
            "w1: [24.79883777] w2: [-22.54326182] bias: [16.39004453] loss: 30.634519842260396\n",
            "Epoch: 3249 / 5000\n",
            "w1: [24.79968055] w2: [-22.54443972] bias: [16.38995972] loss: 30.634309054628748\n",
            "Epoch: 3250 / 5000\n",
            "w1: [24.80052263] w2: [-22.54561644] bias: [16.38987491] loss: 30.63409866288706\n",
            "Epoch: 3251 / 5000\n",
            "w1: [24.80136403] w2: [-22.54679196] bias: [16.3897901] loss: 30.633888666282363\n",
            "Epoch: 3252 / 5000\n",
            "w1: [24.80220474] w2: [-22.54796631] bias: [16.38970529] loss: 30.633679064063113\n",
            "Epoch: 3253 / 5000\n",
            "w1: [24.80304476] w2: [-22.54913947] bias: [16.38962048] loss: 30.63346985547921\n",
            "Epoch: 3254 / 5000\n",
            "w1: [24.80388411] w2: [-22.55031144] bias: [16.38953567] loss: 30.633261039781967\n",
            "Epoch: 3255 / 5000\n",
            "w1: [24.80472276] w2: [-22.55148224] bias: [16.38945086] loss: 30.63305261622415\n",
            "Epoch: 3256 / 5000\n",
            "w1: [24.80556074] w2: [-22.55265185] bias: [16.38936606] loss: 30.63284458405991\n",
            "Epoch: 3257 / 5000\n",
            "w1: [24.80639803] w2: [-22.55382029] bias: [16.38928125] loss: 30.63263694254486\n",
            "Epoch: 3258 / 5000\n",
            "w1: [24.80723463] w2: [-22.55498755] bias: [16.38919644] loss: 30.632429690936004\n",
            "Epoch: 3259 / 5000\n",
            "w1: [24.80807056] w2: [-22.55615363] bias: [16.38911164] loss: 30.63222282849177\n",
            "Epoch: 3260 / 5000\n",
            "w1: [24.8089058] w2: [-22.55731854] bias: [16.38902683] loss: 30.632016354471997\n",
            "Epoch: 3261 / 5000\n",
            "w1: [24.80974037] w2: [-22.55848227] bias: [16.38894203] loss: 30.631810268137944\n",
            "Epoch: 3262 / 5000\n",
            "w1: [24.81057425] w2: [-22.55964482] bias: [16.38885722] loss: 30.63160456875226\n",
            "Epoch: 3263 / 5000\n",
            "w1: [24.81140746] w2: [-22.56080621] bias: [16.38877242] loss: 30.631399255579016\n",
            "Epoch: 3264 / 5000\n",
            "w1: [24.81223998] w2: [-22.56196643] bias: [16.38868762] loss: 30.631194327883676\n",
            "Epoch: 3265 / 5000\n",
            "w1: [24.81307183] w2: [-22.56312547] bias: [16.38860281] loss: 30.630989784933103\n",
            "Epoch: 3266 / 5000\n",
            "w1: [24.813903] w2: [-22.56428335] bias: [16.38851801] loss: 30.630785625995568\n",
            "Epoch: 3267 / 5000\n",
            "w1: [24.8147335] w2: [-22.56544005] bias: [16.38843321] loss: 30.630581850340718\n",
            "Epoch: 3268 / 5000\n",
            "w1: [24.81556332] w2: [-22.5665956] bias: [16.38834841] loss: 30.630378457239605\n",
            "Epoch: 3269 / 5000\n",
            "w1: [24.81639246] w2: [-22.56774997] bias: [16.38826361] loss: 30.630175445964667\n",
            "Epoch: 3270 / 5000\n",
            "w1: [24.81722093] w2: [-22.56890318] bias: [16.38817881] loss: 30.62997281578973\n",
            "Epoch: 3271 / 5000\n",
            "w1: [24.81804872] w2: [-22.57005523] bias: [16.38809401] loss: 30.62977056598999\n",
            "Epoch: 3272 / 5000\n",
            "w1: [24.81887584] w2: [-22.57120612] bias: [16.38800922] loss: 30.629568695842053\n",
            "Epoch: 3273 / 5000\n",
            "w1: [24.81970229] w2: [-22.57235584] bias: [16.38792442] loss: 30.62936720462387\n",
            "Epoch: 3274 / 5000\n",
            "w1: [24.82052807] w2: [-22.57350441] bias: [16.38783962] loss: 30.629166091614795\n",
            "Epoch: 3275 / 5000\n",
            "w1: [24.82135317] w2: [-22.57465182] bias: [16.38775483] loss: 30.628965356095534\n",
            "Epoch: 3276 / 5000\n",
            "w1: [24.82217761] w2: [-22.57579806] bias: [16.38767003] loss: 30.628764997348174\n",
            "Epoch: 3277 / 5000\n",
            "w1: [24.82300137] w2: [-22.57694316] bias: [16.38758524] loss: 30.62856501465618\n",
            "Epoch: 3278 / 5000\n",
            "w1: [24.82382446] w2: [-22.57808709] bias: [16.38750045] loss: 30.62836540730435\n",
            "Epoch: 3279 / 5000\n",
            "w1: [24.82464688] w2: [-22.57922988] bias: [16.38741566] loss: 30.628166174578887\n",
            "Epoch: 3280 / 5000\n",
            "w1: [24.82546864] w2: [-22.58037151] bias: [16.38733086] loss: 30.627967315767318\n",
            "Epoch: 3281 / 5000\n",
            "w1: [24.82628973] w2: [-22.58151198] bias: [16.38724607] loss: 30.627768830158555\n",
            "Epoch: 3282 / 5000\n",
            "w1: [24.82711015] w2: [-22.58265131] bias: [16.38716128] loss: 30.627570717042843\n",
            "Epoch: 3283 / 5000\n",
            "w1: [24.8279299] w2: [-22.58378948] bias: [16.3870765] loss: 30.62737297571179\n",
            "Epoch: 3284 / 5000\n",
            "w1: [24.82874899] w2: [-22.58492651] bias: [16.38699171] loss: 30.627175605458355\n",
            "Epoch: 3285 / 5000\n",
            "w1: [24.82956741] w2: [-22.58606239] bias: [16.38690692] loss: 30.626978605576838\n",
            "Epoch: 3286 / 5000\n",
            "w1: [24.83038516] w2: [-22.58719712] bias: [16.38682213] loss: 30.626781975362892\n",
            "Epoch: 3287 / 5000\n",
            "w1: [24.83120226] w2: [-22.58833071] bias: [16.38673735] loss: 30.626585714113496\n",
            "Epoch: 3288 / 5000\n",
            "w1: [24.83201869] w2: [-22.58946316] bias: [16.38665256] loss: 30.626389821127\n",
            "Epoch: 3289 / 5000\n",
            "w1: [24.83283445] w2: [-22.59059446] bias: [16.38656778] loss: 30.62619429570306\n",
            "Epoch: 3290 / 5000\n",
            "w1: [24.83364956] w2: [-22.59172461] bias: [16.386483] loss: 30.62599913714266\n",
            "Epoch: 3291 / 5000\n",
            "w1: [24.834464] w2: [-22.59285363] bias: [16.38639821] loss: 30.625804344748154\n",
            "Epoch: 3292 / 5000\n",
            "w1: [24.83527778] w2: [-22.59398151] bias: [16.38631343] loss: 30.625609917823205\n",
            "Epoch: 3293 / 5000\n",
            "w1: [24.8360909] w2: [-22.59510824] bias: [16.38622865] loss: 30.625415855672784\n",
            "Epoch: 3294 / 5000\n",
            "w1: [24.83690336] w2: [-22.59623384] bias: [16.38614387] loss: 30.625222157603226\n",
            "Epoch: 3295 / 5000\n",
            "w1: [24.83771516] w2: [-22.59735831] bias: [16.3860591] loss: 30.625028822922143\n",
            "Epoch: 3296 / 5000\n",
            "w1: [24.8385263] w2: [-22.59848163] bias: [16.38597432] loss: 30.624835850938506\n",
            "Epoch: 3297 / 5000\n",
            "w1: [24.83933679] w2: [-22.59960383] bias: [16.38588954] loss: 30.624643240962584\n",
            "Epoch: 3298 / 5000\n",
            "w1: [24.84014661] w2: [-22.60072489] bias: [16.38580477] loss: 30.624450992305952\n",
            "Epoch: 3299 / 5000\n",
            "w1: [24.84095578] w2: [-22.60184481] bias: [16.38571999] loss: 30.62425910428152\n",
            "Epoch: 3300 / 5000\n",
            "w1: [24.8417643] w2: [-22.60296361] bias: [16.38563522] loss: 30.62406757620348\n",
            "Epoch: 3301 / 5000\n",
            "w1: [24.84257216] w2: [-22.60408127] bias: [16.38555045] loss: 30.623876407387357\n",
            "Epoch: 3302 / 5000\n",
            "w1: [24.84337936] w2: [-22.60519781] bias: [16.38546568] loss: 30.623685597149947\n",
            "Epoch: 3303 / 5000\n",
            "w1: [24.84418591] w2: [-22.60631322] bias: [16.38538091] loss: 30.623495144809393\n",
            "Epoch: 3304 / 5000\n",
            "w1: [24.8449918] w2: [-22.6074275] bias: [16.38529614] loss: 30.623305049685104\n",
            "Epoch: 3305 / 5000\n",
            "w1: [24.84579705] w2: [-22.60854066] bias: [16.38521137] loss: 30.623115311097784\n",
            "Epoch: 3306 / 5000\n",
            "w1: [24.84660164] w2: [-22.60965269] bias: [16.3851266] loss: 30.622925928369455\n",
            "Epoch: 3307 / 5000\n",
            "w1: [24.84740557] w2: [-22.6107636] bias: [16.38504183] loss: 30.622736900823412\n",
            "Epoch: 3308 / 5000\n",
            "w1: [24.84820886] w2: [-22.61187338] bias: [16.38495707] loss: 30.622548227784243\n",
            "Epoch: 3309 / 5000\n",
            "w1: [24.8490115] w2: [-22.61298204] bias: [16.38487231] loss: 30.622359908577824\n",
            "Epoch: 3310 / 5000\n",
            "w1: [24.84981348] w2: [-22.61408959] bias: [16.38478754] loss: 30.622171942531313\n",
            "Epoch: 3311 / 5000\n",
            "w1: [24.85061482] w2: [-22.61519601] bias: [16.38470278] loss: 30.621984328973166\n",
            "Epoch: 3312 / 5000\n",
            "w1: [24.8514155] w2: [-22.61630131] bias: [16.38461802] loss: 30.621797067233093\n",
            "Epoch: 3313 / 5000\n",
            "w1: [24.85221554] w2: [-22.6174055] bias: [16.38453326] loss: 30.6216101566421\n",
            "Epoch: 3314 / 5000\n",
            "w1: [24.85301493] w2: [-22.61850857] bias: [16.3844485] loss: 30.621423596532455\n",
            "Epoch: 3315 / 5000\n",
            "w1: [24.85381368] w2: [-22.61961053] bias: [16.38436374] loss: 30.621237386237713\n",
            "Epoch: 3316 / 5000\n",
            "w1: [24.85461177] w2: [-22.62071137] bias: [16.38427899] loss: 30.621051525092675\n",
            "Epoch: 3317 / 5000\n",
            "w1: [24.85540922] w2: [-22.6218111] bias: [16.38419423] loss: 30.62086601243344\n",
            "Epoch: 3318 / 5000\n",
            "w1: [24.85620603] w2: [-22.62290972] bias: [16.38410948] loss: 30.620680847597352\n",
            "Epoch: 3319 / 5000\n",
            "w1: [24.85700219] w2: [-22.62400723] bias: [16.38402473] loss: 30.620496029923025\n",
            "Epoch: 3320 / 5000\n",
            "w1: [24.85779771] w2: [-22.62510362] bias: [16.38393998] loss: 30.620311558750323\n",
            "Epoch: 3321 / 5000\n",
            "w1: [24.85859258] w2: [-22.62619891] bias: [16.38385523] loss: 30.620127433420375\n",
            "Epoch: 3322 / 5000\n",
            "w1: [24.85938681] w2: [-22.62729309] bias: [16.38377048] loss: 30.619943653275573\n",
            "Epoch: 3323 / 5000\n",
            "w1: [24.8601804] w2: [-22.62838617] bias: [16.38368573] loss: 30.61976021765955\n",
            "Epoch: 3324 / 5000\n",
            "w1: [24.86097335] w2: [-22.62947814] bias: [16.38360098] loss: 30.619577125917196\n",
            "Epoch: 3325 / 5000\n",
            "w1: [24.86176565] w2: [-22.630569] bias: [16.38351624] loss: 30.61939437739464\n",
            "Epoch: 3326 / 5000\n",
            "w1: [24.86255732] w2: [-22.63165876] bias: [16.38343149] loss: 30.61921197143928\n",
            "Epoch: 3327 / 5000\n",
            "w1: [24.86334834] w2: [-22.63274742] bias: [16.38334675] loss: 30.619029907399725\n",
            "Epoch: 3328 / 5000\n",
            "w1: [24.86413873] w2: [-22.63383498] bias: [16.38326201] loss: 30.61884818462585\n",
            "Epoch: 3329 / 5000\n",
            "w1: [24.86492848] w2: [-22.63492144] bias: [16.38317727] loss: 30.61866680246876\n",
            "Epoch: 3330 / 5000\n",
            "w1: [24.86571758] w2: [-22.63600679] bias: [16.38309253] loss: 30.618485760280787\n",
            "Epoch: 3331 / 5000\n",
            "w1: [24.86650606] w2: [-22.63709106] bias: [16.38300779] loss: 30.61830505741552\n",
            "Epoch: 3332 / 5000\n",
            "w1: [24.86729389] w2: [-22.63817422] bias: [16.38292305] loss: 30.618124693227756\n",
            "Epoch: 3333 / 5000\n",
            "w1: [24.86808109] w2: [-22.63925629] bias: [16.38283832] loss: 30.617944667073544\n",
            "Epoch: 3334 / 5000\n",
            "w1: [24.86886765] w2: [-22.64033726] bias: [16.38275358] loss: 30.617764978310127\n",
            "Epoch: 3335 / 5000\n",
            "w1: [24.86965358] w2: [-22.64141714] bias: [16.38266885] loss: 30.617585626296016\n",
            "Epoch: 3336 / 5000\n",
            "w1: [24.87043887] w2: [-22.64249592] bias: [16.38258412] loss: 30.61740661039091\n",
            "Epoch: 3337 / 5000\n",
            "w1: [24.87122353] w2: [-22.64357362] bias: [16.38249939] loss: 30.61722792995573\n",
            "Epoch: 3338 / 5000\n",
            "w1: [24.87200755] w2: [-22.64465022] bias: [16.38241466] loss: 30.617049584352642\n",
            "Epoch: 3339 / 5000\n",
            "w1: [24.87279094] w2: [-22.64572573] bias: [16.38232994] loss: 30.616871572944998\n",
            "Epoch: 3340 / 5000\n",
            "w1: [24.8735737] w2: [-22.64680016] bias: [16.38224521] loss: 30.616693895097377\n",
            "Epoch: 3341 / 5000\n",
            "w1: [24.87435583] w2: [-22.6478735] bias: [16.38216049] loss: 30.61651655017557\n",
            "Epoch: 3342 / 5000\n",
            "w1: [24.87513733] w2: [-22.64894575] bias: [16.38207576] loss: 30.616339537546573\n",
            "Epoch: 3343 / 5000\n",
            "w1: [24.87591819] w2: [-22.65001691] bias: [16.38199104] loss: 30.616162856578566\n",
            "Epoch: 3344 / 5000\n",
            "w1: [24.87669843] w2: [-22.651087] bias: [16.38190632] loss: 30.615986506640983\n",
            "Epoch: 3345 / 5000\n",
            "w1: [24.87747804] w2: [-22.65215599] bias: [16.3818216] loss: 30.61581048710441\n",
            "Epoch: 3346 / 5000\n",
            "w1: [24.87825702] w2: [-22.65322391] bias: [16.38173688] loss: 30.61563479734067\n",
            "Epoch: 3347 / 5000\n",
            "w1: [24.87903537] w2: [-22.65429074] bias: [16.38165217] loss: 30.615459436722748\n",
            "Epoch: 3348 / 5000\n",
            "w1: [24.87981309] w2: [-22.6553565] bias: [16.38156745] loss: 30.615284404624862\n",
            "Epoch: 3349 / 5000\n",
            "w1: [24.88059019] w2: [-22.65642117] bias: [16.38148274] loss: 30.615109700422376\n",
            "Epoch: 3350 / 5000\n",
            "w1: [24.88136665] w2: [-22.65748477] bias: [16.38139803] loss: 30.61493532349189\n",
            "Epoch: 3351 / 5000\n",
            "w1: [24.8821425] w2: [-22.65854729] bias: [16.38131332] loss: 30.614761273211162\n",
            "Epoch: 3352 / 5000\n",
            "w1: [24.88291772] w2: [-22.65960873] bias: [16.38122861] loss: 30.614587548959147\n",
            "Epoch: 3353 / 5000\n",
            "w1: [24.88369231] w2: [-22.6606691] bias: [16.3811439] loss: 30.614414150115977\n",
            "Epoch: 3354 / 5000\n",
            "w1: [24.88446628] w2: [-22.6617284] bias: [16.3810592] loss: 30.614241076062985\n",
            "Epoch: 3355 / 5000\n",
            "w1: [24.88523963] w2: [-22.66278662] bias: [16.38097449] loss: 30.61406832618264\n",
            "Epoch: 3356 / 5000\n",
            "w1: [24.88601235] w2: [-22.66384377] bias: [16.38088979] loss: 30.61389589985864\n",
            "Epoch: 3357 / 5000\n",
            "w1: [24.88678445] w2: [-22.66489984] bias: [16.38080509] loss: 30.613723796475824\n",
            "Epoch: 3358 / 5000\n",
            "w1: [24.88755593] w2: [-22.66595485] bias: [16.38072039] loss: 30.613552015420204\n",
            "Epoch: 3359 / 5000\n",
            "w1: [24.88832679] w2: [-22.66700879] bias: [16.38063569] loss: 30.613380556078972\n",
            "Epoch: 3360 / 5000\n",
            "w1: [24.88909702] w2: [-22.66806167] bias: [16.380551] loss: 30.613209417840487\n",
            "Epoch: 3361 / 5000\n",
            "w1: [24.88986664] w2: [-22.66911347] bias: [16.3804663] loss: 30.613038600094274\n",
            "Epoch: 3362 / 5000\n",
            "w1: [24.89063564] w2: [-22.67016421] bias: [16.38038161] loss: 30.61286810223101\n",
            "Epoch: 3363 / 5000\n",
            "w1: [24.89140402] w2: [-22.67121388] bias: [16.38029692] loss: 30.61269792364254\n",
            "Epoch: 3364 / 5000\n",
            "w1: [24.89217178] w2: [-22.67226249] bias: [16.38021223] loss: 30.612528063721875\n",
            "Epoch: 3365 / 5000\n",
            "w1: [24.89293892] w2: [-22.67331004] bias: [16.38012754] loss: 30.612358521863168\n",
            "Epoch: 3366 / 5000\n",
            "w1: [24.89370545] w2: [-22.67435653] bias: [16.38004285] loss: 30.612189297461747\n",
            "Epoch: 3367 / 5000\n",
            "w1: [24.89447136] w2: [-22.67540195] bias: [16.37995817] loss: 30.612020389914065\n",
            "Epoch: 3368 / 5000\n",
            "w1: [24.89523665] w2: [-22.67644632] bias: [16.37987348] loss: 30.61185179861775\n",
            "Epoch: 3369 / 5000\n",
            "w1: [24.89600133] w2: [-22.67748963] bias: [16.3797888] loss: 30.611683522971557\n",
            "Epoch: 3370 / 5000\n",
            "w1: [24.89676539] w2: [-22.67853188] bias: [16.37970412] loss: 30.611515562375406\n",
            "Epoch: 3371 / 5000\n",
            "w1: [24.89752884] w2: [-22.67957307] bias: [16.37961944] loss: 30.61134791623034\n",
            "Epoch: 3372 / 5000\n",
            "w1: [24.89829167] w2: [-22.68061321] bias: [16.37953477] loss: 30.611180583938562\n",
            "Epoch: 3373 / 5000\n",
            "w1: [24.89905389] w2: [-22.68165229] bias: [16.37945009] loss: 30.611013564903402\n",
            "Epoch: 3374 / 5000\n",
            "w1: [24.8998155] w2: [-22.68269032] bias: [16.37936542] loss: 30.610846858529335\n",
            "Epoch: 3375 / 5000\n",
            "w1: [24.9005765] w2: [-22.6837273] bias: [16.37928075] loss: 30.61068046422196\n",
            "Epoch: 3376 / 5000\n",
            "w1: [24.90133688] w2: [-22.68476322] bias: [16.37919608] loss: 30.61051438138802\n",
            "Epoch: 3377 / 5000\n",
            "w1: [24.90209666] w2: [-22.6857981] bias: [16.37911141] loss: 30.61034860943538\n",
            "Epoch: 3378 / 5000\n",
            "w1: [24.90285582] w2: [-22.68683192] bias: [16.37902674] loss: 30.610183147773036\n",
            "Epoch: 3379 / 5000\n",
            "w1: [24.90361438] w2: [-22.6878647] bias: [16.37894208] loss: 30.61001799581111\n",
            "Epoch: 3380 / 5000\n",
            "w1: [24.90437232] w2: [-22.68889643] bias: [16.37885741] loss: 30.609853152960852\n",
            "Epoch: 3381 / 5000\n",
            "w1: [24.90512966] w2: [-22.68992711] bias: [16.37877275] loss: 30.609688618634628\n",
            "Epoch: 3382 / 5000\n",
            "w1: [24.90588639] w2: [-22.69095675] bias: [16.37868809] loss: 30.609524392245923\n",
            "Epoch: 3383 / 5000\n",
            "w1: [24.90664251] w2: [-22.69198534] bias: [16.37860343] loss: 30.609360473209343\n",
            "Epoch: 3384 / 5000\n",
            "w1: [24.90739802] w2: [-22.69301289] bias: [16.37851878] loss: 30.609196860940617\n",
            "Epoch: 3385 / 5000\n",
            "w1: [24.90815293] w2: [-22.6940394] bias: [16.37843412] loss: 30.60903355485656\n",
            "Epoch: 3386 / 5000\n",
            "w1: [24.90890723] w2: [-22.69506486] bias: [16.37834947] loss: 30.608870554375123\n",
            "Epoch: 3387 / 5000\n",
            "w1: [24.90966093] w2: [-22.69608928] bias: [16.37826482] loss: 30.608707858915366\n",
            "Epoch: 3388 / 5000\n",
            "w1: [24.91041402] w2: [-22.69711267] bias: [16.37818017] loss: 30.608545467897446\n",
            "Epoch: 3389 / 5000\n",
            "w1: [24.91116651] w2: [-22.69813502] bias: [16.37809552] loss: 30.608383380742637\n",
            "Epoch: 3390 / 5000\n",
            "w1: [24.91191839] w2: [-22.69915633] bias: [16.37801088] loss: 30.60822159687329\n",
            "Epoch: 3391 / 5000\n",
            "w1: [24.91266968] w2: [-22.7001766] bias: [16.37792623] loss: 30.608060115712878\n",
            "Epoch: 3392 / 5000\n",
            "w1: [24.91342035] w2: [-22.70119584] bias: [16.37784159] loss: 30.607898936685974\n",
            "Epoch: 3393 / 5000\n",
            "w1: [24.91417043] w2: [-22.70221404] bias: [16.37775695] loss: 30.607738059218235\n",
            "Epoch: 3394 / 5000\n",
            "w1: [24.91491991] w2: [-22.70323121] bias: [16.37767232] loss: 30.607577482736424\n",
            "Epoch: 3395 / 5000\n",
            "w1: [24.91566878] w2: [-22.70424735] bias: [16.37758768] loss: 30.607417206668384\n",
            "Epoch: 3396 / 5000\n",
            "w1: [24.91641706] w2: [-22.70526245] bias: [16.37750305] loss: 30.607257230443068\n",
            "Epoch: 3397 / 5000\n",
            "w1: [24.91716473] w2: [-22.70627653] bias: [16.37741841] loss: 30.607097553490473\n",
            "Epoch: 3398 / 5000\n",
            "w1: [24.91791181] w2: [-22.70728957] bias: [16.37733378] loss: 30.606938175241744\n",
            "Epoch: 3399 / 5000\n",
            "w1: [24.91865829] w2: [-22.70830159] bias: [16.37724915] loss: 30.60677909512907\n",
            "Epoch: 3400 / 5000\n",
            "w1: [24.91940417] w2: [-22.70931258] bias: [16.37716453] loss: 30.60662031258572\n",
            "Epoch: 3401 / 5000\n",
            "w1: [24.92014945] w2: [-22.71032254] bias: [16.3770799] loss: 30.606461827046054\n",
            "Epoch: 3402 / 5000\n",
            "w1: [24.92089413] w2: [-22.71133148] bias: [16.37699528] loss: 30.606303637945523\n",
            "Epoch: 3403 / 5000\n",
            "w1: [24.92163822] w2: [-22.71233939] bias: [16.37691066] loss: 30.606145744720628\n",
            "Epoch: 3404 / 5000\n",
            "w1: [24.92238172] w2: [-22.71334628] bias: [16.37682604] loss: 30.605988146808954\n",
            "Epoch: 3405 / 5000\n",
            "w1: [24.92312461] w2: [-22.71435214] bias: [16.37674142] loss: 30.605830843649162\n",
            "Epoch: 3406 / 5000\n",
            "w1: [24.92386692] w2: [-22.71535699] bias: [16.37665681] loss: 30.60567383468097\n",
            "Epoch: 3407 / 5000\n",
            "w1: [24.92460863] w2: [-22.71636081] bias: [16.3765722] loss: 30.60551711934519\n",
            "Epoch: 3408 / 5000\n",
            "w1: [24.92534974] w2: [-22.71736362] bias: [16.37648759] loss: 30.605360697083665\n",
            "Epoch: 3409 / 5000\n",
            "w1: [24.92609026] w2: [-22.7183654] bias: [16.37640298] loss: 30.605204567339324\n",
            "Epoch: 3410 / 5000\n",
            "w1: [24.92683019] w2: [-22.71936617] bias: [16.37631837] loss: 30.605048729556152\n",
            "Epoch: 3411 / 5000\n",
            "w1: [24.92756953] w2: [-22.72036592] bias: [16.37623377] loss: 30.604893183179193\n",
            "Epoch: 3412 / 5000\n",
            "w1: [24.92830828] w2: [-22.72136466] bias: [16.37614916] loss: 30.604737927654543\n",
            "Epoch: 3413 / 5000\n",
            "w1: [24.92904644] w2: [-22.72236238] bias: [16.37606456] loss: 30.60458296242936\n",
            "Epoch: 3414 / 5000\n",
            "w1: [24.929784] w2: [-22.72335909] bias: [16.37597996] loss: 30.60442828695186\n",
            "Epoch: 3415 / 5000\n",
            "w1: [24.93052098] w2: [-22.72435478] bias: [16.37589537] loss: 30.6042739006713\n",
            "Epoch: 3416 / 5000\n",
            "w1: [24.93125737] w2: [-22.72534946] bias: [16.37581077] loss: 30.604119803037985\n",
            "Epoch: 3417 / 5000\n",
            "w1: [24.93199316] w2: [-22.72634313] bias: [16.37572618] loss: 30.603965993503273\n",
            "Epoch: 3418 / 5000\n",
            "w1: [24.93272837] w2: [-22.72733579] bias: [16.37564159] loss: 30.603812471519575\n",
            "Epoch: 3419 / 5000\n",
            "w1: [24.933463] w2: [-22.72832745] bias: [16.375557] loss: 30.60365923654033\n",
            "Epoch: 3420 / 5000\n",
            "w1: [24.93419703] w2: [-22.72931809] bias: [16.37547241] loss: 30.603506288020032\n",
            "Epoch: 3421 / 5000\n",
            "w1: [24.93493048] w2: [-22.73030773] bias: [16.37538783] loss: 30.603353625414197\n",
            "Epoch: 3422 / 5000\n",
            "w1: [24.93566335] w2: [-22.73129636] bias: [16.37530325] loss: 30.603201248179396\n",
            "Epoch: 3423 / 5000\n",
            "w1: [24.93639562] w2: [-22.73228399] bias: [16.37521867] loss: 30.603049155773224\n",
            "Epoch: 3424 / 5000\n",
            "w1: [24.93712732] w2: [-22.73327061] bias: [16.37513409] loss: 30.60289734765432\n",
            "Epoch: 3425 / 5000\n",
            "w1: [24.93785843] w2: [-22.73425623] bias: [16.37504951] loss: 30.602745823282344\n",
            "Epoch: 3426 / 5000\n",
            "w1: [24.93858895] w2: [-22.73524085] bias: [16.37496494] loss: 30.60259458211799\n",
            "Epoch: 3427 / 5000\n",
            "w1: [24.93931889] w2: [-22.73622446] bias: [16.37488037] loss: 30.602443623622975\n",
            "Epoch: 3428 / 5000\n",
            "w1: [24.94004825] w2: [-22.73720708] bias: [16.3747958] loss: 30.60229294726005\n",
            "Epoch: 3429 / 5000\n",
            "w1: [24.94077703] w2: [-22.73818869] bias: [16.37471123] loss: 30.60214255249298\n",
            "Epoch: 3430 / 5000\n",
            "w1: [24.94150523] w2: [-22.73916931] bias: [16.37462667] loss: 30.60199243878657\n",
            "Epoch: 3431 / 5000\n",
            "w1: [24.94223284] w2: [-22.74014893] bias: [16.3745421] loss: 30.601842605606613\n",
            "Epoch: 3432 / 5000\n",
            "w1: [24.94295988] w2: [-22.74112756] bias: [16.37445754] loss: 30.601693052419957\n",
            "Epoch: 3433 / 5000\n",
            "w1: [24.94368633] w2: [-22.74210519] bias: [16.37437299] loss: 30.601543778694438\n",
            "Epoch: 3434 / 5000\n",
            "w1: [24.94441221] w2: [-22.74308182] bias: [16.37428843] loss: 30.601394783898904\n",
            "Epoch: 3435 / 5000\n",
            "w1: [24.9451375] w2: [-22.74405746] bias: [16.37420388] loss: 30.601246067503237\n",
            "Epoch: 3436 / 5000\n",
            "w1: [24.94586222] w2: [-22.74503211] bias: [16.37411932] loss: 30.601097628978323\n",
            "Epoch: 3437 / 5000\n",
            "w1: [24.94658636] w2: [-22.74600577] bias: [16.37403477] loss: 30.600949467796042\n",
            "Epoch: 3438 / 5000\n",
            "w1: [24.94730992] w2: [-22.74697843] bias: [16.37395023] loss: 30.600801583429288\n",
            "Epoch: 3439 / 5000\n",
            "w1: [24.94803291] w2: [-22.74795011] bias: [16.37386568] loss: 30.600653975351964\n",
            "Epoch: 3440 / 5000\n",
            "w1: [24.94875532] w2: [-22.7489208] bias: [16.37378114] loss: 30.600506643038972\n",
            "Epoch: 3441 / 5000\n",
            "w1: [24.94947715] w2: [-22.7498905] bias: [16.3736966] loss: 30.600359585966206\n",
            "Epoch: 3442 / 5000\n",
            "w1: [24.95019841] w2: [-22.75085921] bias: [16.37361206] loss: 30.60021280361058\n",
            "Epoch: 3443 / 5000\n",
            "w1: [24.9509191] w2: [-22.75182694] bias: [16.37352753] loss: 30.600066295449984\n",
            "Epoch: 3444 / 5000\n",
            "w1: [24.95163921] w2: [-22.75279368] bias: [16.37344299] loss: 30.599920060963306\n",
            "Epoch: 3445 / 5000\n",
            "w1: [24.95235874] w2: [-22.75375944] bias: [16.37335846] loss: 30.599774099630437\n",
            "Epoch: 3446 / 5000\n",
            "w1: [24.95307771] w2: [-22.75472421] bias: [16.37327393] loss: 30.59962841093225\n",
            "Epoch: 3447 / 5000\n",
            "w1: [24.9537961] w2: [-22.75568801] bias: [16.3731894] loss: 30.5994829943506\n",
            "Epoch: 3448 / 5000\n",
            "w1: [24.95451392] w2: [-22.75665082] bias: [16.37310488] loss: 30.599337849368354\n",
            "Epoch: 3449 / 5000\n",
            "w1: [24.95523117] w2: [-22.75761265] bias: [16.37302036] loss: 30.599192975469343\n",
            "Epoch: 3450 / 5000\n",
            "w1: [24.95594784] w2: [-22.7585735] bias: [16.37293584] loss: 30.59904837213838\n",
            "Epoch: 3451 / 5000\n",
            "w1: [24.95666395] w2: [-22.75953338] bias: [16.37285132] loss: 30.59890403886128\n",
            "Epoch: 3452 / 5000\n",
            "w1: [24.95737949] w2: [-22.76049227] bias: [16.37276681] loss: 30.598759975124814\n",
            "Epoch: 3453 / 5000\n",
            "w1: [24.95809446] w2: [-22.76145019] bias: [16.37268229] loss: 30.598616180416744\n",
            "Epoch: 3454 / 5000\n",
            "w1: [24.95880886] w2: [-22.76240714] bias: [16.37259778] loss: 30.59847265422581\n",
            "Epoch: 3455 / 5000\n",
            "w1: [24.95952269] w2: [-22.76336311] bias: [16.37251328] loss: 30.598329396041713\n",
            "Epoch: 3456 / 5000\n",
            "w1: [24.96023595] w2: [-22.76431811] bias: [16.37242877] loss: 30.59818640535514\n",
            "Epoch: 3457 / 5000\n",
            "w1: [24.96094865] w2: [-22.76527213] bias: [16.37234427] loss: 30.598043681657735\n",
            "Epoch: 3458 / 5000\n",
            "w1: [24.96166077] w2: [-22.76622518] bias: [16.37225977] loss: 30.597901224442126\n",
            "Epoch: 3459 / 5000\n",
            "w1: [24.96237234] w2: [-22.76717726] bias: [16.37217527] loss: 30.597759033201896\n",
            "Epoch: 3460 / 5000\n",
            "w1: [24.96308334] w2: [-22.76812837] bias: [16.37209077] loss: 30.5976171074316\n",
            "Epoch: 3461 / 5000\n",
            "w1: [24.96379377] w2: [-22.76907852] bias: [16.37200628] loss: 30.597475446626746\n",
            "Epoch: 3462 / 5000\n",
            "w1: [24.96450364] w2: [-22.77002769] bias: [16.37192179] loss: 30.597334050283816\n",
            "Epoch: 3463 / 5000\n",
            "w1: [24.96521294] w2: [-22.7709759] bias: [16.3718373] loss: 30.597192917900237\n",
            "Epoch: 3464 / 5000\n",
            "w1: [24.96592168] w2: [-22.77192314] bias: [16.37175281] loss: 30.597052048974422\n",
            "Epoch: 3465 / 5000\n",
            "w1: [24.96662986] w2: [-22.77286941] bias: [16.37166833] loss: 30.59691144300569\n",
            "Epoch: 3466 / 5000\n",
            "w1: [24.96733747] w2: [-22.77381473] bias: [16.37158385] loss: 30.596771099494358\n",
            "Epoch: 3467 / 5000\n",
            "w1: [24.96804453] w2: [-22.77475907] bias: [16.37149937] loss: 30.596631017941682\n",
            "Epoch: 3468 / 5000\n",
            "w1: [24.96875102] w2: [-22.77570246] bias: [16.37141489] loss: 30.596491197849865\n",
            "Epoch: 3469 / 5000\n",
            "w1: [24.96945695] w2: [-22.77664488] bias: [16.37133042] loss: 30.596351638722048\n",
            "Epoch: 3470 / 5000\n",
            "w1: [24.97016232] w2: [-22.77758635] bias: [16.37124595] loss: 30.596212340062344\n",
            "Epoch: 3471 / 5000\n",
            "w1: [24.97086713] w2: [-22.77852685] bias: [16.37116148] loss: 30.596073301375796\n",
            "Epoch: 3472 / 5000\n",
            "w1: [24.97157138] w2: [-22.7794664] bias: [16.37107702] loss: 30.59593452216838\n",
            "Epoch: 3473 / 5000\n",
            "w1: [24.97227507] w2: [-22.78040498] bias: [16.37099255] loss: 30.59579600194703\n",
            "Epoch: 3474 / 5000\n",
            "w1: [24.97297821] w2: [-22.78134261] bias: [16.37090809] loss: 30.595657740219618\n",
            "Epoch: 3475 / 5000\n",
            "w1: [24.97368078] w2: [-22.78227929] bias: [16.37082363] loss: 30.59551973649494\n",
            "Epoch: 3476 / 5000\n",
            "w1: [24.9743828] w2: [-22.78321501] bias: [16.37073918] loss: 30.59538199028274\n",
            "Epoch: 3477 / 5000\n",
            "w1: [24.97508426] w2: [-22.78414977] bias: [16.37065472] loss: 30.595244501093692\n",
            "Epoch: 3478 / 5000\n",
            "w1: [24.97578517] w2: [-22.78508358] bias: [16.37057027] loss: 30.595107268439403\n",
            "Epoch: 3479 / 5000\n",
            "w1: [24.97648552] w2: [-22.78601644] bias: [16.37048582] loss: 30.594970291832414\n",
            "Epoch: 3480 / 5000\n",
            "w1: [24.97718531] w2: [-22.78694835] bias: [16.37040138] loss: 30.59483357078619\n",
            "Epoch: 3481 / 5000\n",
            "w1: [24.97788455] w2: [-22.78787931] bias: [16.37031694] loss: 30.594697104815122\n",
            "Epoch: 3482 / 5000\n",
            "w1: [24.97858324] w2: [-22.78880932] bias: [16.3702325] loss: 30.594560893434526\n",
            "Epoch: 3483 / 5000\n",
            "w1: [24.97928137] w2: [-22.78973838] bias: [16.37014806] loss: 30.594424936160642\n",
            "Epoch: 3484 / 5000\n",
            "w1: [24.97997895] w2: [-22.79066649] bias: [16.37006362] loss: 30.594289232510643\n",
            "Epoch: 3485 / 5000\n",
            "w1: [24.98067598] w2: [-22.79159365] bias: [16.36997919] loss: 30.594153782002607\n",
            "Epoch: 3486 / 5000\n",
            "w1: [24.98137245] w2: [-22.79251987] bias: [16.36989476] loss: 30.594018584155524\n",
            "Epoch: 3487 / 5000\n",
            "w1: [24.98206838] w2: [-22.79344515] bias: [16.36981033] loss: 30.593883638489327\n",
            "Epoch: 3488 / 5000\n",
            "w1: [24.98276375] w2: [-22.79436948] bias: [16.36972591] loss: 30.593748944524837\n",
            "Epoch: 3489 / 5000\n",
            "w1: [24.98345857] w2: [-22.79529286] bias: [16.36964149] loss: 30.593614501783808\n",
            "Epoch: 3490 / 5000\n",
            "w1: [24.98415284] w2: [-22.79621531] bias: [16.36955707] loss: 30.593480309788895\n",
            "Epoch: 3491 / 5000\n",
            "w1: [24.98484656] w2: [-22.79713681] bias: [16.36947265] loss: 30.593346368063653\n",
            "Epoch: 3492 / 5000\n",
            "w1: [24.98553973] w2: [-22.79805737] bias: [16.36938824] loss: 30.593212676132566\n",
            "Epoch: 3493 / 5000\n",
            "w1: [24.98623236] w2: [-22.79897699] bias: [16.36930383] loss: 30.593079233521006\n",
            "Epoch: 3494 / 5000\n",
            "w1: [24.98692443] w2: [-22.79989568] bias: [16.36921942] loss: 30.59294603975526\n",
            "Epoch: 3495 / 5000\n",
            "w1: [24.98761596] w2: [-22.80081343] bias: [16.36913501] loss: 30.59281309436252\n",
            "Epoch: 3496 / 5000\n",
            "w1: [24.98830694] w2: [-22.80173023] bias: [16.36905061] loss: 30.59268039687086\n",
            "Epoch: 3497 / 5000\n",
            "w1: [24.98899738] w2: [-22.80264611] bias: [16.36896621] loss: 30.59254794680927\n",
            "Epoch: 3498 / 5000\n",
            "w1: [24.98968727] w2: [-22.80356105] bias: [16.36888181] loss: 30.592415743707633\n",
            "Epoch: 3499 / 5000\n",
            "w1: [24.99037661] w2: [-22.80447505] bias: [16.36879742] loss: 30.592283787096733\n",
            "Epoch: 3500 / 5000\n",
            "w1: [24.99106541] w2: [-22.80538812] bias: [16.36871302] loss: 30.592152076508235\n",
            "Epoch: 3501 / 5000\n",
            "w1: [24.99175366] w2: [-22.80630026] bias: [16.36862863] loss: 30.5920206114747\n",
            "Epoch: 3502 / 5000\n",
            "w1: [24.99244137] w2: [-22.80721147] bias: [16.36854425] loss: 30.591889391529598\n",
            "Epoch: 3503 / 5000\n",
            "w1: [24.99312854] w2: [-22.80812175] bias: [16.36845986] loss: 30.591758416207263\n",
            "Epoch: 3504 / 5000\n",
            "w1: [24.99381516] w2: [-22.80903109] bias: [16.36837548] loss: 30.591627685042926\n",
            "Epoch: 3505 / 5000\n",
            "w1: [24.99450124] w2: [-22.80993951] bias: [16.3682911] loss: 30.591497197572707\n",
            "Epoch: 3506 / 5000\n",
            "w1: [24.99518678] w2: [-22.810847] bias: [16.36820673] loss: 30.591366953333598\n",
            "Epoch: 3507 / 5000\n",
            "w1: [24.99587177] w2: [-22.81175356] bias: [16.36812236] loss: 30.591236951863493\n",
            "Epoch: 3508 / 5000\n",
            "w1: [24.99655623] w2: [-22.8126592] bias: [16.36803799] loss: 30.591107192701145\n",
            "Epoch: 3509 / 5000\n",
            "w1: [24.99724015] w2: [-22.81356391] bias: [16.36795362] loss: 30.5909776753862\n",
            "Epoch: 3510 / 5000\n",
            "w1: [24.99792352] w2: [-22.8144677] bias: [16.36786926] loss: 30.59084839945918\n",
            "Epoch: 3511 / 5000\n",
            "w1: [24.99860636] w2: [-22.81537057] bias: [16.36778489] loss: 30.59071936446147\n",
            "Epoch: 3512 / 5000\n",
            "w1: [24.99928865] w2: [-22.81627251] bias: [16.36770054] loss: 30.590590569935344\n",
            "Epoch: 3513 / 5000\n",
            "w1: [24.99997041] w2: [-22.81717353] bias: [16.36761618] loss: 30.590462015423935\n",
            "Epoch: 3514 / 5000\n",
            "w1: [25.00065163] w2: [-22.81807363] bias: [16.36753183] loss: 30.590333700471266\n",
            "Epoch: 3515 / 5000\n",
            "w1: [25.00133232] w2: [-22.81897281] bias: [16.36744748] loss: 30.590205624622197\n",
            "Epoch: 3516 / 5000\n",
            "w1: [25.00201246] w2: [-22.81987107] bias: [16.36736313] loss: 30.590077787422487\n",
            "Epoch: 3517 / 5000\n",
            "w1: [25.00269207] w2: [-22.82076841] bias: [16.36727879] loss: 30.589950188418744\n",
            "Epoch: 3518 / 5000\n",
            "w1: [25.00337115] w2: [-22.82166484] bias: [16.36719444] loss: 30.58982282715844\n",
            "Epoch: 3519 / 5000\n",
            "w1: [25.00404968] w2: [-22.82256034] bias: [16.36711011] loss: 30.58969570318992\n",
            "Epoch: 3520 / 5000\n",
            "w1: [25.00472769] w2: [-22.82345494] bias: [16.36702577] loss: 30.589568816062364\n",
            "Epoch: 3521 / 5000\n",
            "w1: [25.00540516] w2: [-22.82434862] bias: [16.36694144] loss: 30.58944216532585\n",
            "Epoch: 3522 / 5000\n",
            "w1: [25.00608209] w2: [-22.82524138] bias: [16.36685711] loss: 30.58931575053127\n",
            "Epoch: 3523 / 5000\n",
            "w1: [25.00675849] w2: [-22.82613323] bias: [16.36677278] loss: 30.58918957123041\n",
            "Epoch: 3524 / 5000\n",
            "w1: [25.00743436] w2: [-22.82702417] bias: [16.36668846] loss: 30.589063626975886\n",
            "Epoch: 3525 / 5000\n",
            "w1: [25.00810969] w2: [-22.8279142] bias: [16.36660414] loss: 30.58893791732116\n",
            "Epoch: 3526 / 5000\n",
            "w1: [25.0087845] w2: [-22.82880332] bias: [16.36651982] loss: 30.58881244182059\n",
            "Epoch: 3527 / 5000\n",
            "w1: [25.00945877] w2: [-22.82969153] bias: [16.3664355] loss: 30.58868720002932\n",
            "Epoch: 3528 / 5000\n",
            "w1: [25.01013251] w2: [-22.83057883] bias: [16.36635119] loss: 30.58856219150339\n",
            "Epoch: 3529 / 5000\n",
            "w1: [25.01080572] w2: [-22.83146523] bias: [16.36626688] loss: 30.58843741579965\n",
            "Epoch: 3530 / 5000\n",
            "w1: [25.0114784] w2: [-22.83235071] bias: [16.36618257] loss: 30.58831287247583\n",
            "Epoch: 3531 / 5000\n",
            "w1: [25.01215055] w2: [-22.8332353] bias: [16.36609827] loss: 30.588188561090472\n",
            "Epoch: 3532 / 5000\n",
            "w1: [25.01282217] w2: [-22.83411897] bias: [16.36601397] loss: 30.588064481202984\n",
            "Epoch: 3533 / 5000\n",
            "w1: [25.01349326] w2: [-22.83500174] bias: [16.36592967] loss: 30.587940632373588\n",
            "Epoch: 3534 / 5000\n",
            "w1: [25.01416383] w2: [-22.83588361] bias: [16.36584538] loss: 30.58781701416337\n",
            "Epoch: 3535 / 5000\n",
            "w1: [25.01483386] w2: [-22.83676458] bias: [16.36576109] loss: 30.587693626134232\n",
            "Epoch: 3536 / 5000\n",
            "w1: [25.01550337] w2: [-22.83764464] bias: [16.3656768] loss: 30.587570467848916\n",
            "Epoch: 3537 / 5000\n",
            "w1: [25.01617236] w2: [-22.83852381] bias: [16.36559252] loss: 30.587447538871\n",
            "Epoch: 3538 / 5000\n",
            "w1: [25.01684081] w2: [-22.83940207] bias: [16.36550823] loss: 30.587324838764893\n",
            "Epoch: 3539 / 5000\n",
            "w1: [25.01750874] w2: [-22.84027944] bias: [16.36542395] loss: 30.58720236709583\n",
            "Epoch: 3540 / 5000\n",
            "w1: [25.01817615] w2: [-22.84115591] bias: [16.36533968] loss: 30.587080123429885\n",
            "Epoch: 3541 / 5000\n",
            "w1: [25.01884303] w2: [-22.84203148] bias: [16.36525541] loss: 30.58695810733394\n",
            "Epoch: 3542 / 5000\n",
            "w1: [25.01950939] w2: [-22.84290615] bias: [16.36517114] loss: 30.586836318375724\n",
            "Epoch: 3543 / 5000\n",
            "w1: [25.02017522] w2: [-22.84377993] bias: [16.36508687] loss: 30.58671475612377\n",
            "Epoch: 3544 / 5000\n",
            "w1: [25.02084053] w2: [-22.84465282] bias: [16.36500261] loss: 30.58659342014744\n",
            "Epoch: 3545 / 5000\n",
            "w1: [25.02150532] w2: [-22.84552481] bias: [16.36491834] loss: 30.586472310016923\n",
            "Epoch: 3546 / 5000\n",
            "w1: [25.02216958] w2: [-22.84639591] bias: [16.36483409] loss: 30.58635142530322\n",
            "Epoch: 3547 / 5000\n",
            "w1: [25.02283333] w2: [-22.84726611] bias: [16.36474983] loss: 30.58623076557815\n",
            "Epoch: 3548 / 5000\n",
            "w1: [25.02349655] w2: [-22.84813543] bias: [16.36466558] loss: 30.586110330414353\n",
            "Epoch: 3549 / 5000\n",
            "w1: [25.02415925] w2: [-22.84900385] bias: [16.36458133] loss: 30.585990119385265\n",
            "Epoch: 3550 / 5000\n",
            "w1: [25.02482143] w2: [-22.84987139] bias: [16.36449709] loss: 30.58587013206517\n",
            "Epoch: 3551 / 5000\n",
            "w1: [25.02548309] w2: [-22.85073804] bias: [16.36441285] loss: 30.585750368029125\n",
            "Epoch: 3552 / 5000\n",
            "w1: [25.02614423] w2: [-22.8516038] bias: [16.36432861] loss: 30.58563082685302\n",
            "Epoch: 3553 / 5000\n",
            "w1: [25.02680485] w2: [-22.85246867] bias: [16.36424437] loss: 30.585511508113534\n",
            "Epoch: 3554 / 5000\n",
            "w1: [25.02746496] w2: [-22.85333265] bias: [16.36416014] loss: 30.585392411388174\n",
            "Epoch: 3555 / 5000\n",
            "w1: [25.02812454] w2: [-22.85419576] bias: [16.36407591] loss: 30.585273536255247\n",
            "Epoch: 3556 / 5000\n",
            "w1: [25.02878361] w2: [-22.85505797] bias: [16.36399168] loss: 30.58515488229385\n",
            "Epoch: 3557 / 5000\n",
            "w1: [25.02944216] w2: [-22.85591931] bias: [16.36390746] loss: 30.58503644908389\n",
            "Epoch: 3558 / 5000\n",
            "w1: [25.0301002] w2: [-22.85677976] bias: [16.36382324] loss: 30.58491823620607\n",
            "Epoch: 3559 / 5000\n",
            "w1: [25.03075772] w2: [-22.85763933] bias: [16.36373902] loss: 30.584800243241922\n",
            "Epoch: 3560 / 5000\n",
            "w1: [25.03141472] w2: [-22.85849802] bias: [16.36365481] loss: 30.58468246977372\n",
            "Epoch: 3561 / 5000\n",
            "w1: [25.03207121] w2: [-22.85935582] bias: [16.3635706] loss: 30.584564915384572\n",
            "Epoch: 3562 / 5000\n",
            "w1: [25.03272718] w2: [-22.86021275] bias: [16.36348639] loss: 30.584447579658374\n",
            "Epoch: 3563 / 5000\n",
            "w1: [25.03338264] w2: [-22.8610688] bias: [16.36340219] loss: 30.58433046217981\n",
            "Epoch: 3564 / 5000\n",
            "w1: [25.03403759] w2: [-22.86192398] bias: [16.36331799] loss: 30.584213562534355\n",
            "Epoch: 3565 / 5000\n",
            "w1: [25.03469202] w2: [-22.86277828] bias: [16.36323379] loss: 30.584096880308284\n",
            "Epoch: 3566 / 5000\n",
            "w1: [25.03534594] w2: [-22.8636317] bias: [16.3631496] loss: 30.583980415088647\n",
            "Epoch: 3567 / 5000\n",
            "w1: [25.03599935] w2: [-22.86448424] bias: [16.36306541] loss: 30.583864166463286\n",
            "Epoch: 3568 / 5000\n",
            "w1: [25.03665224] w2: [-22.86533591] bias: [16.36298122] loss: 30.583748134020823\n",
            "Epoch: 3569 / 5000\n",
            "w1: [25.03730463] w2: [-22.86618671] bias: [16.36289704] loss: 30.58363231735068\n",
            "Epoch: 3570 / 5000\n",
            "w1: [25.0379565] w2: [-22.86703664] bias: [16.36281285] loss: 30.58351671604304\n",
            "Epoch: 3571 / 5000\n",
            "w1: [25.03860786] w2: [-22.86788569] bias: [16.36272868] loss: 30.583401329688876\n",
            "Epoch: 3572 / 5000\n",
            "w1: [25.03925871] w2: [-22.86873388] bias: [16.3626445] loss: 30.58328615787995\n",
            "Epoch: 3573 / 5000\n",
            "w1: [25.03990906] w2: [-22.86958119] bias: [16.36256033] loss: 30.58317120020878\n",
            "Epoch: 3574 / 5000\n",
            "w1: [25.04055889] w2: [-22.87042764] bias: [16.36247616] loss: 30.583056456268686\n",
            "Epoch: 3575 / 5000\n",
            "w1: [25.04120822] w2: [-22.87127322] bias: [16.362392] loss: 30.58294192565374\n",
            "Epoch: 3576 / 5000\n",
            "w1: [25.04185704] w2: [-22.87211792] bias: [16.36230784] loss: 30.5828276079588\n",
            "Epoch: 3577 / 5000\n",
            "w1: [25.04250535] w2: [-22.87296177] bias: [16.36222368] loss: 30.582713502779487\n",
            "Epoch: 3578 / 5000\n",
            "w1: [25.04315315] w2: [-22.87380474] bias: [16.36213952] loss: 30.582599609712208\n",
            "Epoch: 3579 / 5000\n",
            "w1: [25.04380045] w2: [-22.87464686] bias: [16.36205537] loss: 30.58248592835412\n",
            "Epoch: 3580 / 5000\n",
            "w1: [25.04444724] w2: [-22.87548811] bias: [16.36197123] loss: 30.582372458303155\n",
            "Epoch: 3581 / 5000\n",
            "w1: [25.04509353] w2: [-22.87632849] bias: [16.36188708] loss: 30.58225919915802\n",
            "Epoch: 3582 / 5000\n",
            "w1: [25.04573931] w2: [-22.87716801] bias: [16.36180294] loss: 30.58214615051817\n",
            "Epoch: 3583 / 5000\n",
            "w1: [25.04638458] w2: [-22.87800667] bias: [16.3617188] loss: 30.582033311983846\n",
            "Epoch: 3584 / 5000\n",
            "w1: [25.04702935] w2: [-22.87884447] bias: [16.36163467] loss: 30.581920683156014\n",
            "Epoch: 3585 / 5000\n",
            "w1: [25.04767362] w2: [-22.87968141] bias: [16.36155054] loss: 30.581808263636436\n",
            "Epoch: 3586 / 5000\n",
            "w1: [25.04831739] w2: [-22.8805175] bias: [16.36146641] loss: 30.58169605302762\n",
            "Epoch: 3587 / 5000\n",
            "w1: [25.04896065] w2: [-22.88135272] bias: [16.36138228] loss: 30.581584050932815\n",
            "Epoch: 3588 / 5000\n",
            "w1: [25.04960341] w2: [-22.88218709] bias: [16.36129816] loss: 30.58147225695606\n",
            "Epoch: 3589 / 5000\n",
            "w1: [25.05024566] w2: [-22.88302059] bias: [16.36121405] loss: 30.581360670702125\n",
            "Epoch: 3590 / 5000\n",
            "w1: [25.05088742] w2: [-22.88385325] bias: [16.36112993] loss: 30.581249291776533\n",
            "Epoch: 3591 / 5000\n",
            "w1: [25.05152867] w2: [-22.88468505] bias: [16.36104582] loss: 30.581138119785557\n",
            "Epoch: 3592 / 5000\n",
            "w1: [25.05216943] w2: [-22.88551599] bias: [16.36096171] loss: 30.58102715433623\n",
            "Epoch: 3593 / 5000\n",
            "w1: [25.05280968] w2: [-22.88634608] bias: [16.36087761] loss: 30.580916395036333\n",
            "Epoch: 3594 / 5000\n",
            "w1: [25.05344943] w2: [-22.88717532] bias: [16.36079351] loss: 30.58080584149438\n",
            "Epoch: 3595 / 5000\n",
            "w1: [25.05408869] w2: [-22.88800371] bias: [16.36070941] loss: 30.580695493319652\n",
            "Epoch: 3596 / 5000\n",
            "w1: [25.05472744] w2: [-22.88883124] bias: [16.36062532] loss: 30.580585350122163\n",
            "Epoch: 3597 / 5000\n",
            "w1: [25.0553657] w2: [-22.88965793] bias: [16.36054123] loss: 30.580475411512666\n",
            "Epoch: 3598 / 5000\n",
            "w1: [25.05600346] w2: [-22.89048377] bias: [16.36045714] loss: 30.58036567710265\n",
            "Epoch: 3599 / 5000\n",
            "w1: [25.05664073] w2: [-22.89130875] bias: [16.36037306] loss: 30.58025614650437\n",
            "Epoch: 3600 / 5000\n",
            "w1: [25.05727749] w2: [-22.89213289] bias: [16.36028898] loss: 30.580146819330807\n",
            "Epoch: 3601 / 5000\n",
            "w1: [25.05791376] w2: [-22.89295619] bias: [16.3602049] loss: 30.580037695195657\n",
            "Epoch: 3602 / 5000\n",
            "w1: [25.05854954] w2: [-22.89377864] bias: [16.36012083] loss: 30.579928773713377\n",
            "Epoch: 3603 / 5000\n",
            "w1: [25.05918481] w2: [-22.89460024] bias: [16.36003676] loss: 30.57982005449916\n",
            "Epoch: 3604 / 5000\n",
            "w1: [25.0598196] w2: [-22.895421] bias: [16.3599527] loss: 30.579711537168915\n",
            "Epoch: 3605 / 5000\n",
            "w1: [25.06045389] w2: [-22.89624091] bias: [16.35986863] loss: 30.5796032213393\n",
            "Epoch: 3606 / 5000\n",
            "w1: [25.06108768] w2: [-22.89705999] bias: [16.35978458] loss: 30.579495106627686\n",
            "Epoch: 3607 / 5000\n",
            "w1: [25.06172098] w2: [-22.89787822] bias: [16.35970052] loss: 30.579387192652188\n",
            "Epoch: 3608 / 5000\n",
            "w1: [25.06235379] w2: [-22.89869561] bias: [16.35961647] loss: 30.57927947903164\n",
            "Epoch: 3609 / 5000\n",
            "w1: [25.0629861] w2: [-22.89951216] bias: [16.35953242] loss: 30.579171965385598\n",
            "Epoch: 3610 / 5000\n",
            "w1: [25.06361793] w2: [-22.90032787] bias: [16.35944838] loss: 30.579064651334367\n",
            "Epoch: 3611 / 5000\n",
            "w1: [25.06424926] w2: [-22.90114274] bias: [16.35936434] loss: 30.578957536498937\n",
            "Epoch: 3612 / 5000\n",
            "w1: [25.0648801] w2: [-22.90195677] bias: [16.3592803] loss: 30.57885062050106\n",
            "Epoch: 3613 / 5000\n",
            "w1: [25.06551044] w2: [-22.90276997] bias: [16.35919627] loss: 30.578743902963176\n",
            "Epoch: 3614 / 5000\n",
            "w1: [25.0661403] w2: [-22.90358233] bias: [16.35911224] loss: 30.57863738350845\n",
            "Epoch: 3615 / 5000\n",
            "w1: [25.06676967] w2: [-22.90439385] bias: [16.35902821] loss: 30.578531061760792\n",
            "Epoch: 3616 / 5000\n",
            "w1: [25.06739855] w2: [-22.90520454] bias: [16.35894419] loss: 30.578424937344803\n",
            "Epoch: 3617 / 5000\n",
            "w1: [25.06802694] w2: [-22.9060144] bias: [16.35886017] loss: 30.578319009885792\n",
            "Epoch: 3618 / 5000\n",
            "w1: [25.06865484] w2: [-22.90682342] bias: [16.35877615] loss: 30.578213279009805\n",
            "Epoch: 3619 / 5000\n",
            "w1: [25.06928225] w2: [-22.90763161] bias: [16.35869214] loss: 30.578107744343583\n",
            "Epoch: 3620 / 5000\n",
            "w1: [25.06990918] w2: [-22.90843897] bias: [16.35860813] loss: 30.578002405514592\n",
            "Epoch: 3621 / 5000\n",
            "w1: [25.07053561] w2: [-22.9092455] bias: [16.35852413] loss: 30.577897262150998\n",
            "Epoch: 3622 / 5000\n",
            "w1: [25.07116156] w2: [-22.9100512] bias: [16.35844013] loss: 30.577792313881684\n",
            "Epoch: 3623 / 5000\n",
            "w1: [25.07178703] w2: [-22.91085607] bias: [16.35835613] loss: 30.577687560336216\n",
            "Epoch: 3624 / 5000\n",
            "w1: [25.07241201] w2: [-22.91166011] bias: [16.35827213] loss: 30.577583001144895\n",
            "Epoch: 3625 / 5000\n",
            "w1: [25.0730365] w2: [-22.91246333] bias: [16.35818814] loss: 30.577478635938714\n",
            "Epoch: 3626 / 5000\n",
            "w1: [25.07366051] w2: [-22.91326572] bias: [16.35810416] loss: 30.577374464349372\n",
            "Epoch: 3627 / 5000\n",
            "w1: [25.07428403] w2: [-22.91406728] bias: [16.35802017] loss: 30.577270486009265\n",
            "Epoch: 3628 / 5000\n",
            "w1: [25.07490707] w2: [-22.91486802] bias: [16.3579362] loss: 30.577166700551487\n",
            "Epoch: 3629 / 5000\n",
            "w1: [25.07552962] w2: [-22.91566793] bias: [16.35785222] loss: 30.577063107609842\n",
            "Epoch: 3630 / 5000\n",
            "w1: [25.07615169] w2: [-22.91646702] bias: [16.35776825] loss: 30.576959706818823\n",
            "Epoch: 3631 / 5000\n",
            "w1: [25.07677328] w2: [-22.91726529] bias: [16.35768428] loss: 30.576856497813612\n",
            "Epoch: 3632 / 5000\n",
            "w1: [25.07739439] w2: [-22.91806273] bias: [16.35760032] loss: 30.576753480230106\n",
            "Epoch: 3633 / 5000\n",
            "w1: [25.07801501] w2: [-22.91885936] bias: [16.35751635] loss: 30.57665065370488\n",
            "Epoch: 3634 / 5000\n",
            "w1: [25.07863516] w2: [-22.91965516] bias: [16.3574324] loss: 30.57654801787521\n",
            "Epoch: 3635 / 5000\n",
            "w1: [25.07925482] w2: [-22.92045015] bias: [16.35734844] loss: 30.57644557237905\n",
            "Epoch: 3636 / 5000\n",
            "w1: [25.079874] w2: [-22.92124431] bias: [16.35726449] loss: 30.576343316855063\n",
            "Epoch: 3637 / 5000\n",
            "w1: [25.0804927] w2: [-22.92203766] bias: [16.35718055] loss: 30.576241250942584\n",
            "Epoch: 3638 / 5000\n",
            "w1: [25.08111092] w2: [-22.92283019] bias: [16.35709661] loss: 30.57613937428164\n",
            "Epoch: 3639 / 5000\n",
            "w1: [25.08172866] w2: [-22.92362191] bias: [16.35701267] loss: 30.57603768651295\n",
            "Epoch: 3640 / 5000\n",
            "w1: [25.08234593] w2: [-22.92441281] bias: [16.35692873] loss: 30.575936187277907\n",
            "Epoch: 3641 / 5000\n",
            "w1: [25.08296271] w2: [-22.92520289] bias: [16.3568448] loss: 30.57583487621859\n",
            "Epoch: 3642 / 5000\n",
            "w1: [25.08357902] w2: [-22.92599216] bias: [16.35676088] loss: 30.57573375297776\n",
            "Epoch: 3643 / 5000\n",
            "w1: [25.08419485] w2: [-22.92678062] bias: [16.35667695] loss: 30.575632817198873\n",
            "Epoch: 3644 / 5000\n",
            "w1: [25.0848102] w2: [-22.92756826] bias: [16.35659303] loss: 30.575532068526044\n",
            "Epoch: 3645 / 5000\n",
            "w1: [25.08542508] w2: [-22.9283551] bias: [16.35650912] loss: 30.575431506604072\n",
            "Epoch: 3646 / 5000\n",
            "w1: [25.08603948] w2: [-22.92914112] bias: [16.35642521] loss: 30.57533113107844\n",
            "Epoch: 3647 / 5000\n",
            "w1: [25.0866534] w2: [-22.92992633] bias: [16.3563413] loss: 30.575230941595283\n",
            "Epoch: 3648 / 5000\n",
            "w1: [25.08726685] w2: [-22.93071074] bias: [16.35625739] loss: 30.575130937801443\n",
            "Epoch: 3649 / 5000\n",
            "w1: [25.08787982] w2: [-22.93149433] bias: [16.35617349] loss: 30.575031119344427\n",
            "Epoch: 3650 / 5000\n",
            "w1: [25.08849232] w2: [-22.93227712] bias: [16.3560896] loss: 30.574931485872384\n",
            "Epoch: 3651 / 5000\n",
            "w1: [25.08910434] w2: [-22.9330591] bias: [16.3560057] loss: 30.574832037034163\n",
            "Epoch: 3652 / 5000\n",
            "w1: [25.0897159] w2: [-22.93384027] bias: [16.35592181] loss: 30.57473277247929\n",
            "Epoch: 3653 / 5000\n",
            "w1: [25.09032697] w2: [-22.93462064] bias: [16.35583793] loss: 30.574633691857915\n",
            "Epoch: 3654 / 5000\n",
            "w1: [25.09093758] w2: [-22.93540021] bias: [16.35575405] loss: 30.5745347948209\n",
            "Epoch: 3655 / 5000\n",
            "w1: [25.09154771] w2: [-22.93617897] bias: [16.35567017] loss: 30.574436081019748\n",
            "Epoch: 3656 / 5000\n",
            "w1: [25.09215737] w2: [-22.93695692] bias: [16.3555863] loss: 30.57433755010663\n",
            "Epoch: 3657 / 5000\n",
            "w1: [25.09276657] w2: [-22.93773408] bias: [16.35550243] loss: 30.574239201734386\n",
            "Epoch: 3658 / 5000\n",
            "w1: [25.09337528] w2: [-22.93851043] bias: [16.35541856] loss: 30.57414103555651\n",
            "Epoch: 3659 / 5000\n",
            "w1: [25.09398353] w2: [-22.93928599] bias: [16.3553347] loss: 30.574043051227168\n",
            "Epoch: 3660 / 5000\n",
            "w1: [25.09459131] w2: [-22.94006074] bias: [16.35525084] loss: 30.57394524840116\n",
            "Epoch: 3661 / 5000\n",
            "w1: [25.09519862] w2: [-22.94083469] bias: [16.35516699] loss: 30.57384762673396\n",
            "Epoch: 3662 / 5000\n",
            "w1: [25.09580546] w2: [-22.94160785] bias: [16.35508314] loss: 30.573750185881703\n",
            "Epoch: 3663 / 5000\n",
            "w1: [25.09641183] w2: [-22.94238021] bias: [16.35499929] loss: 30.573652925501175\n",
            "Epoch: 3664 / 5000\n",
            "w1: [25.09701773] w2: [-22.94315177] bias: [16.35491545] loss: 30.57355584524981\n",
            "Epoch: 3665 / 5000\n",
            "w1: [25.09762317] w2: [-22.94392253] bias: [16.35483161] loss: 30.5734589447857\n",
            "Epoch: 3666 / 5000\n",
            "w1: [25.09822814] w2: [-22.9446925] bias: [16.35474778] loss: 30.573362223767585\n",
            "Epoch: 3667 / 5000\n",
            "w1: [25.09883264] w2: [-22.94546168] bias: [16.35466395] loss: 30.573265681854856\n",
            "Epoch: 3668 / 5000\n",
            "w1: [25.09943667] w2: [-22.94623006] bias: [16.35458012] loss: 30.573169318707556\n",
            "Epoch: 3669 / 5000\n",
            "w1: [25.10004024] w2: [-22.94699765] bias: [16.3544963] loss: 30.573073133986377\n",
            "Epoch: 3670 / 5000\n",
            "w1: [25.10064334] w2: [-22.94776444] bias: [16.35441248] loss: 30.57297712735265\n",
            "Epoch: 3671 / 5000\n",
            "w1: [25.10124597] w2: [-22.94853045] bias: [16.35432866] loss: 30.57288129846835\n",
            "Epoch: 3672 / 5000\n",
            "w1: [25.10184814] w2: [-22.94929566] bias: [16.35424485] loss: 30.572785646996103\n",
            "Epoch: 3673 / 5000\n",
            "w1: [25.10244985] w2: [-22.95006008] bias: [16.35416104] loss: 30.572690172599184\n",
            "Epoch: 3674 / 5000\n",
            "w1: [25.10305109] w2: [-22.95082372] bias: [16.35407724] loss: 30.57259487494148\n",
            "Epoch: 3675 / 5000\n",
            "w1: [25.10365187] w2: [-22.95158656] bias: [16.35399344] loss: 30.572499753687563\n",
            "Epoch: 3676 / 5000\n",
            "w1: [25.10425219] w2: [-22.95234862] bias: [16.35390965] loss: 30.572404808502608\n",
            "Epoch: 3677 / 5000\n",
            "w1: [25.10485204] w2: [-22.95310989] bias: [16.35382586] loss: 30.572310039052432\n",
            "Epoch: 3678 / 5000\n",
            "w1: [25.10545143] w2: [-22.95387038] bias: [16.35374207] loss: 30.572215445003504\n",
            "Epoch: 3679 / 5000\n",
            "w1: [25.10605036] w2: [-22.95463008] bias: [16.35365829] loss: 30.572121026022923\n",
            "Epoch: 3680 / 5000\n",
            "w1: [25.10664883] w2: [-22.95538899] bias: [16.35357451] loss: 30.57202678177842\n",
            "Epoch: 3681 / 5000\n",
            "w1: [25.10724684] w2: [-22.95614712] bias: [16.35349073] loss: 30.571932711938352\n",
            "Epoch: 3682 / 5000\n",
            "w1: [25.10784438] w2: [-22.95690447] bias: [16.35340696] loss: 30.57183881617171\n",
            "Epoch: 3683 / 5000\n",
            "w1: [25.10844147] w2: [-22.95766104] bias: [16.3533232] loss: 30.571745094148135\n",
            "Epoch: 3684 / 5000\n",
            "w1: [25.10903809] w2: [-22.95841682] bias: [16.35323943] loss: 30.571651545537872\n",
            "Epoch: 3685 / 5000\n",
            "w1: [25.10963426] w2: [-22.95917182] bias: [16.35315567] loss: 30.571558170011805\n",
            "Epoch: 3686 / 5000\n",
            "w1: [25.11022997] w2: [-22.95992605] bias: [16.35307192] loss: 30.571464967241447\n",
            "Epoch: 3687 / 5000\n",
            "w1: [25.11082521] w2: [-22.96067949] bias: [16.35298817] loss: 30.571371936898927\n",
            "Epoch: 3688 / 5000\n",
            "w1: [25.11142] w2: [-22.96143215] bias: [16.35290442] loss: 30.57127907865701\n",
            "Epoch: 3689 / 5000\n",
            "w1: [25.11201434] w2: [-22.96218404] bias: [16.35282068] loss: 30.571186392189077\n",
            "Epoch: 3690 / 5000\n",
            "w1: [25.11260821] w2: [-22.96293515] bias: [16.35273694] loss: 30.571093877169126\n",
            "Epoch: 3691 / 5000\n",
            "w1: [25.11320163] w2: [-22.96368548] bias: [16.35265321] loss: 30.571001533271808\n",
            "Epoch: 3692 / 5000\n",
            "w1: [25.11379459] w2: [-22.96443504] bias: [16.35256948] loss: 30.570909360172347\n",
            "Epoch: 3693 / 5000\n",
            "w1: [25.1143871] w2: [-22.96518383] bias: [16.35248575] loss: 30.570817357546616\n",
            "Epoch: 3694 / 5000\n",
            "w1: [25.11497915] w2: [-22.96593183] bias: [16.35240203] loss: 30.570725525071094\n",
            "Epoch: 3695 / 5000\n",
            "w1: [25.11557075] w2: [-22.96667907] bias: [16.35231831] loss: 30.57063386242288\n",
            "Epoch: 3696 / 5000\n",
            "w1: [25.11616189] w2: [-22.96742553] bias: [16.3522346] loss: 30.570542369279693\n",
            "Epoch: 3697 / 5000\n",
            "w1: [25.11675258] w2: [-22.96817122] bias: [16.35215089] loss: 30.57045104531985\n",
            "Epoch: 3698 / 5000\n",
            "w1: [25.11734281] w2: [-22.96891614] bias: [16.35206718] loss: 30.57035989022231\n",
            "Epoch: 3699 / 5000\n",
            "w1: [25.11793259] w2: [-22.96966029] bias: [16.35198348] loss: 30.570268903666598\n",
            "Epoch: 3700 / 5000\n",
            "w1: [25.11852191] w2: [-22.97040367] bias: [16.35189978] loss: 30.5701780853329\n",
            "Epoch: 3701 / 5000\n",
            "w1: [25.11911079] w2: [-22.97114628] bias: [16.35181609] loss: 30.57008743490197\n",
            "Epoch: 3702 / 5000\n",
            "w1: [25.11969921] w2: [-22.97188812] bias: [16.3517324] loss: 30.569996952055202\n",
            "Epoch: 3703 / 5000\n",
            "w1: [25.12028718] w2: [-22.9726292] bias: [16.35164872] loss: 30.569906636474578\n",
            "Epoch: 3704 / 5000\n",
            "w1: [25.1208747] w2: [-22.97336951] bias: [16.35156504] loss: 30.569816487842683\n",
            "Epoch: 3705 / 5000\n",
            "w1: [25.12146176] w2: [-22.97410905] bias: [16.35148136] loss: 30.569726505842716\n",
            "Epoch: 3706 / 5000\n",
            "w1: [25.12204838] w2: [-22.97484783] bias: [16.35139769] loss: 30.56963669015848\n",
            "Epoch: 3707 / 5000\n",
            "w1: [25.12263455] w2: [-22.97558584] bias: [16.35131402] loss: 30.56954704047438\n",
            "Epoch: 3708 / 5000\n",
            "w1: [25.12322026] w2: [-22.97632309] bias: [16.35123036] loss: 30.569457556475417\n",
            "Epoch: 3709 / 5000\n",
            "w1: [25.12380553] w2: [-22.97705957] bias: [16.3511467] loss: 30.56936823784719\n",
            "Epoch: 3710 / 5000\n",
            "w1: [25.12439035] w2: [-22.9777953] bias: [16.35106304] loss: 30.56927908427591\n",
            "Epoch: 3711 / 5000\n",
            "w1: [25.12497472] w2: [-22.97853026] bias: [16.35097939] loss: 30.569190095448366\n",
            "Epoch: 3712 / 5000\n",
            "w1: [25.12555864] w2: [-22.97926446] bias: [16.35089574] loss: 30.569101271051967\n",
            "Epoch: 3713 / 5000\n",
            "w1: [25.12614211] w2: [-22.9799979] bias: [16.3508121] loss: 30.569012610774692\n",
            "Epoch: 3714 / 5000\n",
            "w1: [25.12672514] w2: [-22.98073058] bias: [16.35072846] loss: 30.56892411430514\n",
            "Epoch: 3715 / 5000\n",
            "w1: [25.12730772] w2: [-22.98146251] bias: [16.35064483] loss: 30.56883578133248\n",
            "Epoch: 3716 / 5000\n",
            "w1: [25.12788985] w2: [-22.98219367] bias: [16.3505612] loss: 30.568747611546495\n",
            "Epoch: 3717 / 5000\n",
            "w1: [25.12847154] w2: [-22.98292408] bias: [16.35047757] loss: 30.56865960463753\n",
            "Epoch: 3718 / 5000\n",
            "w1: [25.12905278] w2: [-22.98365373] bias: [16.35039395] loss: 30.568571760296557\n",
            "Epoch: 3719 / 5000\n",
            "w1: [25.12963358] w2: [-22.98438263] bias: [16.35031033] loss: 30.568484078215103\n",
            "Epoch: 3720 / 5000\n",
            "w1: [25.13021393] w2: [-22.98511077] bias: [16.35022672] loss: 30.568396558085304\n",
            "Epoch: 3721 / 5000\n",
            "w1: [25.13079384] w2: [-22.98583815] bias: [16.35014311] loss: 30.56830919959987\n",
            "Epoch: 3722 / 5000\n",
            "w1: [25.1313733] w2: [-22.98656479] bias: [16.35005951] loss: 30.56822200245211\n",
            "Epoch: 3723 / 5000\n",
            "w1: [25.13195232] w2: [-22.98729067] bias: [16.3499759] loss: 30.568134966335894\n",
            "Epoch: 3724 / 5000\n",
            "w1: [25.1325309] w2: [-22.98801579] bias: [16.34989231] loss: 30.568048090945698\n",
            "Epoch: 3725 / 5000\n",
            "w1: [25.13310904] w2: [-22.98874017] bias: [16.34980872] loss: 30.567961375976573\n",
            "Epoch: 3726 / 5000\n",
            "w1: [25.13368673] w2: [-22.9894638] bias: [16.34972513] loss: 30.56787482112415\n",
            "Epoch: 3727 / 5000\n",
            "w1: [25.13426398] w2: [-22.99018667] bias: [16.34964155] loss: 30.56778842608463\n",
            "Epoch: 3728 / 5000\n",
            "w1: [25.13484078] w2: [-22.9909088] bias: [16.34955797] loss: 30.567702190554808\n",
            "Epoch: 3729 / 5000\n",
            "w1: [25.13541715] w2: [-22.99163018] bias: [16.34947439] loss: 30.567616114232052\n",
            "Epoch: 3730 / 5000\n",
            "w1: [25.13599308] w2: [-22.99235081] bias: [16.34939082] loss: 30.567530196814303\n",
            "Epoch: 3731 / 5000\n",
            "w1: [25.13656856] w2: [-22.99307069] bias: [16.34930726] loss: 30.567444438000074\n",
            "Epoch: 3732 / 5000\n",
            "w1: [25.13714361] w2: [-22.99378983] bias: [16.3492237] loss: 30.567358837488456\n",
            "Epoch: 3733 / 5000\n",
            "w1: [25.13771822] w2: [-22.99450822] bias: [16.34914014] loss: 30.567273394979132\n",
            "Epoch: 3734 / 5000\n",
            "w1: [25.13829239] w2: [-22.99522587] bias: [16.34905659] loss: 30.56718811017232\n",
            "Epoch: 3735 / 5000\n",
            "w1: [25.13886611] w2: [-22.99594277] bias: [16.34897304] loss: 30.567102982768827\n",
            "Epoch: 3736 / 5000\n",
            "w1: [25.13943941] w2: [-22.99665893] bias: [16.34888949] loss: 30.56701801247005\n",
            "Epoch: 3737 / 5000\n",
            "w1: [25.14001226] w2: [-22.99737435] bias: [16.34880595] loss: 30.56693319897792\n",
            "Epoch: 3738 / 5000\n",
            "w1: [25.14058467] w2: [-22.99808903] bias: [16.34872242] loss: 30.566848541994958\n",
            "Epoch: 3739 / 5000\n",
            "w1: [25.14115665] w2: [-22.99880296] bias: [16.34863889] loss: 30.566764041224246\n",
            "Epoch: 3740 / 5000\n",
            "w1: [25.14172819] w2: [-22.99951615] bias: [16.34855536] loss: 30.566679696369423\n",
            "Epoch: 3741 / 5000\n",
            "w1: [25.1422993] w2: [-23.00022861] bias: [16.34847184] loss: 30.566595507134718\n",
            "Epoch: 3742 / 5000\n",
            "w1: [25.14286997] w2: [-23.00094032] bias: [16.34838832] loss: 30.56651147322488\n",
            "Epoch: 3743 / 5000\n",
            "w1: [25.1434402] w2: [-23.0016513] bias: [16.34830481] loss: 30.566427594345267\n",
            "Epoch: 3744 / 5000\n",
            "w1: [25.14401] w2: [-23.00236154] bias: [16.3482213] loss: 30.56634387020177\n",
            "Epoch: 3745 / 5000\n",
            "w1: [25.14457936] w2: [-23.00307104] bias: [16.3481378] loss: 30.566260300500844\n",
            "Epoch: 3746 / 5000\n",
            "w1: [25.14514829] w2: [-23.0037798] bias: [16.3480543] loss: 30.566176884949506\n",
            "Epoch: 3747 / 5000\n",
            "w1: [25.14571679] w2: [-23.00448783] bias: [16.3479708] loss: 30.566093623255345\n",
            "Epoch: 3748 / 5000\n",
            "w1: [25.14628485] w2: [-23.00519513] bias: [16.34788731] loss: 30.566010515126475\n",
            "Epoch: 3749 / 5000\n",
            "w1: [25.14685248] w2: [-23.00590169] bias: [16.34780382] loss: 30.5659275602716\n",
            "Epoch: 3750 / 5000\n",
            "w1: [25.14741968] w2: [-23.00660751] bias: [16.34772034] loss: 30.56584475839995\n",
            "Epoch: 3751 / 5000\n",
            "w1: [25.14798644] w2: [-23.00731261] bias: [16.34763686] loss: 30.56576210922134\n",
            "Epoch: 3752 / 5000\n",
            "w1: [25.14855278] w2: [-23.00801697] bias: [16.34755339] loss: 30.565679612446104\n",
            "Epoch: 3753 / 5000\n",
            "w1: [25.14911868] w2: [-23.0087206] bias: [16.34746992] loss: 30.565597267785144\n",
            "Epoch: 3754 / 5000\n",
            "w1: [25.14968415] w2: [-23.0094235] bias: [16.34738645] loss: 30.565515074949918\n",
            "Epoch: 3755 / 5000\n",
            "w1: [25.15024919] w2: [-23.01012567] bias: [16.34730299] loss: 30.565433033652422\n",
            "Epoch: 3756 / 5000\n",
            "w1: [25.1508138] w2: [-23.01082711] bias: [16.34721954] loss: 30.565351143605216\n",
            "Epoch: 3757 / 5000\n",
            "w1: [25.15137798] w2: [-23.01152782] bias: [16.34713609] loss: 30.565269404521395\n",
            "Epoch: 3758 / 5000\n",
            "w1: [25.15194172] w2: [-23.01222781] bias: [16.34705264] loss: 30.56518781611459\n",
            "Epoch: 3759 / 5000\n",
            "w1: [25.15250505] w2: [-23.01292707] bias: [16.3469692] loss: 30.565106378099006\n",
            "Epoch: 3760 / 5000\n",
            "w1: [25.15306794] w2: [-23.0136256] bias: [16.34688576] loss: 30.565025090189376\n",
            "Epoch: 3761 / 5000\n",
            "w1: [25.1536304] w2: [-23.0143234] bias: [16.34680233] loss: 30.56494395210097\n",
            "Epoch: 3762 / 5000\n",
            "w1: [25.15419244] w2: [-23.01502048] bias: [16.3467189] loss: 30.564862963549608\n",
            "Epoch: 3763 / 5000\n",
            "w1: [25.15475404] w2: [-23.01571684] bias: [16.34663548] loss: 30.564782124251657\n",
            "Epoch: 3764 / 5000\n",
            "w1: [25.15531523] w2: [-23.01641247] bias: [16.34655206] loss: 30.564701433924018\n",
            "Epoch: 3765 / 5000\n",
            "w1: [25.15587598] w2: [-23.01710738] bias: [16.34646864] loss: 30.56462089228413\n",
            "Epoch: 3766 / 5000\n",
            "w1: [25.15643631] w2: [-23.01780157] bias: [16.34638523] loss: 30.564540499049965\n",
            "Epoch: 3767 / 5000\n",
            "w1: [25.15699621] w2: [-23.01849504] bias: [16.34630183] loss: 30.564460253940037\n",
            "Epoch: 3768 / 5000\n",
            "w1: [25.15755569] w2: [-23.01918778] bias: [16.34621842] loss: 30.564380156673412\n",
            "Epoch: 3769 / 5000\n",
            "w1: [25.15811474] w2: [-23.01987981] bias: [16.34613503] loss: 30.564300206969676\n",
            "Epoch: 3770 / 5000\n",
            "w1: [25.15867336] w2: [-23.02057111] bias: [16.34605164] loss: 30.56422040454893\n",
            "Epoch: 3771 / 5000\n",
            "w1: [25.15923156] w2: [-23.0212617] bias: [16.34596825] loss: 30.56414074913185\n",
            "Epoch: 3772 / 5000\n",
            "w1: [25.15978934] w2: [-23.02195157] bias: [16.34588486] loss: 30.56406124043961\n",
            "Epoch: 3773 / 5000\n",
            "w1: [25.1603467] w2: [-23.02264072] bias: [16.34580149] loss: 30.563981878193943\n",
            "Epoch: 3774 / 5000\n",
            "w1: [25.16090363] w2: [-23.02332916] bias: [16.34571811] loss: 30.56390266211707\n",
            "Epoch: 3775 / 5000\n",
            "w1: [25.16146014] w2: [-23.02401688] bias: [16.34563474] loss: 30.56382359193178\n",
            "Epoch: 3776 / 5000\n",
            "w1: [25.16201622] w2: [-23.02470388] bias: [16.34555138] loss: 30.563744667361384\n",
            "Epoch: 3777 / 5000\n",
            "w1: [25.16257189] w2: [-23.02539017] bias: [16.34546802] loss: 30.56366588812971\n",
            "Epoch: 3778 / 5000\n",
            "w1: [25.16312713] w2: [-23.02607574] bias: [16.34538466] loss: 30.563587253961114\n",
            "Epoch: 3779 / 5000\n",
            "w1: [25.16368195] w2: [-23.02676061] bias: [16.34530131] loss: 30.56350876458047\n",
            "Epoch: 3780 / 5000\n",
            "w1: [25.16423635] w2: [-23.02744476] bias: [16.34521797] loss: 30.563430419713193\n",
            "Epoch: 3781 / 5000\n",
            "w1: [25.16479033] w2: [-23.02812819] bias: [16.34513462] loss: 30.563352219085214\n",
            "Epoch: 3782 / 5000\n",
            "w1: [25.16534389] w2: [-23.02881092] bias: [16.34505129] loss: 30.563274162422978\n",
            "Epoch: 3783 / 5000\n",
            "w1: [25.16589703] w2: [-23.02949294] bias: [16.34496795] loss: 30.563196249453462\n",
            "Epoch: 3784 / 5000\n",
            "w1: [25.16644975] w2: [-23.03017424] bias: [16.34488463] loss: 30.563118479904148\n",
            "Epoch: 3785 / 5000\n",
            "w1: [25.16700205] w2: [-23.03085484] bias: [16.3448013] loss: 30.56304085350307\n",
            "Epoch: 3786 / 5000\n",
            "w1: [25.16755393] w2: [-23.03153473] bias: [16.34471798] loss: 30.562963369978725\n",
            "Epoch: 3787 / 5000\n",
            "w1: [25.16810539] w2: [-23.03221391] bias: [16.34463467] loss: 30.56288602906019\n",
            "Epoch: 3788 / 5000\n",
            "w1: [25.16865644] w2: [-23.03289239] bias: [16.34455136] loss: 30.562808830477014\n",
            "Epoch: 3789 / 5000\n",
            "w1: [25.16920707] w2: [-23.03357015] bias: [16.34446806] loss: 30.56273177395927\n",
            "Epoch: 3790 / 5000\n",
            "w1: [25.16975728] w2: [-23.03424722] bias: [16.34438476] loss: 30.562654859237558\n",
            "Epoch: 3791 / 5000\n",
            "w1: [25.17030708] w2: [-23.03492357] bias: [16.34430146] loss: 30.562578086042972\n",
            "Epoch: 3792 / 5000\n",
            "w1: [25.17085646] w2: [-23.03559923] bias: [16.34421817] loss: 30.562501454107146\n",
            "Epoch: 3793 / 5000\n",
            "w1: [25.17140542] w2: [-23.03627418] bias: [16.34413489] loss: 30.56242496316219\n",
            "Epoch: 3794 / 5000\n",
            "w1: [25.17195397] w2: [-23.03694842] bias: [16.3440516] loss: 30.562348612940752\n",
            "Epoch: 3795 / 5000\n",
            "w1: [25.1725021] w2: [-23.03762197] bias: [16.34396833] loss: 30.562272403175985\n",
            "Epoch: 3796 / 5000\n",
            "w1: [25.17304982] w2: [-23.03829481] bias: [16.34388506] loss: 30.562196333601523\n",
            "Epoch: 3797 / 5000\n",
            "w1: [25.17359712] w2: [-23.03896695] bias: [16.34380179] loss: 30.56212040395155\n",
            "Epoch: 3798 / 5000\n",
            "w1: [25.17414401] w2: [-23.03963839] bias: [16.34371853] loss: 30.562044613960726\n",
            "Epoch: 3799 / 5000\n",
            "w1: [25.17469048] w2: [-23.04030913] bias: [16.34363527] loss: 30.56196896336423\n",
            "Epoch: 3800 / 5000\n",
            "w1: [25.17523655] w2: [-23.04097918] bias: [16.34355202] loss: 30.56189345189773\n",
            "Epoch: 3801 / 5000\n",
            "w1: [25.1757822] w2: [-23.04164852] bias: [16.34346877] loss: 30.561818079297428\n",
            "Epoch: 3802 / 5000\n",
            "w1: [25.17632743] w2: [-23.04231717] bias: [16.34338553] loss: 30.56174284529999\n",
            "Epoch: 3803 / 5000\n",
            "w1: [25.17687226] w2: [-23.04298512] bias: [16.34330229] loss: 30.56166774964261\n",
            "Epoch: 3804 / 5000\n",
            "w1: [25.17741667] w2: [-23.04365237] bias: [16.34321905] loss: 30.561592792062978\n",
            "Epoch: 3805 / 5000\n",
            "w1: [25.17796067] w2: [-23.04431893] bias: [16.34313582] loss: 30.561517972299274\n",
            "Epoch: 3806 / 5000\n",
            "w1: [25.17850427] w2: [-23.0449848] bias: [16.3430526] loss: 30.561443290090185\n",
            "Epoch: 3807 / 5000\n",
            "w1: [25.17904745] w2: [-23.04564997] bias: [16.34296938] loss: 30.561368745174892\n",
            "Epoch: 3808 / 5000\n",
            "w1: [25.17959022] w2: [-23.04631444] bias: [16.34288617] loss: 30.561294337293084\n",
            "Epoch: 3809 / 5000\n",
            "w1: [25.18013258] w2: [-23.04697822] bias: [16.34280296] loss: 30.56122006618493\n",
            "Epoch: 3810 / 5000\n",
            "w1: [25.18067453] w2: [-23.04764132] bias: [16.34271975] loss: 30.561145931591096\n",
            "Epoch: 3811 / 5000\n",
            "w1: [25.18121607] w2: [-23.04830372] bias: [16.34263655] loss: 30.561071933252748\n",
            "Epoch: 3812 / 5000\n",
            "w1: [25.1817572] w2: [-23.04896542] bias: [16.34255335] loss: 30.560998070911538\n",
            "Epoch: 3813 / 5000\n",
            "w1: [25.18229793] w2: [-23.04962644] bias: [16.34247016] loss: 30.560924344309637\n",
            "Epoch: 3814 / 5000\n",
            "w1: [25.18283824] w2: [-23.05028677] bias: [16.34238698] loss: 30.56085075318966\n",
            "Epoch: 3815 / 5000\n",
            "w1: [25.18337815] w2: [-23.05094641] bias: [16.3423038] loss: 30.560777297294745\n",
            "Epoch: 3816 / 5000\n",
            "w1: [25.18391766] w2: [-23.05160536] bias: [16.34222062] loss: 30.56070397636852\n",
            "Epoch: 3817 / 5000\n",
            "w1: [25.18445675] w2: [-23.05226363] bias: [16.34213745] loss: 30.56063079015508\n",
            "Epoch: 3818 / 5000\n",
            "w1: [25.18499544] w2: [-23.05292121] bias: [16.34205428] loss: 30.560557738399034\n",
            "Epoch: 3819 / 5000\n",
            "w1: [25.18553372] w2: [-23.0535781] bias: [16.34197112] loss: 30.56048482084545\n",
            "Epoch: 3820 / 5000\n",
            "w1: [25.1860716] w2: [-23.0542343] bias: [16.34188796] loss: 30.560412037239896\n",
            "Epoch: 3821 / 5000\n",
            "w1: [25.18660907] w2: [-23.05488982] bias: [16.34180481] loss: 30.560339387328433\n",
            "Epoch: 3822 / 5000\n",
            "w1: [25.18714614] w2: [-23.05554466] bias: [16.34172166] loss: 30.560266870857596\n",
            "Epoch: 3823 / 5000\n",
            "w1: [25.1876828] w2: [-23.05619881] bias: [16.34163852] loss: 30.560194487574392\n",
            "Epoch: 3824 / 5000\n",
            "w1: [25.18821906] w2: [-23.05685229] bias: [16.34155538] loss: 30.560122237226334\n",
            "Epoch: 3825 / 5000\n",
            "w1: [25.18875491] w2: [-23.05750507] bias: [16.34147225] loss: 30.56005011956139\n",
            "Epoch: 3826 / 5000\n",
            "w1: [25.18929036] w2: [-23.05815718] bias: [16.34138912] loss: 30.559978134328034\n",
            "Epoch: 3827 / 5000\n",
            "w1: [25.18982541] w2: [-23.05880861] bias: [16.341306] loss: 30.559906281275193\n",
            "Epoch: 3828 / 5000\n",
            "w1: [25.19036005] w2: [-23.05945935] bias: [16.34122288] loss: 30.559834560152286\n",
            "Epoch: 3829 / 5000\n",
            "w1: [25.1908943] w2: [-23.06010942] bias: [16.34113977] loss: 30.55976297070922\n",
            "Epoch: 3830 / 5000\n",
            "w1: [25.19142814] w2: [-23.0607588] bias: [16.34105666] loss: 30.559691512696357\n",
            "Epoch: 3831 / 5000\n",
            "w1: [25.19196158] w2: [-23.06140751] bias: [16.34097356] loss: 30.559620185864546\n",
            "Epoch: 3832 / 5000\n",
            "w1: [25.19249461] w2: [-23.06205554] bias: [16.34089046] loss: 30.559548989965112\n",
            "Epoch: 3833 / 5000\n",
            "w1: [25.19302725] w2: [-23.0627029] bias: [16.34080736] loss: 30.55947792474984\n",
            "Epoch: 3834 / 5000\n",
            "w1: [25.19355949] w2: [-23.06334958] bias: [16.34072427] loss: 30.559406989971006\n",
            "Epoch: 3835 / 5000\n",
            "w1: [25.19409132] w2: [-23.06399558] bias: [16.34064119] loss: 30.559336185381344\n",
            "Epoch: 3836 / 5000\n",
            "w1: [25.19462276] w2: [-23.06464091] bias: [16.34055811] loss: 30.559265510734075\n",
            "Epoch: 3837 / 5000\n",
            "w1: [25.19515379] w2: [-23.06528556] bias: [16.34047504] loss: 30.559194965782865\n",
            "Epoch: 3838 / 5000\n",
            "w1: [25.19568443] w2: [-23.06592954] bias: [16.34039197] loss: 30.559124550281876\n",
            "Epoch: 3839 / 5000\n",
            "w1: [25.19621467] w2: [-23.06657284] bias: [16.3403089] loss: 30.559054263985715\n",
            "Epoch: 3840 / 5000\n",
            "w1: [25.19674451] w2: [-23.06721548] bias: [16.34022585] loss: 30.558984106649472\n",
            "Epoch: 3841 / 5000\n",
            "w1: [25.19727395] w2: [-23.06785744] bias: [16.34014279] loss: 30.558914078028707\n",
            "Epoch: 3842 / 5000\n",
            "w1: [25.197803] w2: [-23.06849873] bias: [16.34005974] loss: 30.558844177879426\n",
            "Epoch: 3843 / 5000\n",
            "w1: [25.19833165] w2: [-23.06913935] bias: [16.3399767] loss: 30.55877440595811\n",
            "Epoch: 3844 / 5000\n",
            "w1: [25.1988599] w2: [-23.0697793] bias: [16.33989366] loss: 30.55870476202171\n",
            "Epoch: 3845 / 5000\n",
            "w1: [25.19938775] w2: [-23.07041858] bias: [16.33981062] loss: 30.55863524582763\n",
            "Epoch: 3846 / 5000\n",
            "w1: [25.19991521] w2: [-23.07105719] bias: [16.33972759] loss: 30.558565857133747\n",
            "Epoch: 3847 / 5000\n",
            "w1: [25.20044227] w2: [-23.07169514] bias: [16.33964457] loss: 30.558496595698386\n",
            "Epoch: 3848 / 5000\n",
            "w1: [25.20096894] w2: [-23.07233242] bias: [16.33956155] loss: 30.558427461280345\n",
            "Epoch: 3849 / 5000\n",
            "w1: [25.20149521] w2: [-23.07296903] bias: [16.33947854] loss: 30.558358453638863\n",
            "Epoch: 3850 / 5000\n",
            "w1: [25.20202109] w2: [-23.07360497] bias: [16.33939553] loss: 30.558289572533663\n",
            "Epoch: 3851 / 5000\n",
            "w1: [25.20254657] w2: [-23.07424025] bias: [16.33931252] loss: 30.558220817724905\n",
            "Epoch: 3852 / 5000\n",
            "w1: [25.20307166] w2: [-23.07487487] bias: [16.33922952] loss: 30.558152188973217\n",
            "Epoch: 3853 / 5000\n",
            "w1: [25.20359635] w2: [-23.07550882] bias: [16.33914653] loss: 30.558083686039673\n",
            "Epoch: 3854 / 5000\n",
            "w1: [25.20412066] w2: [-23.07614211] bias: [16.33906354] loss: 30.558015308685818\n",
            "Epoch: 3855 / 5000\n",
            "w1: [25.20464457] w2: [-23.07677473] bias: [16.33898055] loss: 30.557947056673633\n",
            "Epoch: 3856 / 5000\n",
            "w1: [25.20516808] w2: [-23.07740669] bias: [16.33889757] loss: 30.55787892976556\n",
            "Epoch: 3857 / 5000\n",
            "w1: [25.20569121] w2: [-23.078038] bias: [16.3388146] loss: 30.557810927724493\n",
            "Epoch: 3858 / 5000\n",
            "w1: [25.20621394] w2: [-23.07866864] bias: [16.33873163] loss: 30.55774305031379\n",
            "Epoch: 3859 / 5000\n",
            "w1: [25.20673628] w2: [-23.07929862] bias: [16.33864867] loss: 30.557675297297227\n",
            "Epoch: 3860 / 5000\n",
            "w1: [25.20725823] w2: [-23.07992794] bias: [16.33856571] loss: 30.557607668439072\n",
            "Epoch: 3861 / 5000\n",
            "w1: [25.20777979] w2: [-23.0805566] bias: [16.33848275] loss: 30.557540163504004\n",
            "Epoch: 3862 / 5000\n",
            "w1: [25.20830096] w2: [-23.0811846] bias: [16.3383998] loss: 30.557472782257182\n",
            "Epoch: 3863 / 5000\n",
            "w1: [25.20882174] w2: [-23.08181195] bias: [16.33831686] loss: 30.557405524464183\n",
            "Epoch: 3864 / 5000\n",
            "w1: [25.20934213] w2: [-23.08243864] bias: [16.33823392] loss: 30.557338389891058\n",
            "Epoch: 3865 / 5000\n",
            "w1: [25.20986213] w2: [-23.08306467] bias: [16.33815099] loss: 30.557271378304275\n",
            "Epoch: 3866 / 5000\n",
            "w1: [25.21038174] w2: [-23.08369005] bias: [16.33806806] loss: 30.557204489470777\n",
            "Epoch: 3867 / 5000\n",
            "w1: [25.21090096] w2: [-23.08431477] bias: [16.33798513] loss: 30.557137723157926\n",
            "Epoch: 3868 / 5000\n",
            "w1: [25.21141979] w2: [-23.08493884] bias: [16.33790222] loss: 30.557071079133532\n",
            "Epoch: 3869 / 5000\n",
            "w1: [25.21193824] w2: [-23.08556225] bias: [16.3378193] loss: 30.557004557165868\n",
            "Epoch: 3870 / 5000\n",
            "w1: [25.2124563] w2: [-23.08618501] bias: [16.33773639] loss: 30.556938157023627\n",
            "Epoch: 3871 / 5000\n",
            "w1: [25.21297397] w2: [-23.08680712] bias: [16.33765349] loss: 30.556871878475935\n",
            "Epoch: 3872 / 5000\n",
            "w1: [25.21349126] w2: [-23.08742857] bias: [16.33757059] loss: 30.55680572129238\n",
            "Epoch: 3873 / 5000\n",
            "w1: [25.21400815] w2: [-23.08804938] bias: [16.3374877] loss: 30.556739685242984\n",
            "Epoch: 3874 / 5000\n",
            "w1: [25.21452467] w2: [-23.08866953] bias: [16.33740481] loss: 30.55667377009819\n",
            "Epoch: 3875 / 5000\n",
            "w1: [25.21504079] w2: [-23.08928904] bias: [16.33732193] loss: 30.556607975628904\n",
            "Epoch: 3876 / 5000\n",
            "w1: [25.21555654] w2: [-23.08990789] bias: [16.33723905] loss: 30.556542301606445\n",
            "Epoch: 3877 / 5000\n",
            "w1: [25.21607189] w2: [-23.0905261] bias: [16.33715618] loss: 30.556476747802584\n",
            "Epoch: 3878 / 5000\n",
            "w1: [25.21658686] w2: [-23.09114365] bias: [16.33707331] loss: 30.556411313989503\n",
            "Epoch: 3879 / 5000\n",
            "w1: [25.21710145] w2: [-23.09176056] bias: [16.33699045] loss: 30.556345999939865\n",
            "Epoch: 3880 / 5000\n",
            "w1: [25.21761565] w2: [-23.09237683] bias: [16.33690759] loss: 30.556280805426706\n",
            "Epoch: 3881 / 5000\n",
            "w1: [25.21812947] w2: [-23.09299244] bias: [16.33682474] loss: 30.556215730223535\n",
            "Epoch: 3882 / 5000\n",
            "w1: [25.21864291] w2: [-23.09360741] bias: [16.33674189] loss: 30.556150774104285\n",
            "Epoch: 3883 / 5000\n",
            "w1: [25.21915596] w2: [-23.09422174] bias: [16.33665905] loss: 30.55608593684331\n",
            "Epoch: 3884 / 5000\n",
            "w1: [25.21966863] w2: [-23.09483542] bias: [16.33657621] loss: 30.556021218215403\n",
            "Epoch: 3885 / 5000\n",
            "w1: [25.22018092] w2: [-23.09544846] bias: [16.33649338] loss: 30.55595661799578\n",
            "Epoch: 3886 / 5000\n",
            "w1: [25.22069283] w2: [-23.09606085] bias: [16.33641056] loss: 30.555892135960093\n",
            "Epoch: 3887 / 5000\n",
            "w1: [25.22120435] w2: [-23.0966726] bias: [16.33632773] loss: 30.555827771884406\n",
            "Epoch: 3888 / 5000\n",
            "w1: [25.2217155] w2: [-23.09728371] bias: [16.33624492] loss: 30.555763525545224\n",
            "Epoch: 3889 / 5000\n",
            "w1: [25.22222626] w2: [-23.09789418] bias: [16.33616211] loss: 30.555699396719465\n",
            "Epoch: 3890 / 5000\n",
            "w1: [25.22273664] w2: [-23.09850401] bias: [16.3360793] loss: 30.555635385184495\n",
            "Epoch: 3891 / 5000\n",
            "w1: [25.22324664] w2: [-23.0991132] bias: [16.3359965] loss: 30.55557149071808\n",
            "Epoch: 3892 / 5000\n",
            "w1: [25.22375627] w2: [-23.09972175] bias: [16.33591371] loss: 30.555507713098418\n",
            "Epoch: 3893 / 5000\n",
            "w1: [25.22426551] w2: [-23.10032965] bias: [16.33583092] loss: 30.555444052104125\n",
            "Epoch: 3894 / 5000\n",
            "w1: [25.22477438] w2: [-23.10093693] bias: [16.33574813] loss: 30.555380507514244\n",
            "Epoch: 3895 / 5000\n",
            "w1: [25.22528286] w2: [-23.10154356] bias: [16.33566535] loss: 30.555317079108235\n",
            "Epoch: 3896 / 5000\n",
            "w1: [25.22579097] w2: [-23.10214956] bias: [16.33558258] loss: 30.555253766665995\n",
            "Epoch: 3897 / 5000\n",
            "w1: [25.2262987] w2: [-23.10275492] bias: [16.33549981] loss: 30.555190569967806\n",
            "Epoch: 3898 / 5000\n",
            "w1: [25.22680605] w2: [-23.10335964] bias: [16.33541705] loss: 30.5551274887944\n",
            "Epoch: 3899 / 5000\n",
            "w1: [25.22731302] w2: [-23.10396373] bias: [16.33533429] loss: 30.55506452292691\n",
            "Epoch: 3900 / 5000\n",
            "w1: [25.22781962] w2: [-23.10456718] bias: [16.33525154] loss: 30.555001672146894\n",
            "Epoch: 3901 / 5000\n",
            "w1: [25.22832584] w2: [-23.10517] bias: [16.33516879] loss: 30.55493893623631\n",
            "Epoch: 3902 / 5000\n",
            "w1: [25.22883168] w2: [-23.10577219] bias: [16.33508604] loss: 30.55487631497757\n",
            "Epoch: 3903 / 5000\n",
            "w1: [25.22933715] w2: [-23.10637375] bias: [16.33500331] loss: 30.554813808153444\n",
            "Epoch: 3904 / 5000\n",
            "w1: [25.22984224] w2: [-23.10697467] bias: [16.33492058] loss: 30.554751415547166\n",
            "Epoch: 3905 / 5000\n",
            "w1: [25.23034696] w2: [-23.10757496] bias: [16.33483785] loss: 30.554689136942347\n",
            "Epoch: 3906 / 5000\n",
            "w1: [25.2308513] w2: [-23.10817462] bias: [16.33475513] loss: 30.55462697212303\n",
            "Epoch: 3907 / 5000\n",
            "w1: [25.23135527] w2: [-23.10877365] bias: [16.33467241] loss: 30.55456492087368\n",
            "Epoch: 3908 / 5000\n",
            "w1: [25.23185886] w2: [-23.10937205] bias: [16.3345897] loss: 30.554502982979137\n",
            "Epoch: 3909 / 5000\n",
            "w1: [25.23236208] w2: [-23.10996982] bias: [16.33450699] loss: 30.55444115822468\n",
            "Epoch: 3910 / 5000\n",
            "w1: [25.23286493] w2: [-23.11056696] bias: [16.33442429] loss: 30.55437944639598\n",
            "Epoch: 3911 / 5000\n",
            "w1: [25.2333674] w2: [-23.11116348] bias: [16.3343416] loss: 30.55431784727914\n",
            "Epoch: 3912 / 5000\n",
            "w1: [25.2338695] w2: [-23.11175937] bias: [16.33425891] loss: 30.554256360660638\n",
            "Epoch: 3913 / 5000\n",
            "w1: [25.23437123] w2: [-23.11235463] bias: [16.33417622] loss: 30.554194986327378\n",
            "Epoch: 3914 / 5000\n",
            "w1: [25.23487258] w2: [-23.11294926] bias: [16.33409354] loss: 30.554133724066673\n",
            "Epoch: 3915 / 5000\n",
            "w1: [25.23537356] w2: [-23.11354327] bias: [16.33401087] loss: 30.554072573666232\n",
            "Epoch: 3916 / 5000\n",
            "w1: [25.23587418] w2: [-23.11413666] bias: [16.3339282] loss: 30.554011534914167\n",
            "Epoch: 3917 / 5000\n",
            "w1: [25.23637442] w2: [-23.11472942] bias: [16.33384554] loss: 30.553950607599003\n",
            "Epoch: 3918 / 5000\n",
            "w1: [25.23687429] w2: [-23.11532156] bias: [16.33376288] loss: 30.553889791509654\n",
            "Epoch: 3919 / 5000\n",
            "w1: [25.23737379] w2: [-23.11591307] bias: [16.33368023] loss: 30.55382908643545\n",
            "Epoch: 3920 / 5000\n",
            "w1: [25.23787291] w2: [-23.11650396] bias: [16.33359758] loss: 30.553768492166114\n",
            "Epoch: 3921 / 5000\n",
            "w1: [25.23837167] w2: [-23.11709423] bias: [16.33351494] loss: 30.553708008491775\n",
            "Epoch: 3922 / 5000\n",
            "w1: [25.23887006] w2: [-23.11768388] bias: [16.3334323] loss: 30.553647635202953\n",
            "Epoch: 3923 / 5000\n",
            "w1: [25.23936808] w2: [-23.11827291] bias: [16.33334967] loss: 30.55358737209058\n",
            "Epoch: 3924 / 5000\n",
            "w1: [25.23986573] w2: [-23.11886132] bias: [16.33326705] loss: 30.553527218945963\n",
            "Epoch: 3925 / 5000\n",
            "w1: [25.24036302] w2: [-23.1194491] bias: [16.33318443] loss: 30.55346717556084\n",
            "Epoch: 3926 / 5000\n",
            "w1: [25.24085993] w2: [-23.12003627] bias: [16.33310181] loss: 30.553407241727317\n",
            "Epoch: 3927 / 5000\n",
            "w1: [25.24135648] w2: [-23.12062283] bias: [16.3330192] loss: 30.553347417237905\n",
            "Epoch: 3928 / 5000\n",
            "w1: [25.24185266] w2: [-23.12120876] bias: [16.3329366] loss: 30.553287701885523\n",
            "Epoch: 3929 / 5000\n",
            "w1: [25.24234847] w2: [-23.12179408] bias: [16.332854] loss: 30.55322809546345\n",
            "Epoch: 3930 / 5000\n",
            "w1: [25.24284392] w2: [-23.12237878] bias: [16.3327714] loss: 30.553168597765403\n",
            "Epoch: 3931 / 5000\n",
            "w1: [25.243339] w2: [-23.12296286] bias: [16.33268882] loss: 30.553109208585465\n",
            "Epoch: 3932 / 5000\n",
            "w1: [25.24383371] w2: [-23.12354633] bias: [16.33260623] loss: 30.55304992771811\n",
            "Epoch: 3933 / 5000\n",
            "w1: [25.24432806] w2: [-23.12412919] bias: [16.33252366] loss: 30.552990754958213\n",
            "Epoch: 3934 / 5000\n",
            "w1: [25.24482204] w2: [-23.12471143] bias: [16.33244108] loss: 30.552931690101037\n",
            "Epoch: 3935 / 5000\n",
            "w1: [25.24531565] w2: [-23.12529305] bias: [16.33235852] loss: 30.55287273294223\n",
            "Epoch: 3936 / 5000\n",
            "w1: [25.24580891] w2: [-23.12587407] bias: [16.33227596] loss: 30.55281388327784\n",
            "Epoch: 3937 / 5000\n",
            "w1: [25.24630179] w2: [-23.12645447] bias: [16.3321934] loss: 30.55275514090429\n",
            "Epoch: 3938 / 5000\n",
            "w1: [25.24679432] w2: [-23.12703426] bias: [16.33211085] loss: 30.552696505618396\n",
            "Epoch: 3939 / 5000\n",
            "w1: [25.24728648] w2: [-23.12761344] bias: [16.33202831] loss: 30.55263797721737\n",
            "Epoch: 3940 / 5000\n",
            "w1: [25.24777827] w2: [-23.12819201] bias: [16.33194577] loss: 30.5525795554988\n",
            "Epoch: 3941 / 5000\n",
            "w1: [25.24826971] w2: [-23.12876997] bias: [16.33186323] loss: 30.552521240260656\n",
            "Epoch: 3942 / 5000\n",
            "w1: [25.24876078] w2: [-23.12934732] bias: [16.3317807] loss: 30.5524630313013\n",
            "Epoch: 3943 / 5000\n",
            "w1: [25.24925148] w2: [-23.12992406] bias: [16.33169818] loss: 30.552404928419485\n",
            "Epoch: 3944 / 5000\n",
            "w1: [25.24974183] w2: [-23.13050019] bias: [16.33161566] loss: 30.552346931414323\n",
            "Epoch: 3945 / 5000\n",
            "w1: [25.25023181] w2: [-23.13107572] bias: [16.33153315] loss: 30.552289040085345\n",
            "Epoch: 3946 / 5000\n",
            "w1: [25.25072144] w2: [-23.13165064] bias: [16.33145064] loss: 30.552231254232424\n",
            "Epoch: 3947 / 5000\n",
            "w1: [25.2512107] w2: [-23.13222495] bias: [16.33136814] loss: 30.552173573655836\n",
            "Epoch: 3948 / 5000\n",
            "w1: [25.2516996] w2: [-23.13279866] bias: [16.33128565] loss: 30.55211599815624\n",
            "Epoch: 3949 / 5000\n",
            "w1: [25.25218814] w2: [-23.13337176] bias: [16.33120316] loss: 30.55205852753467\n",
            "Epoch: 3950 / 5000\n",
            "w1: [25.25267632] w2: [-23.13394426] bias: [16.33112067] loss: 30.55200116159253\n",
            "Epoch: 3951 / 5000\n",
            "w1: [25.25316414] w2: [-23.13451615] bias: [16.33103819] loss: 30.55194390013161\n",
            "Epoch: 3952 / 5000\n",
            "w1: [25.2536516] w2: [-23.13508744] bias: [16.33095572] loss: 30.55188674295409\n",
            "Epoch: 3953 / 5000\n",
            "w1: [25.2541387] w2: [-23.13565812] bias: [16.33087325] loss: 30.5518296898625\n",
            "Epoch: 3954 / 5000\n",
            "w1: [25.25462544] w2: [-23.13622821] bias: [16.33079079] loss: 30.55177274065977\n",
            "Epoch: 3955 / 5000\n",
            "w1: [25.25511182] w2: [-23.13679769] bias: [16.33070833] loss: 30.551715895149187\n",
            "Epoch: 3956 / 5000\n",
            "w1: [25.25559785] w2: [-23.13736657] bias: [16.33062588] loss: 30.55165915313442\n",
            "Epoch: 3957 / 5000\n",
            "w1: [25.25608352] w2: [-23.13793485] bias: [16.33054343] loss: 30.551602514419514\n",
            "Epoch: 3958 / 5000\n",
            "w1: [25.25656883] w2: [-23.13850253] bias: [16.33046099] loss: 30.5515459788089\n",
            "Epoch: 3959 / 5000\n",
            "w1: [25.25705378] w2: [-23.13906961] bias: [16.33037856] loss: 30.551489546107348\n",
            "Epoch: 3960 / 5000\n",
            "w1: [25.25753838] w2: [-23.1396361] bias: [16.33029613] loss: 30.55143321612003\n",
            "Epoch: 3961 / 5000\n",
            "w1: [25.25802262] w2: [-23.14020198] bias: [16.3302137] loss: 30.551376988652475\n",
            "Epoch: 3962 / 5000\n",
            "w1: [25.2585065] w2: [-23.14076727] bias: [16.33013129] loss: 30.551320863510586\n",
            "Epoch: 3963 / 5000\n",
            "w1: [25.25899003] w2: [-23.14133196] bias: [16.33004887] loss: 30.55126484050063\n",
            "Epoch: 3964 / 5000\n",
            "w1: [25.2594732] w2: [-23.14189605] bias: [16.32996647] loss: 30.551208919429246\n",
            "Epoch: 3965 / 5000\n",
            "w1: [25.25995602] w2: [-23.14245955] bias: [16.32988406] loss: 30.551153100103456\n",
            "Epoch: 3966 / 5000\n",
            "w1: [25.26043848] w2: [-23.14302245] bias: [16.32980167] loss: 30.551097382330624\n",
            "Epoch: 3967 / 5000\n",
            "w1: [25.26092059] w2: [-23.14358476] bias: [16.32971928] loss: 30.551041765918495\n",
            "Epoch: 3968 / 5000\n",
            "w1: [25.26140234] w2: [-23.14414647] bias: [16.32963689] loss: 30.550986250675184\n",
            "Epoch: 3969 / 5000\n",
            "w1: [25.26188374] w2: [-23.14470759] bias: [16.32955451] loss: 30.550930836409165\n",
            "Epoch: 3970 / 5000\n",
            "w1: [25.26236479] w2: [-23.14526811] bias: [16.32947214] loss: 30.550875522929267\n",
            "Epoch: 3971 / 5000\n",
            "w1: [25.26284548] w2: [-23.14582805] bias: [16.32938977] loss: 30.550820310044706\n",
            "Epoch: 3972 / 5000\n",
            "w1: [25.26332582] w2: [-23.14638739] bias: [16.32930741] loss: 30.55076519756504\n",
            "Epoch: 3973 / 5000\n",
            "w1: [25.2638058] w2: [-23.14694614] bias: [16.32922505] loss: 30.5507101853002\n",
            "Epoch: 3974 / 5000\n",
            "w1: [25.26428544] w2: [-23.1475043] bias: [16.3291427] loss: 30.550655273060485\n",
            "Epoch: 3975 / 5000\n",
            "w1: [25.26476472] w2: [-23.14806187] bias: [16.32906035] loss: 30.550600460656533\n",
            "Epoch: 3976 / 5000\n",
            "w1: [25.26524365] w2: [-23.14861885] bias: [16.32897801] loss: 30.55054574789937\n",
            "Epoch: 3977 / 5000\n",
            "w1: [25.26572223] w2: [-23.14917524] bias: [16.32889568] loss: 30.550491134600364\n",
            "Epoch: 3978 / 5000\n",
            "w1: [25.26620046] w2: [-23.14973105] bias: [16.32881335] loss: 30.55043662057124\n",
            "Epoch: 3979 / 5000\n",
            "w1: [25.26667834] w2: [-23.15028626] bias: [16.32873102] loss: 30.550382205624096\n",
            "Epoch: 3980 / 5000\n",
            "w1: [25.26715586] w2: [-23.15084089] bias: [16.3286487] loss: 30.55032788957138\n",
            "Epoch: 3981 / 5000\n",
            "w1: [25.26763304] w2: [-23.15139493] bias: [16.32856639] loss: 30.5502736722259\n",
            "Epoch: 3982 / 5000\n",
            "w1: [25.26810987] w2: [-23.15194838] bias: [16.32848409] loss: 30.550219553400815\n",
            "Epoch: 3983 / 5000\n",
            "w1: [25.26858634] w2: [-23.15250125] bias: [16.32840178] loss: 30.55016553290963\n",
            "Epoch: 3984 / 5000\n",
            "w1: [25.26906247] w2: [-23.15305354] bias: [16.32831949] loss: 30.55011161056624\n",
            "Epoch: 3985 / 5000\n",
            "w1: [25.26953825] w2: [-23.15360524] bias: [16.3282372] loss: 30.550057786184855\n",
            "Epoch: 3986 / 5000\n",
            "w1: [25.27001368] w2: [-23.15415635] bias: [16.32815491] loss: 30.550004059580065\n",
            "Epoch: 3987 / 5000\n",
            "w1: [25.27048877] w2: [-23.15470688] bias: [16.32807264] loss: 30.549950430566795\n",
            "Epoch: 3988 / 5000\n",
            "w1: [25.2709635] w2: [-23.15525683] bias: [16.32799036] loss: 30.54989689896034\n",
            "Epoch: 3989 / 5000\n",
            "w1: [25.27143789] w2: [-23.1558062] bias: [16.3279081] loss: 30.54984346457633\n",
            "Epoch: 3990 / 5000\n",
            "w1: [25.27191193] w2: [-23.15635498] bias: [16.32782583] loss: 30.549790127230764\n",
            "Epoch: 3991 / 5000\n",
            "w1: [25.27238562] w2: [-23.15690319] bias: [16.32774358] loss: 30.549736886739968\n",
            "Epoch: 3992 / 5000\n",
            "w1: [25.27285897] w2: [-23.15745081] bias: [16.32766133] loss: 30.549683742920642\n",
            "Epoch: 3993 / 5000\n",
            "w1: [25.27333197] w2: [-23.15799786] bias: [16.32757908] loss: 30.549630695589823\n",
            "Epoch: 3994 / 5000\n",
            "w1: [25.27380463] w2: [-23.15854432] bias: [16.32749684] loss: 30.549577744564893\n",
            "Epoch: 3995 / 5000\n",
            "w1: [25.27427694] w2: [-23.15909021] bias: [16.32741461] loss: 30.549524889663584\n",
            "Epoch: 3996 / 5000\n",
            "w1: [25.2747489] w2: [-23.15963551] bias: [16.32733238] loss: 30.549472130703982\n",
            "Epoch: 3997 / 5000\n",
            "w1: [25.27522052] w2: [-23.16018024] bias: [16.32725016] loss: 30.54941946750451\n",
            "Epoch: 3998 / 5000\n",
            "w1: [25.27569179] w2: [-23.16072439] bias: [16.32716795] loss: 30.54936689988395\n",
            "Epoch: 3999 / 5000\n",
            "w1: [25.27616272] w2: [-23.16126797] bias: [16.32708574] loss: 30.549314427661415\n",
            "Epoch: 4000 / 5000\n",
            "w1: [25.27663331] w2: [-23.16181097] bias: [16.32700353] loss: 30.54926205065636\n",
            "Epoch: 4001 / 5000\n",
            "w1: [25.27710355] w2: [-23.16235339] bias: [16.32692133] loss: 30.549209768688605\n",
            "Epoch: 4002 / 5000\n",
            "w1: [25.27757345] w2: [-23.16289524] bias: [16.32683914] loss: 30.549157581578285\n",
            "Epoch: 4003 / 5000\n",
            "w1: [25.278043] w2: [-23.16343652] bias: [16.32675695] loss: 30.549105489145912\n",
            "Epoch: 4004 / 5000\n",
            "w1: [25.27851222] w2: [-23.16397722] bias: [16.32667477] loss: 30.5490534912123\n",
            "Epoch: 4005 / 5000\n",
            "w1: [25.27898109] w2: [-23.16451735] bias: [16.32659259] loss: 30.549001587598635\n",
            "Epoch: 4006 / 5000\n",
            "w1: [25.27944962] w2: [-23.1650569] bias: [16.32651042] loss: 30.548949778126424\n",
            "Epoch: 4007 / 5000\n",
            "w1: [25.2799178] w2: [-23.16559588] bias: [16.32642826] loss: 30.54889806261753\n",
            "Epoch: 4008 / 5000\n",
            "w1: [25.28038565] w2: [-23.16613429] bias: [16.3263461] loss: 30.54884644089415\n",
            "Epoch: 4009 / 5000\n",
            "w1: [25.28085315] w2: [-23.16667213] bias: [16.32626395] loss: 30.548794912778803\n",
            "Epoch: 4010 / 5000\n",
            "w1: [25.28132031] w2: [-23.1672094] bias: [16.3261818] loss: 30.54874347809437\n",
            "Epoch: 4011 / 5000\n",
            "w1: [25.28178713] w2: [-23.1677461] bias: [16.32609966] loss: 30.548692136664062\n",
            "Epoch: 4012 / 5000\n",
            "w1: [25.28225361] w2: [-23.16828223] bias: [16.32601752] loss: 30.548640888311414\n",
            "Epoch: 4013 / 5000\n",
            "w1: [25.28271976] w2: [-23.16881779] bias: [16.32593539] loss: 30.54858973286031\n",
            "Epoch: 4014 / 5000\n",
            "w1: [25.28318556] w2: [-23.16935279] bias: [16.32585327] loss: 30.548538670134977\n",
            "Epoch: 4015 / 5000\n",
            "w1: [25.28365102] w2: [-23.16988721] bias: [16.32577115] loss: 30.54848769995995\n",
            "Epoch: 4016 / 5000\n",
            "w1: [25.28411614] w2: [-23.17042107] bias: [16.32568903] loss: 30.548436822160127\n",
            "Epoch: 4017 / 5000\n",
            "w1: [25.28458093] w2: [-23.17095436] bias: [16.32560693] loss: 30.54838603656071\n",
            "Epoch: 4018 / 5000\n",
            "w1: [25.28504537] w2: [-23.17148709] bias: [16.32552483] loss: 30.54833534298726\n",
            "Epoch: 4019 / 5000\n",
            "w1: [25.28550948] w2: [-23.17201925] bias: [16.32544273] loss: 30.548284741265668\n",
            "Epoch: 4020 / 5000\n",
            "w1: [25.28597325] w2: [-23.17255084] bias: [16.32536064] loss: 30.548234231222132\n",
            "Epoch: 4021 / 5000\n",
            "w1: [25.28643668] w2: [-23.17308187] bias: [16.32527856] loss: 30.548183812683213\n",
            "Epoch: 4022 / 5000\n",
            "w1: [25.28689978] w2: [-23.17361234] bias: [16.32519648] loss: 30.54813348547577\n",
            "Epoch: 4023 / 5000\n",
            "w1: [25.28736254] w2: [-23.17414224] bias: [16.32511441] loss: 30.548083249427027\n",
            "Epoch: 4024 / 5000\n",
            "w1: [25.28782496] w2: [-23.17467158] bias: [16.32503234] loss: 30.548033104364503\n",
            "Epoch: 4025 / 5000\n",
            "w1: [25.28828704] w2: [-23.17520036] bias: [16.32495028] loss: 30.547983050116066\n",
            "Epoch: 4026 / 5000\n",
            "w1: [25.28874879] w2: [-23.17572857] bias: [16.32486823] loss: 30.547933086509918\n",
            "Epoch: 4027 / 5000\n",
            "w1: [25.28921021] w2: [-23.17625623] bias: [16.32478618] loss: 30.547883213374554\n",
            "Epoch: 4028 / 5000\n",
            "w1: [25.28967129] w2: [-23.17678332] bias: [16.32470413] loss: 30.547833430538837\n",
            "Epoch: 4029 / 5000\n",
            "w1: [25.29013203] w2: [-23.17730985] bias: [16.3246221] loss: 30.54778373783192\n",
            "Epoch: 4030 / 5000\n",
            "w1: [25.29059244] w2: [-23.17783583] bias: [16.32454006] loss: 30.54773413508332\n",
            "Epoch: 4031 / 5000\n",
            "w1: [25.29105251] w2: [-23.17836124] bias: [16.32445804] loss: 30.547684622122837\n",
            "Epoch: 4032 / 5000\n",
            "w1: [25.29151225] w2: [-23.1788861] bias: [16.32437602] loss: 30.547635198780625\n",
            "Epoch: 4033 / 5000\n",
            "w1: [25.29197166] w2: [-23.1794104] bias: [16.324294] loss: 30.547585864887157\n",
            "Epoch: 4034 / 5000\n",
            "w1: [25.29243073] w2: [-23.17993414] bias: [16.324212] loss: 30.5475366202732\n",
            "Epoch: 4035 / 5000\n",
            "w1: [25.29288947] w2: [-23.18045732] bias: [16.32412999] loss: 30.547487464769894\n",
            "Epoch: 4036 / 5000\n",
            "w1: [25.29334788] w2: [-23.18097995] bias: [16.324048] loss: 30.547438398208648\n",
            "Epoch: 4037 / 5000\n",
            "w1: [25.29380595] w2: [-23.18150202] bias: [16.32396601] loss: 30.54738942042124\n",
            "Epoch: 4038 / 5000\n",
            "w1: [25.29426369] w2: [-23.18202354] bias: [16.32388402] loss: 30.547340531239726\n",
            "Epoch: 4039 / 5000\n",
            "w1: [25.2947211] w2: [-23.1825445] bias: [16.32380204] loss: 30.547291730496514\n",
            "Epoch: 4040 / 5000\n",
            "w1: [25.29517818] w2: [-23.18306491] bias: [16.32372007] loss: 30.54724301802431\n",
            "Epoch: 4041 / 5000\n",
            "w1: [25.29563492] w2: [-23.18358476] bias: [16.3236381] loss: 30.547194393656145\n",
            "Epoch: 4042 / 5000\n",
            "w1: [25.29609134] w2: [-23.18410406] bias: [16.32355614] loss: 30.547145857225377\n",
            "Epoch: 4043 / 5000\n",
            "w1: [25.29654742] w2: [-23.18462281] bias: [16.32347419] loss: 30.547097408565676\n",
            "Epoch: 4044 / 5000\n",
            "w1: [25.29700318] w2: [-23.18514101] bias: [16.32339224] loss: 30.547049047511003\n",
            "Epoch: 4045 / 5000\n",
            "w1: [25.2974586] w2: [-23.18565865] bias: [16.32331029] loss: 30.547000773895682\n",
            "Epoch: 4046 / 5000\n",
            "w1: [25.29791369] w2: [-23.18617575] bias: [16.32322836] loss: 30.546952587554326\n",
            "Epoch: 4047 / 5000\n",
            "w1: [25.29836846] w2: [-23.18669229] bias: [16.32314642] loss: 30.546904488321857\n",
            "Epoch: 4048 / 5000\n",
            "w1: [25.29882289] w2: [-23.18720828] bias: [16.3230645] loss: 30.546856476033525\n",
            "Epoch: 4049 / 5000\n",
            "w1: [25.299277] w2: [-23.18772373] bias: [16.32298258] loss: 30.546808550524883\n",
            "Epoch: 4050 / 5000\n",
            "w1: [25.29973077] w2: [-23.18823862] bias: [16.32290066] loss: 30.546760711631816\n",
            "Epoch: 4051 / 5000\n",
            "w1: [25.30018422] w2: [-23.18875297] bias: [16.32281876] loss: 30.54671295919049\n",
            "Epoch: 4052 / 5000\n",
            "w1: [25.30063734] w2: [-23.18926677] bias: [16.32273685] loss: 30.546665293037414\n",
            "Epoch: 4053 / 5000\n",
            "w1: [25.30109013] w2: [-23.18978002] bias: [16.32265496] loss: 30.54661771300939\n",
            "Epoch: 4054 / 5000\n",
            "w1: [25.3015426] w2: [-23.19029273] bias: [16.32257307] loss: 30.54657021894354\n",
            "Epoch: 4055 / 5000\n",
            "w1: [25.30199474] w2: [-23.19080489] bias: [16.32249118] loss: 30.546522810677285\n",
            "Epoch: 4056 / 5000\n",
            "w1: [25.30244655] w2: [-23.1913165] bias: [16.32240931] loss: 30.546475488048372\n",
            "Epoch: 4057 / 5000\n",
            "w1: [25.30289803] w2: [-23.19182757] bias: [16.32232743] loss: 30.546428250894845\n",
            "Epoch: 4058 / 5000\n",
            "w1: [25.30334919] w2: [-23.19233809] bias: [16.32224557] loss: 30.546381099055047\n",
            "Epoch: 4059 / 5000\n",
            "w1: [25.30380002] w2: [-23.19284807] bias: [16.32216371] loss: 30.546334032367653\n",
            "Epoch: 4060 / 5000\n",
            "w1: [25.30425053] w2: [-23.19335751] bias: [16.32208185] loss: 30.546287050671634\n",
            "Epoch: 4061 / 5000\n",
            "w1: [25.30470071] w2: [-23.1938664] bias: [16.322] loss: 30.54624015380626\n",
            "Epoch: 4062 / 5000\n",
            "w1: [25.30515056] w2: [-23.19437475] bias: [16.32191816] loss: 30.546193341611115\n",
            "Epoch: 4063 / 5000\n",
            "w1: [25.30560009] w2: [-23.19488256] bias: [16.32183632] loss: 30.546146613926087\n",
            "Epoch: 4064 / 5000\n",
            "w1: [25.3060493] w2: [-23.19538982] bias: [16.32175449] loss: 30.546099970591364\n",
            "Epoch: 4065 / 5000\n",
            "w1: [25.30649818] w2: [-23.19589655] bias: [16.32167267] loss: 30.54605341144746\n",
            "Epoch: 4066 / 5000\n",
            "w1: [25.30694674] w2: [-23.19640273] bias: [16.32159085] loss: 30.54600693633515\n",
            "Epoch: 4067 / 5000\n",
            "w1: [25.30739497] w2: [-23.19690838] bias: [16.32150904] loss: 30.545960545095546\n",
            "Epoch: 4068 / 5000\n",
            "w1: [25.30784288] w2: [-23.19741349] bias: [16.32142723] loss: 30.545914237570074\n",
            "Epoch: 4069 / 5000\n",
            "w1: [25.30829047] w2: [-23.19791805] bias: [16.32134543] loss: 30.545868013600412\n",
            "Epoch: 4070 / 5000\n",
            "w1: [25.30873774] w2: [-23.19842208] bias: [16.32126363] loss: 30.545821873028594\n",
            "Epoch: 4071 / 5000\n",
            "w1: [25.30918468] w2: [-23.19892557] bias: [16.32118185] loss: 30.545775815696924\n",
            "Epoch: 4072 / 5000\n",
            "w1: [25.3096313] w2: [-23.19942853] bias: [16.32110006] loss: 30.545729841447997\n",
            "Epoch: 4073 / 5000\n",
            "w1: [25.3100776] w2: [-23.19993094] bias: [16.32101829] loss: 30.545683950124744\n",
            "Epoch: 4074 / 5000\n",
            "w1: [25.31052357] w2: [-23.20043282] bias: [16.32093651] loss: 30.545638141570358\n",
            "Epoch: 4075 / 5000\n",
            "w1: [25.31096923] w2: [-23.20093417] bias: [16.32085475] loss: 30.545592415628352\n",
            "Epoch: 4076 / 5000\n",
            "w1: [25.31141456] w2: [-23.20143498] bias: [16.32077299] loss: 30.545546772142536\n",
            "Epoch: 4077 / 5000\n",
            "w1: [25.31185958] w2: [-23.20193525] bias: [16.32069124] loss: 30.545501210957006\n",
            "Epoch: 4078 / 5000\n",
            "w1: [25.31230427] w2: [-23.20243499] bias: [16.32060949] loss: 30.545455731916174\n",
            "Epoch: 4079 / 5000\n",
            "w1: [25.31274864] w2: [-23.2029342] bias: [16.32052775] loss: 30.545410334864716\n",
            "Epoch: 4080 / 5000\n",
            "w1: [25.31319269] w2: [-23.20343288] bias: [16.32044602] loss: 30.545365019647637\n",
            "Epoch: 4081 / 5000\n",
            "w1: [25.31363643] w2: [-23.20393102] bias: [16.32036429] loss: 30.545319786110223\n",
            "Epoch: 4082 / 5000\n",
            "w1: [25.31407984] w2: [-23.20442863] bias: [16.32028257] loss: 30.54527463409805\n",
            "Epoch: 4083 / 5000\n",
            "w1: [25.31452294] w2: [-23.2049257] bias: [16.32020085] loss: 30.545229563456996\n",
            "Epoch: 4084 / 5000\n",
            "w1: [25.31496571] w2: [-23.20542225] bias: [16.32011914] loss: 30.545184574033232\n",
            "Epoch: 4085 / 5000\n",
            "w1: [25.31540817] w2: [-23.20591827] bias: [16.32003743] loss: 30.54513966567322\n",
            "Epoch: 4086 / 5000\n",
            "w1: [25.31585031] w2: [-23.20641375] bias: [16.31995574] loss: 30.545094838223708\n",
            "Epoch: 4087 / 5000\n",
            "w1: [25.31629213] w2: [-23.20690871] bias: [16.31987404] loss: 30.545050091531742\n",
            "Epoch: 4088 / 5000\n",
            "w1: [25.31673364] w2: [-23.20740314] bias: [16.31979236] loss: 30.545005425444664\n",
            "Epoch: 4089 / 5000\n",
            "w1: [25.31717483] w2: [-23.20789703] bias: [16.31971068] loss: 30.544960839810095\n",
            "Epoch: 4090 / 5000\n",
            "w1: [25.3176157] w2: [-23.20839041] bias: [16.319629] loss: 30.54491633447596\n",
            "Epoch: 4091 / 5000\n",
            "w1: [25.31805625] w2: [-23.20888325] bias: [16.31954734] loss: 30.544871909290464\n",
            "Epoch: 4092 / 5000\n",
            "w1: [25.31849649] w2: [-23.20937557] bias: [16.31946567] loss: 30.544827564102093\n",
            "Epoch: 4093 / 5000\n",
            "w1: [25.31893641] w2: [-23.20986736] bias: [16.31938402] loss: 30.544783298759643\n",
            "Epoch: 4094 / 5000\n",
            "w1: [25.31937601] w2: [-23.21035862] bias: [16.31930237] loss: 30.54473911311218\n",
            "Epoch: 4095 / 5000\n",
            "w1: [25.3198153] w2: [-23.21084936] bias: [16.31922072] loss: 30.544695007009068\n",
            "Epoch: 4096 / 5000\n",
            "w1: [25.32025428] w2: [-23.21133957] bias: [16.31913909] loss: 30.544650980299945\n",
            "Epoch: 4097 / 5000\n",
            "w1: [25.32069294] w2: [-23.21182926] bias: [16.31905746] loss: 30.544607032834755\n",
            "Epoch: 4098 / 5000\n",
            "w1: [25.32113128] w2: [-23.21231842] bias: [16.31897583] loss: 30.5445631644637\n",
            "Epoch: 4099 / 5000\n",
            "w1: [25.32156931] w2: [-23.21280707] bias: [16.31889421] loss: 30.544519375037307\n",
            "Epoch: 4100 / 5000\n",
            "w1: [25.32200703] w2: [-23.21329519] bias: [16.3188126] loss: 30.54447566440634\n",
            "Epoch: 4101 / 5000\n",
            "w1: [25.32244443] w2: [-23.21378278] bias: [16.31873099] loss: 30.54443203242189\n",
            "Epoch: 4102 / 5000\n",
            "w1: [25.32288152] w2: [-23.21426986] bias: [16.31864939] loss: 30.544388478935296\n",
            "Epoch: 4103 / 5000\n",
            "w1: [25.3233183] w2: [-23.21475641] bias: [16.3185678] loss: 30.544345003798206\n",
            "Epoch: 4104 / 5000\n",
            "w1: [25.32375476] w2: [-23.21524244] bias: [16.31848621] loss: 30.544301606862543\n",
            "Epoch: 4105 / 5000\n",
            "w1: [25.32419091] w2: [-23.21572795] bias: [16.31840462] loss: 30.544258287980497\n",
            "Epoch: 4106 / 5000\n",
            "w1: [25.32462675] w2: [-23.21621294] bias: [16.31832305] loss: 30.54421504700457\n",
            "Epoch: 4107 / 5000\n",
            "w1: [25.32506227] w2: [-23.21669742] bias: [16.31824148] loss: 30.544171883787516\n",
            "Epoch: 4108 / 5000\n",
            "w1: [25.32549749] w2: [-23.21718137] bias: [16.31815991] loss: 30.54412879818238\n",
            "Epoch: 4109 / 5000\n",
            "w1: [25.32593239] w2: [-23.21766481] bias: [16.31807836] loss: 30.544085790042487\n",
            "Epoch: 4110 / 5000\n",
            "w1: [25.32636698] w2: [-23.21814772] bias: [16.3179968] loss: 30.544042859221445\n",
            "Epoch: 4111 / 5000\n",
            "w1: [25.32680126] w2: [-23.21863012] bias: [16.31791526] loss: 30.54400000557314\n",
            "Epoch: 4112 / 5000\n",
            "w1: [25.32723523] w2: [-23.21911201] bias: [16.31783372] loss: 30.54395722895173\n",
            "Epoch: 4113 / 5000\n",
            "w1: [25.32766889] w2: [-23.21959337] bias: [16.31775219] loss: 30.543914529211644\n",
            "Epoch: 4114 / 5000\n",
            "w1: [25.32810224] w2: [-23.22007422] bias: [16.31767066] loss: 30.543871906207606\n",
            "Epoch: 4115 / 5000\n",
            "w1: [25.32853528] w2: [-23.22055456] bias: [16.31758914] loss: 30.54382935979461\n",
            "Epoch: 4116 / 5000\n",
            "w1: [25.32896801] w2: [-23.22103438] bias: [16.31750762] loss: 30.543786889827924\n",
            "Epoch: 4117 / 5000\n",
            "w1: [25.32940043] w2: [-23.22151368] bias: [16.31742611] loss: 30.543744496163093\n",
            "Epoch: 4118 / 5000\n",
            "w1: [25.32983254] w2: [-23.22199248] bias: [16.31734461] loss: 30.543702178655924\n",
            "Epoch: 4119 / 5000\n",
            "w1: [25.33026434] w2: [-23.22247076] bias: [16.31726312] loss: 30.54365993716253\n",
            "Epoch: 4120 / 5000\n",
            "w1: [25.33069584] w2: [-23.22294852] bias: [16.31718163] loss: 30.543617771539264\n",
            "Epoch: 4121 / 5000\n",
            "w1: [25.33112702] w2: [-23.22342577] bias: [16.31710014] loss: 30.543575681642768\n",
            "Epoch: 4122 / 5000\n",
            "w1: [25.3315579] w2: [-23.22390252] bias: [16.31701866] loss: 30.543533667329957\n",
            "Epoch: 4123 / 5000\n",
            "w1: [25.33198847] w2: [-23.22437875] bias: [16.31693719] loss: 30.543491728458026\n",
            "Epoch: 4124 / 5000\n",
            "w1: [25.33241873] w2: [-23.22485447] bias: [16.31685573] loss: 30.543449864884423\n",
            "Epoch: 4125 / 5000\n",
            "w1: [25.33284869] w2: [-23.22532967] bias: [16.31677427] loss: 30.543408076466875\n",
            "Epoch: 4126 / 5000\n",
            "w1: [25.33327834] w2: [-23.22580437] bias: [16.31669281] loss: 30.543366363063388\n",
            "Epoch: 4127 / 5000\n",
            "w1: [25.33370768] w2: [-23.22627856] bias: [16.31661137] loss: 30.543324724532233\n",
            "Epoch: 4128 / 5000\n",
            "w1: [25.33413672] w2: [-23.22675224] bias: [16.31652993] loss: 30.543283160731942\n",
            "Epoch: 4129 / 5000\n",
            "w1: [25.33456545] w2: [-23.22722542] bias: [16.31644849] loss: 30.543241671521336\n",
            "Epoch: 4130 / 5000\n",
            "w1: [25.33499388] w2: [-23.22769808] bias: [16.31636707] loss: 30.54320025675949\n",
            "Epoch: 4131 / 5000\n",
            "w1: [25.335422] w2: [-23.22817024] bias: [16.31628564] loss: 30.543158916305742\n",
            "Epoch: 4132 / 5000\n",
            "w1: [25.33584981] w2: [-23.22864189] bias: [16.31620423] loss: 30.543117650019713\n",
            "Epoch: 4133 / 5000\n",
            "w1: [25.33627732] w2: [-23.22911303] bias: [16.31612282] loss: 30.543076457761288\n",
            "Epoch: 4134 / 5000\n",
            "w1: [25.33670453] w2: [-23.22958367] bias: [16.31604142] loss: 30.543035339390606\n",
            "Epoch: 4135 / 5000\n",
            "w1: [25.33713143] w2: [-23.2300538] bias: [16.31596002] loss: 30.542994294768093\n",
            "Epoch: 4136 / 5000\n",
            "w1: [25.33755802] w2: [-23.23052343] bias: [16.31587863] loss: 30.542953323754414\n",
            "Epoch: 4137 / 5000\n",
            "w1: [25.33798432] w2: [-23.23099255] bias: [16.31579724] loss: 30.54291242621053\n",
            "Epoch: 4138 / 5000\n",
            "w1: [25.33841031] w2: [-23.23146117] bias: [16.31571587] loss: 30.542871601997636\n",
            "Epoch: 4139 / 5000\n",
            "w1: [25.33883599] w2: [-23.23192928] bias: [16.31563449] loss: 30.54283085097723\n",
            "Epoch: 4140 / 5000\n",
            "w1: [25.33926138] w2: [-23.23239689] bias: [16.31555313] loss: 30.542790173011014\n",
            "Epoch: 4141 / 5000\n",
            "w1: [25.33968646] w2: [-23.232864] bias: [16.31547177] loss: 30.542749567961017\n",
            "Epoch: 4142 / 5000\n",
            "w1: [25.34011124] w2: [-23.23333061] bias: [16.31539042] loss: 30.542709035689498\n",
            "Epoch: 4143 / 5000\n",
            "w1: [25.34053572] w2: [-23.23379671] bias: [16.31530907] loss: 30.542668576058972\n",
            "Epoch: 4144 / 5000\n",
            "w1: [25.34095989] w2: [-23.23426232] bias: [16.31522773] loss: 30.54262818893224\n",
            "Epoch: 4145 / 5000\n",
            "w1: [25.34138376] w2: [-23.23472742] bias: [16.3151464] loss: 30.54258787417234\n",
            "Epoch: 4146 / 5000\n",
            "w1: [25.34180734] w2: [-23.23519203] bias: [16.31506507] loss: 30.54254763164259\n",
            "Epoch: 4147 / 5000\n",
            "w1: [25.34223061] w2: [-23.23565613] bias: [16.31498375] loss: 30.542507461206554\n",
            "Epoch: 4148 / 5000\n",
            "w1: [25.34265358] w2: [-23.23611974] bias: [16.31490243] loss: 30.542467362728065\n",
            "Epoch: 4149 / 5000\n",
            "w1: [25.34307625] w2: [-23.23658284] bias: [16.31482112] loss: 30.542427336071217\n",
            "Epoch: 4150 / 5000\n",
            "w1: [25.34349862] w2: [-23.23704545] bias: [16.31473982] loss: 30.54238738110034\n",
            "Epoch: 4151 / 5000\n",
            "w1: [25.34392069] w2: [-23.23750756] bias: [16.31465852] loss: 30.54234749768005\n",
            "Epoch: 4152 / 5000\n",
            "w1: [25.34434246] w2: [-23.23796917] bias: [16.31457723] loss: 30.542307685675222\n",
            "Epoch: 4153 / 5000\n",
            "w1: [25.34476393] w2: [-23.23843029] bias: [16.31449595] loss: 30.54226794495095\n",
            "Epoch: 4154 / 5000\n",
            "w1: [25.34518511] w2: [-23.23889091] bias: [16.31441467] loss: 30.542228275372633\n",
            "Epoch: 4155 / 5000\n",
            "w1: [25.34560598] w2: [-23.23935103] bias: [16.3143334] loss: 30.5421886768059\n",
            "Epoch: 4156 / 5000\n",
            "w1: [25.34602655] w2: [-23.23981066] bias: [16.31425213] loss: 30.542149149116636\n",
            "Epoch: 4157 / 5000\n",
            "w1: [25.34644683] w2: [-23.2402698] bias: [16.31417088] loss: 30.542109692170982\n",
            "Epoch: 4158 / 5000\n",
            "w1: [25.34686681] w2: [-23.24072844] bias: [16.31408962] loss: 30.54207030583535\n",
            "Epoch: 4159 / 5000\n",
            "w1: [25.34728649] w2: [-23.24118658] bias: [16.31400838] loss: 30.542030989976382\n",
            "Epoch: 4160 / 5000\n",
            "w1: [25.34770587] w2: [-23.24164423] bias: [16.31392714] loss: 30.54199174446099\n",
            "Epoch: 4161 / 5000\n",
            "w1: [25.34812496] w2: [-23.24210139] bias: [16.31384591] loss: 30.541952569156333\n",
            "Epoch: 4162 / 5000\n",
            "w1: [25.34854375] w2: [-23.24255806] bias: [16.31376468] loss: 30.541913463929827\n",
            "Epoch: 4163 / 5000\n",
            "w1: [25.34896224] w2: [-23.24301424] bias: [16.31368346] loss: 30.541874428649137\n",
            "Epoch: 4164 / 5000\n",
            "w1: [25.34938044] w2: [-23.24346992] bias: [16.31360224] loss: 30.54183546318218\n",
            "Epoch: 4165 / 5000\n",
            "w1: [25.34979834] w2: [-23.24392511] bias: [16.31352104] loss: 30.541796567397128\n",
            "Epoch: 4166 / 5000\n",
            "w1: [25.35021595] w2: [-23.24437981] bias: [16.31343983] loss: 30.541757741162396\n",
            "Epoch: 4167 / 5000\n",
            "w1: [25.35063326] w2: [-23.24483403] bias: [16.31335864] loss: 30.541718984346662\n",
            "Epoch: 4168 / 5000\n",
            "w1: [25.35105027] w2: [-23.24528775] bias: [16.31327745] loss: 30.54168029681884\n",
            "Epoch: 4169 / 5000\n",
            "w1: [25.35146699] w2: [-23.24574098] bias: [16.31319627] loss: 30.541641678448105\n",
            "Epoch: 4170 / 5000\n",
            "w1: [25.35188341] w2: [-23.24619373] bias: [16.31311509] loss: 30.541603129103873\n",
            "Epoch: 4171 / 5000\n",
            "w1: [25.35229954] w2: [-23.24664598] bias: [16.31303392] loss: 30.541564648655807\n",
            "Epoch: 4172 / 5000\n",
            "w1: [25.35271538] w2: [-23.24709775] bias: [16.31295276] loss: 30.541526236973834\n",
            "Epoch: 4173 / 5000\n",
            "w1: [25.35313092] w2: [-23.24754903] bias: [16.3128716] loss: 30.54148789392811\n",
            "Epoch: 4174 / 5000\n",
            "w1: [25.35354617] w2: [-23.24799983] bias: [16.31279045] loss: 30.541449619389052\n",
            "Epoch: 4175 / 5000\n",
            "w1: [25.35396112] w2: [-23.24845014] bias: [16.31270931] loss: 30.54141141322731\n",
            "Epoch: 4176 / 5000\n",
            "w1: [25.35437578] w2: [-23.24889996] bias: [16.31262817] loss: 30.54137327531379\n",
            "Epoch: 4177 / 5000\n",
            "w1: [25.35479015] w2: [-23.2493493] bias: [16.31254704] loss: 30.54133520551964\n",
            "Epoch: 4178 / 5000\n",
            "w1: [25.35520423] w2: [-23.24979815] bias: [16.31246591] loss: 30.541297203716262\n",
            "Epoch: 4179 / 5000\n",
            "w1: [25.35561801] w2: [-23.25024652] bias: [16.31238479] loss: 30.541259269775278\n",
            "Epoch: 4180 / 5000\n",
            "w1: [25.3560315] w2: [-23.2506944] bias: [16.31230368] loss: 30.541221403568596\n",
            "Epoch: 4181 / 5000\n",
            "w1: [25.3564447] w2: [-23.25114181] bias: [16.31222258] loss: 30.541183604968314\n",
            "Epoch: 4182 / 5000\n",
            "w1: [25.35685761] w2: [-23.25158872] bias: [16.31214148] loss: 30.54114587384683\n",
            "Epoch: 4183 / 5000\n",
            "w1: [25.35727023] w2: [-23.25203516] bias: [16.31206038] loss: 30.54110821007674\n",
            "Epoch: 4184 / 5000\n",
            "w1: [25.35768255] w2: [-23.25248111] bias: [16.3119793] loss: 30.541070613530906\n",
            "Epoch: 4185 / 5000\n",
            "w1: [25.35809459] w2: [-23.25292658] bias: [16.31189822] loss: 30.54103308408243\n",
            "Epoch: 4186 / 5000\n",
            "w1: [25.35850633] w2: [-23.25337158] bias: [16.31181714] loss: 30.540995621604647\n",
            "Epoch: 4187 / 5000\n",
            "w1: [25.35891779] w2: [-23.25381609] bias: [16.31173608] loss: 30.540958225971142\n",
            "Epoch: 4188 / 5000\n",
            "w1: [25.35932895] w2: [-23.25426012] bias: [16.31165502] loss: 30.540920897055724\n",
            "Epoch: 4189 / 5000\n",
            "w1: [25.35973983] w2: [-23.25470367] bias: [16.31157396] loss: 30.540883634732474\n",
            "Epoch: 4190 / 5000\n",
            "w1: [25.36015041] w2: [-23.25514674] bias: [16.31149291] loss: 30.540846438875672\n",
            "Epoch: 4191 / 5000\n",
            "w1: [25.36056071] w2: [-23.25558933] bias: [16.31141187] loss: 30.540809309359876\n",
            "Epoch: 4192 / 5000\n",
            "w1: [25.36097072] w2: [-23.25603144] bias: [16.31133084] loss: 30.54077224605986\n",
            "Epoch: 4193 / 5000\n",
            "w1: [25.36138043] w2: [-23.25647308] bias: [16.31124981] loss: 30.540735248850634\n",
            "Epoch: 4194 / 5000\n",
            "w1: [25.36178986] w2: [-23.25691424] bias: [16.31116879] loss: 30.540698317607465\n",
            "Epoch: 4195 / 5000\n",
            "w1: [25.36219901] w2: [-23.25735492] bias: [16.31108777] loss: 30.54066145220584\n",
            "Epoch: 4196 / 5000\n",
            "w1: [25.36260786] w2: [-23.25779512] bias: [16.31100676] loss: 30.540624652521487\n",
            "Epoch: 4197 / 5000\n",
            "w1: [25.36301643] w2: [-23.25823485] bias: [16.31092576] loss: 30.540587918430372\n",
            "Epoch: 4198 / 5000\n",
            "w1: [25.3634247] w2: [-23.25867411] bias: [16.31084476] loss: 30.540551249808704\n",
            "Epoch: 4199 / 5000\n",
            "w1: [25.3638327] w2: [-23.25911289] bias: [16.31076377] loss: 30.54051464653292\n",
            "Epoch: 4200 / 5000\n",
            "w1: [25.3642404] w2: [-23.25955119] bias: [16.31068279] loss: 30.540478108479686\n",
            "Epoch: 4201 / 5000\n",
            "w1: [25.36464782] w2: [-23.25998902] bias: [16.31060181] loss: 30.54044163552592\n",
            "Epoch: 4202 / 5000\n",
            "w1: [25.36505495] w2: [-23.26042638] bias: [16.31052084] loss: 30.540405227548753\n",
            "Epoch: 4203 / 5000\n",
            "w1: [25.3654618] w2: [-23.26086326] bias: [16.31043988] loss: 30.540368884425572\n",
            "Epoch: 4204 / 5000\n",
            "w1: [25.36586836] w2: [-23.26129967] bias: [16.31035892] loss: 30.54033260603398\n",
            "Epoch: 4205 / 5000\n",
            "w1: [25.36627463] w2: [-23.26173561] bias: [16.31027797] loss: 30.540296392251822\n",
            "Epoch: 4206 / 5000\n",
            "w1: [25.36668062] w2: [-23.26217107] bias: [16.31019702] loss: 30.540260242957167\n",
            "Epoch: 4207 / 5000\n",
            "w1: [25.36708633] w2: [-23.26260607] bias: [16.31011608] loss: 30.540224158028334\n",
            "Epoch: 4208 / 5000\n",
            "w1: [25.36749175] w2: [-23.26304059] bias: [16.31003515] loss: 30.540188137343854\n",
            "Epoch: 4209 / 5000\n",
            "w1: [25.36789688] w2: [-23.26347464] bias: [16.30995423] loss: 30.540152180782496\n",
            "Epoch: 4210 / 5000\n",
            "w1: [25.36830173] w2: [-23.26390823] bias: [16.30987331] loss: 30.540116288223263\n",
            "Epoch: 4211 / 5000\n",
            "w1: [25.3687063] w2: [-23.26434134] bias: [16.3097924] loss: 30.54008045954538\n",
            "Epoch: 4212 / 5000\n",
            "w1: [25.36911058] w2: [-23.26477398] bias: [16.30971149] loss: 30.540044694628318\n",
            "Epoch: 4213 / 5000\n",
            "w1: [25.36951458] w2: [-23.26520616] bias: [16.30963059] loss: 30.540008993351755\n",
            "Epoch: 4214 / 5000\n",
            "w1: [25.3699183] w2: [-23.26563787] bias: [16.3095497] loss: 30.539973355595627\n",
            "Epoch: 4215 / 5000\n",
            "w1: [25.37032173] w2: [-23.26606911] bias: [16.30946881] loss: 30.539937781240067\n",
            "Epoch: 4216 / 5000\n",
            "w1: [25.37072488] w2: [-23.26649988] bias: [16.30938793] loss: 30.539902270165452\n",
            "Epoch: 4217 / 5000\n",
            "w1: [25.37112775] w2: [-23.26693018] bias: [16.30930706] loss: 30.53986682225239\n",
            "Epoch: 4218 / 5000\n",
            "w1: [25.37153034] w2: [-23.26736002] bias: [16.30922619] loss: 30.53983143738171\n",
            "Epoch: 4219 / 5000\n",
            "w1: [25.37193264] w2: [-23.2677894] bias: [16.30914533] loss: 30.53979611543447\n",
            "Epoch: 4220 / 5000\n",
            "w1: [25.37233467] w2: [-23.2682183] bias: [16.30906448] loss: 30.539760856291956\n",
            "Epoch: 4221 / 5000\n",
            "w1: [25.37273641] w2: [-23.26864675] bias: [16.30898363] loss: 30.53972565983567\n",
            "Epoch: 4222 / 5000\n",
            "w1: [25.37313787] w2: [-23.26907472] bias: [16.30890279] loss: 30.53969052594736\n",
            "Epoch: 4223 / 5000\n",
            "w1: [25.37353905] w2: [-23.26950224] bias: [16.30882196] loss: 30.539655454508974\n",
            "Epoch: 4224 / 5000\n",
            "w1: [25.37393995] w2: [-23.26992929] bias: [16.30874113] loss: 30.539620445402704\n",
            "Epoch: 4225 / 5000\n",
            "w1: [25.37434056] w2: [-23.27035587] bias: [16.30866031] loss: 30.539585498510956\n",
            "Epoch: 4226 / 5000\n",
            "w1: [25.3747409] w2: [-23.270782] bias: [16.3085795] loss: 30.539550613716365\n",
            "Epoch: 4227 / 5000\n",
            "w1: [25.37514096] w2: [-23.27120766] bias: [16.30849869] loss: 30.539515790901785\n",
            "Epoch: 4228 / 5000\n",
            "w1: [25.37554074] w2: [-23.27163286] bias: [16.30841789] loss: 30.5394810299503\n",
            "Epoch: 4229 / 5000\n",
            "w1: [25.37594024] w2: [-23.2720576] bias: [16.3083371] loss: 30.5394463307452\n",
            "Epoch: 4230 / 5000\n",
            "w1: [25.37633946] w2: [-23.27248188] bias: [16.30825631] loss: 30.539411693170024\n",
            "Epoch: 4231 / 5000\n",
            "w1: [25.3767384] w2: [-23.27290569] bias: [16.30817553] loss: 30.539377117108508\n",
            "Epoch: 4232 / 5000\n",
            "w1: [25.37713706] w2: [-23.27332905] bias: [16.30809475] loss: 30.53934260244463\n",
            "Epoch: 4233 / 5000\n",
            "w1: [25.37753544] w2: [-23.27375195] bias: [16.30801398] loss: 30.53930814906256\n",
            "Epoch: 4234 / 5000\n",
            "w1: [25.37793355] w2: [-23.27417438] bias: [16.30793322] loss: 30.53927375684672\n",
            "Epoch: 4235 / 5000\n",
            "w1: [25.37833138] w2: [-23.27459636] bias: [16.30785247] loss: 30.539239425681735\n",
            "Epoch: 4236 / 5000\n",
            "w1: [25.37872893] w2: [-23.27501788] bias: [16.30777172] loss: 30.53920515545245\n",
            "Epoch: 4237 / 5000\n",
            "w1: [25.3791262] w2: [-23.27543895] bias: [16.30769098] loss: 30.539170946043935\n",
            "Epoch: 4238 / 5000\n",
            "w1: [25.37952319] w2: [-23.27585955] bias: [16.30761024] loss: 30.539136797341474\n",
            "Epoch: 4239 / 5000\n",
            "w1: [25.37991991] w2: [-23.2762797] bias: [16.30752951] loss: 30.53910270923057\n",
            "Epoch: 4240 / 5000\n",
            "w1: [25.38031636] w2: [-23.27669939] bias: [16.30744879] loss: 30.539068681596945\n",
            "Epoch: 4241 / 5000\n",
            "w1: [25.38071252] w2: [-23.27711863] bias: [16.30736807] loss: 30.53903471432654\n",
            "Epoch: 4242 / 5000\n",
            "w1: [25.38110841] w2: [-23.27753741] bias: [16.30728737] loss: 30.539000807305506\n",
            "Epoch: 4243 / 5000\n",
            "w1: [25.38150402] w2: [-23.27795573] bias: [16.30720666] loss: 30.53896696042023\n",
            "Epoch: 4244 / 5000\n",
            "w1: [25.38189936] w2: [-23.2783736] bias: [16.30712597] loss: 30.538933173557282\n",
            "Epoch: 4245 / 5000\n",
            "w1: [25.38229442] w2: [-23.27879102] bias: [16.30704528] loss: 30.538899446603487\n",
            "Epoch: 4246 / 5000\n",
            "w1: [25.38268921] w2: [-23.27920798] bias: [16.3069646] loss: 30.538865779445846\n",
            "Epoch: 4247 / 5000\n",
            "w1: [25.38308372] w2: [-23.27962449] bias: [16.30688392] loss: 30.538832171971606\n",
            "Epoch: 4248 / 5000\n",
            "w1: [25.38347796] w2: [-23.28004054] bias: [16.30680325] loss: 30.538798624068214\n",
            "Epoch: 4249 / 5000\n",
            "w1: [25.38387193] w2: [-23.28045614] bias: [16.30672259] loss: 30.53876513562334\n",
            "Epoch: 4250 / 5000\n",
            "w1: [25.38426562] w2: [-23.28087129] bias: [16.30664193] loss: 30.538731706524846\n",
            "Epoch: 4251 / 5000\n",
            "w1: [25.38465903] w2: [-23.28128599] bias: [16.30656128] loss: 30.53869833666084\n",
            "Epoch: 4252 / 5000\n",
            "w1: [25.38505217] w2: [-23.28170024] bias: [16.30648064] loss: 30.538665025919624\n",
            "Epoch: 4253 / 5000\n",
            "w1: [25.38544504] w2: [-23.28211404] bias: [16.30640001] loss: 30.53863177418971\n",
            "Epoch: 4254 / 5000\n",
            "w1: [25.38583764] w2: [-23.28252738] bias: [16.30631938] loss: 30.538598581359825\n",
            "Epoch: 4255 / 5000\n",
            "w1: [25.38622996] w2: [-23.28294028] bias: [16.30623875] loss: 30.538565447318923\n",
            "Epoch: 4256 / 5000\n",
            "w1: [25.38662201] w2: [-23.28335272] bias: [16.30615814] loss: 30.538532371956137\n",
            "Epoch: 4257 / 5000\n",
            "w1: [25.38701379] w2: [-23.28376472] bias: [16.30607753] loss: 30.538499355160845\n",
            "Epoch: 4258 / 5000\n",
            "w1: [25.38740529] w2: [-23.28417627] bias: [16.30599693] loss: 30.538466396822614\n",
            "Epoch: 4259 / 5000\n",
            "w1: [25.38779653] w2: [-23.28458737] bias: [16.30591633] loss: 30.53843349683123\n",
            "Epoch: 4260 / 5000\n",
            "w1: [25.38818749] w2: [-23.28499803] bias: [16.30583574] loss: 30.53840065507669\n",
            "Epoch: 4261 / 5000\n",
            "w1: [25.38857818] w2: [-23.28540823] bias: [16.30575516] loss: 30.53836787144919\n",
            "Epoch: 4262 / 5000\n",
            "w1: [25.3889686] w2: [-23.28581799] bias: [16.30567458] loss: 30.53833514583915\n",
            "Epoch: 4263 / 5000\n",
            "w1: [25.38935874] w2: [-23.2862273] bias: [16.30559401] loss: 30.538302478137176\n",
            "Epoch: 4264 / 5000\n",
            "w1: [25.38974862] w2: [-23.28663617] bias: [16.30551345] loss: 30.538269868234114\n",
            "Epoch: 4265 / 5000\n",
            "w1: [25.39013823] w2: [-23.28704459] bias: [16.30543289] loss: 30.538237316020993\n",
            "Epoch: 4266 / 5000\n",
            "w1: [25.39052757] w2: [-23.28745256] bias: [16.30535234] loss: 30.538204821389055\n",
            "Epoch: 4267 / 5000\n",
            "w1: [25.39091663] w2: [-23.28786009] bias: [16.3052718] loss: 30.53817238422975\n",
            "Epoch: 4268 / 5000\n",
            "w1: [25.39130543] w2: [-23.28826718] bias: [16.30519126] loss: 30.538140004434737\n",
            "Epoch: 4269 / 5000\n",
            "w1: [25.39169396] w2: [-23.28867382] bias: [16.30511074] loss: 30.538107681895884\n",
            "Epoch: 4270 / 5000\n",
            "w1: [25.39208221] w2: [-23.28908002] bias: [16.30503021] loss: 30.538075416505258\n",
            "Epoch: 4271 / 5000\n",
            "w1: [25.3924702] w2: [-23.28948578] bias: [16.3049497] loss: 30.538043208155123\n",
            "Epoch: 4272 / 5000\n",
            "w1: [25.39285792] w2: [-23.28989109] bias: [16.30486919] loss: 30.538011056737965\n",
            "Epoch: 4273 / 5000\n",
            "w1: [25.39324538] w2: [-23.29029596] bias: [16.30478868] loss: 30.53797896214648\n",
            "Epoch: 4274 / 5000\n",
            "w1: [25.39363256] w2: [-23.29070039] bias: [16.30470819] loss: 30.537946924273545\n",
            "Epoch: 4275 / 5000\n",
            "w1: [25.39401948] w2: [-23.29110438] bias: [16.3046277] loss: 30.53791494301225\n",
            "Epoch: 4276 / 5000\n",
            "w1: [25.39440612] w2: [-23.29150793] bias: [16.30454722] loss: 30.537883018255894\n",
            "Epoch: 4277 / 5000\n",
            "w1: [25.3947925] w2: [-23.29191103] bias: [16.30446674] loss: 30.537851149897982\n",
            "Epoch: 4278 / 5000\n",
            "w1: [25.39517862] w2: [-23.2923137] bias: [16.30438627] loss: 30.53781933783221\n",
            "Epoch: 4279 / 5000\n",
            "w1: [25.39556446] w2: [-23.29271593] bias: [16.30430581] loss: 30.53778758195248\n",
            "Epoch: 4280 / 5000\n",
            "w1: [25.39595004] w2: [-23.29311772] bias: [16.30422535] loss: 30.537755882152894\n",
            "Epoch: 4281 / 5000\n",
            "w1: [25.39633536] w2: [-23.29351906] bias: [16.30414491] loss: 30.537724238327765\n",
            "Epoch: 4282 / 5000\n",
            "w1: [25.3967204] w2: [-23.29391998] bias: [16.30406446] loss: 30.53769265037161\n",
            "Epoch: 4283 / 5000\n",
            "w1: [25.39710518] w2: [-23.29432045] bias: [16.30398403] loss: 30.53766111817912\n",
            "Epoch: 4284 / 5000\n",
            "w1: [25.3974897] w2: [-23.29472048] bias: [16.3039036] loss: 30.537629641645214\n",
            "Epoch: 4285 / 5000\n",
            "w1: [25.39787395] w2: [-23.29512008] bias: [16.30382318] loss: 30.537598220665\n",
            "Epoch: 4286 / 5000\n",
            "w1: [25.39825793] w2: [-23.29551924] bias: [16.30374276] loss: 30.537566855133786\n",
            "Epoch: 4287 / 5000\n",
            "w1: [25.39864165] w2: [-23.29591797] bias: [16.30366235] loss: 30.537535544947083\n",
            "Epoch: 4288 / 5000\n",
            "w1: [25.3990251] w2: [-23.29631626] bias: [16.30358195] loss: 30.537504290000594\n",
            "Epoch: 4289 / 5000\n",
            "w1: [25.39940829] w2: [-23.29671411] bias: [16.30350156] loss: 30.53747309019023\n",
            "Epoch: 4290 / 5000\n",
            "w1: [25.39979121] w2: [-23.29711153] bias: [16.30342117] loss: 30.537441945412088\n",
            "Epoch: 4291 / 5000\n",
            "w1: [25.40017387] w2: [-23.29750852] bias: [16.30334079] loss: 30.53741085556247\n",
            "Epoch: 4292 / 5000\n",
            "w1: [25.40055627] w2: [-23.29790507] bias: [16.30326041] loss: 30.537379820537883\n",
            "Epoch: 4293 / 5000\n",
            "w1: [25.4009384] w2: [-23.29830119] bias: [16.30318004] loss: 30.537348840235005\n",
            "Epoch: 4294 / 5000\n",
            "w1: [25.40132027] w2: [-23.29869687] bias: [16.30309968] loss: 30.537317914550748\n",
            "Epoch: 4295 / 5000\n",
            "w1: [25.40170188] w2: [-23.29909212] bias: [16.30301933] loss: 30.537287043382186\n",
            "Epoch: 4296 / 5000\n",
            "w1: [25.40208322] w2: [-23.29948694] bias: [16.30293898] loss: 30.537256226626614\n",
            "Epoch: 4297 / 5000\n",
            "w1: [25.4024643] w2: [-23.29988133] bias: [16.30285864] loss: 30.5372254641815\n",
            "Epoch: 4298 / 5000\n",
            "w1: [25.40284512] w2: [-23.30027528] bias: [16.30277831] loss: 30.537194755944526\n",
            "Epoch: 4299 / 5000\n",
            "w1: [25.40322567] w2: [-23.30066881] bias: [16.30269798] loss: 30.537164101813563\n",
            "Epoch: 4300 / 5000\n",
            "w1: [25.40360597] w2: [-23.3010619] bias: [16.30261766] loss: 30.537133501686668\n",
            "Epoch: 4301 / 5000\n",
            "w1: [25.403986] w2: [-23.30145456] bias: [16.30253735] loss: 30.537102955462096\n",
            "Epoch: 4302 / 5000\n",
            "w1: [25.40436577] w2: [-23.3018468] bias: [16.30245704] loss: 30.537072463038314\n",
            "Epoch: 4303 / 5000\n",
            "w1: [25.40474528] w2: [-23.3022386] bias: [16.30237674] loss: 30.53704202431395\n",
            "Epoch: 4304 / 5000\n",
            "w1: [25.40512453] w2: [-23.30262998] bias: [16.30229644] loss: 30.537011639187853\n",
            "Epoch: 4305 / 5000\n",
            "w1: [25.40550351] w2: [-23.30302092] bias: [16.30221616] loss: 30.536981307559042\n",
            "Epoch: 4306 / 5000\n",
            "w1: [25.40588224] w2: [-23.30341144] bias: [16.30213588] loss: 30.536951029326747\n",
            "Epoch: 4307 / 5000\n",
            "w1: [25.4062607] w2: [-23.30380153] bias: [16.30205561] loss: 30.536920804390377\n",
            "Epoch: 4308 / 5000\n",
            "w1: [25.40663891] w2: [-23.30419119] bias: [16.30197534] loss: 30.536890632649545\n",
            "Epoch: 4309 / 5000\n",
            "w1: [25.40701685] w2: [-23.30458043] bias: [16.30189508] loss: 30.536860514004033\n",
            "Epoch: 4310 / 5000\n",
            "w1: [25.40739454] w2: [-23.30496924] bias: [16.30181483] loss: 30.53683044835384\n",
            "Epoch: 4311 / 5000\n",
            "w1: [25.40777197] w2: [-23.30535762] bias: [16.30173458] loss: 30.536800435599137\n",
            "Epoch: 4312 / 5000\n",
            "w1: [25.40814913] w2: [-23.30574558] bias: [16.30165434] loss: 30.536770475640296\n",
            "Epoch: 4313 / 5000\n",
            "w1: [25.40852604] w2: [-23.30613311] bias: [16.30157411] loss: 30.536740568377876\n",
            "Epoch: 4314 / 5000\n",
            "w1: [25.40890269] w2: [-23.30652022] bias: [16.30149389] loss: 30.536710713712605\n",
            "Epoch: 4315 / 5000\n",
            "w1: [25.40927908] w2: [-23.3069069] bias: [16.30141367] loss: 30.536680911545435\n",
            "Epoch: 4316 / 5000\n",
            "w1: [25.40965522] w2: [-23.30729316] bias: [16.30133345] loss: 30.53665116177749\n",
            "Epoch: 4317 / 5000\n",
            "w1: [25.41003109] w2: [-23.30767899] bias: [16.30125325] loss: 30.536621464310066\n",
            "Epoch: 4318 / 5000\n",
            "w1: [25.41040671] w2: [-23.3080644] bias: [16.30117305] loss: 30.536591819044673\n",
            "Epoch: 4319 / 5000\n",
            "w1: [25.41078207] w2: [-23.30844939] bias: [16.30109286] loss: 30.536562225883\n",
            "Epoch: 4320 / 5000\n",
            "w1: [25.41115717] w2: [-23.30883396] bias: [16.30101268] loss: 30.536532684726915\n",
            "Epoch: 4321 / 5000\n",
            "w1: [25.41153201] w2: [-23.3092181] bias: [16.3009325] loss: 30.536503195478485\n",
            "Epoch: 4322 / 5000\n",
            "w1: [25.4119066] w2: [-23.30960183] bias: [16.30085233] loss: 30.53647375803994\n",
            "Epoch: 4323 / 5000\n",
            "w1: [25.41228093] w2: [-23.30998513] bias: [16.30077216] loss: 30.53644437231373\n",
            "Epoch: 4324 / 5000\n",
            "w1: [25.41265501] w2: [-23.31036801] bias: [16.30069201] loss: 30.53641503820247\n",
            "Epoch: 4325 / 5000\n",
            "w1: [25.41302883] w2: [-23.31075047] bias: [16.30061186] loss: 30.536385755608954\n",
            "Epoch: 4326 / 5000\n",
            "w1: [25.41340239] w2: [-23.31113251] bias: [16.30053171] loss: 30.536356524436183\n",
            "Epoch: 4327 / 5000\n",
            "w1: [25.4137757] w2: [-23.31151413] bias: [16.30045158] loss: 30.536327344587324\n",
            "Epoch: 4328 / 5000\n",
            "w1: [25.41414875] w2: [-23.31189533] bias: [16.30037145] loss: 30.536298215965733\n",
            "Epoch: 4329 / 5000\n",
            "w1: [25.41452154] w2: [-23.31227611] bias: [16.30029133] loss: 30.53626913847495\n",
            "Epoch: 4330 / 5000\n",
            "w1: [25.41489408] w2: [-23.31265648] bias: [16.30021121] loss: 30.536240112018707\n",
            "Epoch: 4331 / 5000\n",
            "w1: [25.41526637] w2: [-23.31303642] bias: [16.3001311] loss: 30.536211136500903\n",
            "Epoch: 4332 / 5000\n",
            "w1: [25.4156384] w2: [-23.31341595] bias: [16.300051] loss: 30.536182211825636\n",
            "Epoch: 4333 / 5000\n",
            "w1: [25.41601018] w2: [-23.31379506] bias: [16.2999709] loss: 30.536153337897172\n",
            "Epoch: 4334 / 5000\n",
            "w1: [25.4163817] w2: [-23.31417376] bias: [16.29989082] loss: 30.53612451461997\n",
            "Epoch: 4335 / 5000\n",
            "w1: [25.41675297] w2: [-23.31455204] bias: [16.29981073] loss: 30.536095741898674\n",
            "Epoch: 4336 / 5000\n",
            "w1: [25.41712399] w2: [-23.3149299] bias: [16.29973066] loss: 30.536067019638093\n",
            "Epoch: 4337 / 5000\n",
            "w1: [25.41749475] w2: [-23.31530735] bias: [16.29965059] loss: 30.536038347743233\n",
            "Epoch: 4338 / 5000\n",
            "w1: [25.41786526] w2: [-23.31568438] bias: [16.29957053] loss: 30.53600972611926\n",
            "Epoch: 4339 / 5000\n",
            "w1: [25.41823551] w2: [-23.316061] bias: [16.29949048] loss: 30.535981154671557\n",
            "Epoch: 4340 / 5000\n",
            "w1: [25.41860552] w2: [-23.3164372] bias: [16.29941043] loss: 30.535952633305644\n",
            "Epoch: 4341 / 5000\n",
            "w1: [25.41897527] w2: [-23.31681299] bias: [16.29933039] loss: 30.53592416192726\n",
            "Epoch: 4342 / 5000\n",
            "w1: [25.41934477] w2: [-23.31718837] bias: [16.29925036] loss: 30.535895740442292\n",
            "Epoch: 4343 / 5000\n",
            "w1: [25.41971401] w2: [-23.31756333] bias: [16.29917033] loss: 30.535867368756826\n",
            "Epoch: 4344 / 5000\n",
            "w1: [25.42008301] w2: [-23.31793788] bias: [16.29909031] loss: 30.535839046777113\n",
            "Epoch: 4345 / 5000\n",
            "w1: [25.42045175] w2: [-23.31831202] bias: [16.2990103] loss: 30.535810774409594\n",
            "Epoch: 4346 / 5000\n",
            "w1: [25.42082024] w2: [-23.31868574] bias: [16.29893029] loss: 30.535782551560878\n",
            "Epoch: 4347 / 5000\n",
            "w1: [25.42118848] w2: [-23.31905905] bias: [16.29885029] loss: 30.53575437813777\n",
            "Epoch: 4348 / 5000\n",
            "w1: [25.42155647] w2: [-23.31943196] bias: [16.2987703] loss: 30.535726254047223\n",
            "Epoch: 4349 / 5000\n",
            "w1: [25.42192421] w2: [-23.31980445] bias: [16.29869032] loss: 30.535698179196395\n",
            "Epoch: 4350 / 5000\n",
            "w1: [25.4222917] w2: [-23.32017653] bias: [16.29861034] loss: 30.535670153492603\n",
            "Epoch: 4351 / 5000\n",
            "w1: [25.42265894] w2: [-23.3205482] bias: [16.29853037] loss: 30.535642176843336\n",
            "Epoch: 4352 / 5000\n",
            "w1: [25.42302592] w2: [-23.32091947] bias: [16.2984504] loss: 30.535614249156296\n",
            "Epoch: 4353 / 5000\n",
            "w1: [25.42339266] w2: [-23.32129032] bias: [16.29837045] loss: 30.535586370339306\n",
            "Epoch: 4354 / 5000\n",
            "w1: [25.42375915] w2: [-23.32166077] bias: [16.2982905] loss: 30.535558540300404\n",
            "Epoch: 4355 / 5000\n",
            "w1: [25.42412539] w2: [-23.3220308] bias: [16.29821055] loss: 30.535530758947786\n",
            "Epoch: 4356 / 5000\n",
            "w1: [25.42449138] w2: [-23.32240043] bias: [16.29813062] loss: 30.535503026189847\n",
            "Epoch: 4357 / 5000\n",
            "w1: [25.42485712] w2: [-23.32276965] bias: [16.29805069] loss: 30.535475341935108\n",
            "Epoch: 4358 / 5000\n",
            "w1: [25.42522261] w2: [-23.32313847] bias: [16.29797077] loss: 30.535447706092313\n",
            "Epoch: 4359 / 5000\n",
            "w1: [25.42558786] w2: [-23.32350687] bias: [16.29789085] loss: 30.535420118570357\n",
            "Epoch: 4360 / 5000\n",
            "w1: [25.42595285] w2: [-23.32387487] bias: [16.29781094] loss: 30.535392579278298\n",
            "Epoch: 4361 / 5000\n",
            "w1: [25.4263176] w2: [-23.32424247] bias: [16.29773104] loss: 30.535365088125396\n",
            "Epoch: 4362 / 5000\n",
            "w1: [25.4266821] w2: [-23.32460966] bias: [16.29765115] loss: 30.53533764502106\n",
            "Epoch: 4363 / 5000\n",
            "w1: [25.42704635] w2: [-23.32497644] bias: [16.29757126] loss: 30.535310249874875\n",
            "Epoch: 4364 / 5000\n",
            "w1: [25.42741035] w2: [-23.32534282] bias: [16.29749138] loss: 30.535282902596606\n",
            "Epoch: 4365 / 5000\n",
            "w1: [25.42777411] w2: [-23.3257088] bias: [16.2974115] loss: 30.535255603096193\n",
            "Epoch: 4366 / 5000\n",
            "w1: [25.42813762] w2: [-23.32607437] bias: [16.29733164] loss: 30.535228351283727\n",
            "Epoch: 4367 / 5000\n",
            "w1: [25.42850089] w2: [-23.32643954] bias: [16.29725178] loss: 30.53520114706949\n",
            "Epoch: 4368 / 5000\n",
            "w1: [25.4288639] w2: [-23.3268043] bias: [16.29717192] loss: 30.535173990363926\n",
            "Epoch: 4369 / 5000\n",
            "w1: [25.42922668] w2: [-23.32716866] bias: [16.29709208] loss: 30.535146881077658\n",
            "Epoch: 4370 / 5000\n",
            "w1: [25.4295892] w2: [-23.32753262] bias: [16.29701224] loss: 30.535119819121455\n",
            "Epoch: 4371 / 5000\n",
            "w1: [25.42995148] w2: [-23.32789618] bias: [16.29693241] loss: 30.535092804406297\n",
            "Epoch: 4372 / 5000\n",
            "w1: [25.43031351] w2: [-23.32825933] bias: [16.29685258] loss: 30.535065836843287\n",
            "Epoch: 4373 / 5000\n",
            "w1: [25.4306753] w2: [-23.32862209] bias: [16.29677276] loss: 30.53503891634373\n",
            "Epoch: 4374 / 5000\n",
            "w1: [25.43103685] w2: [-23.32898444] bias: [16.29669295] loss: 30.53501204281909\n",
            "Epoch: 4375 / 5000\n",
            "w1: [25.43139814] w2: [-23.32934639] bias: [16.29661315] loss: 30.534985216180992\n",
            "Epoch: 4376 / 5000\n",
            "w1: [25.4317592] w2: [-23.32970795] bias: [16.29653335] loss: 30.534958436341235\n",
            "Epoch: 4377 / 5000\n",
            "w1: [25.43212001] w2: [-23.3300691] bias: [16.29645356] loss: 30.534931703211797\n",
            "Epoch: 4378 / 5000\n",
            "w1: [25.43248057] w2: [-23.33042986] bias: [16.29637378] loss: 30.534905016704805\n",
            "Epoch: 4379 / 5000\n",
            "w1: [25.43284089] w2: [-23.33079021] bias: [16.296294] loss: 30.534878376732564\n",
            "Epoch: 4380 / 5000\n",
            "w1: [25.43320097] w2: [-23.33115017] bias: [16.29621423] loss: 30.534851783207536\n",
            "Epoch: 4381 / 5000\n",
            "w1: [25.4335608] w2: [-23.33150973] bias: [16.29613447] loss: 30.534825236042366\n",
            "Epoch: 4382 / 5000\n",
            "w1: [25.43392039] w2: [-23.33186889] bias: [16.29605472] loss: 30.53479873514985\n",
            "Epoch: 4383 / 5000\n",
            "w1: [25.43427974] w2: [-23.33222765] bias: [16.29597497] loss: 30.534772280442958\n",
            "Epoch: 4384 / 5000\n",
            "w1: [25.43463885] w2: [-23.33258602] bias: [16.29589523] loss: 30.53474587183482\n",
            "Epoch: 4385 / 5000\n",
            "w1: [25.43499771] w2: [-23.33294399] bias: [16.29581549] loss: 30.534719509238734\n",
            "Epoch: 4386 / 5000\n",
            "w1: [25.43535633] w2: [-23.33330156] bias: [16.29573577] loss: 30.53469319256817\n",
            "Epoch: 4387 / 5000\n",
            "w1: [25.4357147] w2: [-23.33365874] bias: [16.29565605] loss: 30.53466692173675\n",
            "Epoch: 4388 / 5000\n",
            "w1: [25.43607284] w2: [-23.33401552] bias: [16.29557633] loss: 30.53464069665827\n",
            "Epoch: 4389 / 5000\n",
            "w1: [25.43643073] w2: [-23.33437191] bias: [16.29549663] loss: 30.534614517246673\n",
            "Epoch: 4390 / 5000\n",
            "w1: [25.43678838] w2: [-23.3347279] bias: [16.29541693] loss: 30.534588383416107\n",
            "Epoch: 4391 / 5000\n",
            "w1: [25.43714579] w2: [-23.3350835] bias: [16.29533724] loss: 30.534562295080818\n",
            "Epoch: 4392 / 5000\n",
            "w1: [25.43750296] w2: [-23.3354387] bias: [16.29525755] loss: 30.53453625215528\n",
            "Epoch: 4393 / 5000\n",
            "w1: [25.43785989] w2: [-23.33579351] bias: [16.29517787] loss: 30.534510254554093\n",
            "Epoch: 4394 / 5000\n",
            "w1: [25.43821657] w2: [-23.33614793] bias: [16.2950982] loss: 30.53448430219203\n",
            "Epoch: 4395 / 5000\n",
            "w1: [25.43857302] w2: [-23.33650196] bias: [16.29501854] loss: 30.53445839498402\n",
            "Epoch: 4396 / 5000\n",
            "w1: [25.43892922] w2: [-23.33685559] bias: [16.29493888] loss: 30.534432532845166\n",
            "Epoch: 4397 / 5000\n",
            "w1: [25.43928519] w2: [-23.33720883] bias: [16.29485923] loss: 30.534406715690725\n",
            "Epoch: 4398 / 5000\n",
            "w1: [25.43964091] w2: [-23.33756168] bias: [16.29477959] loss: 30.53438094343611\n",
            "Epoch: 4399 / 5000\n",
            "w1: [25.4399964] w2: [-23.33791414] bias: [16.29469996] loss: 30.534355215996886\n",
            "Epoch: 4400 / 5000\n",
            "w1: [25.44035165] w2: [-23.33826621] bias: [16.29462033] loss: 30.534329533288826\n",
            "Epoch: 4401 / 5000\n",
            "w1: [25.44070665] w2: [-23.33861788] bias: [16.29454071] loss: 30.5343038952278\n",
            "Epoch: 4402 / 5000\n",
            "w1: [25.44106142] w2: [-23.33896917] bias: [16.29446109] loss: 30.53427830172988\n",
            "Epoch: 4403 / 5000\n",
            "w1: [25.44141595] w2: [-23.33932007] bias: [16.29438148] loss: 30.53425275271129\n",
            "Epoch: 4404 / 5000\n",
            "w1: [25.44177024] w2: [-23.33967058] bias: [16.29430188] loss: 30.534227248088406\n",
            "Epoch: 4405 / 5000\n",
            "w1: [25.44212429] w2: [-23.34002069] bias: [16.29422229] loss: 30.534201787777757\n",
            "Epoch: 4406 / 5000\n",
            "w1: [25.44247811] w2: [-23.34037042] bias: [16.2941427] loss: 30.53417637169605\n",
            "Epoch: 4407 / 5000\n",
            "w1: [25.44283168] w2: [-23.34071977] bias: [16.29406312] loss: 30.534150999760133\n",
            "Epoch: 4408 / 5000\n",
            "w1: [25.44318502] w2: [-23.34106872] bias: [16.29398355] loss: 30.534125671887033\n",
            "Epoch: 4409 / 5000\n",
            "w1: [25.44353812] w2: [-23.34141729] bias: [16.29390399] loss: 30.534100387993902\n",
            "Epoch: 4410 / 5000\n",
            "w1: [25.44389099] w2: [-23.34176547] bias: [16.29382443] loss: 30.534075147998085\n",
            "Epoch: 4411 / 5000\n",
            "w1: [25.44424361] w2: [-23.34211326] bias: [16.29374488] loss: 30.534049951817057\n",
            "Epoch: 4412 / 5000\n",
            "w1: [25.444596] w2: [-23.34246067] bias: [16.29366534] loss: 30.534024799368463\n",
            "Epoch: 4413 / 5000\n",
            "w1: [25.44494816] w2: [-23.34280769] bias: [16.2935858] loss: 30.533999690570113\n",
            "Epoch: 4414 / 5000\n",
            "w1: [25.44530007] w2: [-23.34315432] bias: [16.29350627] loss: 30.533974625339948\n",
            "Epoch: 4415 / 5000\n",
            "w1: [25.44565175] w2: [-23.34350057] bias: [16.29342675] loss: 30.533949603596092\n",
            "Epoch: 4416 / 5000\n",
            "w1: [25.4460032] w2: [-23.34384644] bias: [16.29334723] loss: 30.5339246252568\n",
            "Epoch: 4417 / 5000\n",
            "w1: [25.4463544] w2: [-23.34419192] bias: [16.29326772] loss: 30.53389969024051\n",
            "Epoch: 4418 / 5000\n",
            "w1: [25.44670538] w2: [-23.34453702] bias: [16.29318822] loss: 30.5338747984658\n",
            "Epoch: 4419 / 5000\n",
            "w1: [25.44705611] w2: [-23.34488173] bias: [16.29310873] loss: 30.533849949851387\n",
            "Epoch: 4420 / 5000\n",
            "w1: [25.44740661] w2: [-23.34522606] bias: [16.29302924] loss: 30.533825144316168\n",
            "Epoch: 4421 / 5000\n",
            "w1: [25.44775688] w2: [-23.34557001] bias: [16.29294976] loss: 30.533800381779194\n",
            "Epoch: 4422 / 5000\n",
            "w1: [25.44810691] w2: [-23.34591357] bias: [16.29287029] loss: 30.533775662159652\n",
            "Epoch: 4423 / 5000\n",
            "w1: [25.44845671] w2: [-23.34625676] bias: [16.29279082] loss: 30.53375098537689\n",
            "Epoch: 4424 / 5000\n",
            "w1: [25.44880627] w2: [-23.34659956] bias: [16.29271136] loss: 30.533726351350413\n",
            "Epoch: 4425 / 5000\n",
            "w1: [25.4491556] w2: [-23.34694197] bias: [16.29263191] loss: 30.53370175999989\n",
            "Epoch: 4426 / 5000\n",
            "w1: [25.4495047] w2: [-23.34728401] bias: [16.29255247] loss: 30.53367721124511\n",
            "Epoch: 4427 / 5000\n",
            "w1: [25.44985356] w2: [-23.34762567] bias: [16.29247303] loss: 30.53365270500605\n",
            "Epoch: 4428 / 5000\n",
            "w1: [25.45020219] w2: [-23.34796695] bias: [16.2923936] loss: 30.533628241202813\n",
            "Epoch: 4429 / 5000\n",
            "w1: [25.45055058] w2: [-23.34830784] bias: [16.29231418] loss: 30.533603819755673\n",
            "Epoch: 4430 / 5000\n",
            "w1: [25.45089874] w2: [-23.34864836] bias: [16.29223476] loss: 30.533579440585047\n",
            "Epoch: 4431 / 5000\n",
            "w1: [25.45124667] w2: [-23.3489885] bias: [16.29215535] loss: 30.5335551036115\n",
            "Epoch: 4432 / 5000\n",
            "w1: [25.45159436] w2: [-23.34932826] bias: [16.29207595] loss: 30.533530808755756\n",
            "Epoch: 4433 / 5000\n",
            "w1: [25.45194183] w2: [-23.34966764] bias: [16.29199656] loss: 30.533506555938686\n",
            "Epoch: 4434 / 5000\n",
            "w1: [25.45228906] w2: [-23.35000664] bias: [16.29191717] loss: 30.533482345081303\n",
            "Epoch: 4435 / 5000\n",
            "w1: [25.45263606] w2: [-23.35034527] bias: [16.29183779] loss: 30.53345817610479\n",
            "Epoch: 4436 / 5000\n",
            "w1: [25.45298282] w2: [-23.35068352] bias: [16.29175842] loss: 30.533434048930467\n",
            "Epoch: 4437 / 5000\n",
            "w1: [25.45332936] w2: [-23.35102139] bias: [16.29167905] loss: 30.533409963479794\n",
            "Epoch: 4438 / 5000\n",
            "w1: [25.45367566] w2: [-23.35135888] bias: [16.29159969] loss: 30.53338591967441\n",
            "Epoch: 4439 / 5000\n",
            "w1: [25.45402173] w2: [-23.351696] bias: [16.29152034] loss: 30.533361917436068\n",
            "Epoch: 4440 / 5000\n",
            "w1: [25.45436758] w2: [-23.35203274] bias: [16.291441] loss: 30.533337956686694\n",
            "Epoch: 4441 / 5000\n",
            "w1: [25.45471319] w2: [-23.35236911] bias: [16.29136166] loss: 30.533314037348354\n",
            "Epoch: 4442 / 5000\n",
            "w1: [25.45505857] w2: [-23.3527051] bias: [16.29128233] loss: 30.533290159343263\n",
            "Epoch: 4443 / 5000\n",
            "w1: [25.45540372] w2: [-23.35304071] bias: [16.29120301] loss: 30.533266322593786\n",
            "Epoch: 4444 / 5000\n",
            "w1: [25.45574863] w2: [-23.35337595] bias: [16.29112369] loss: 30.533242527022427\n",
            "Epoch: 4445 / 5000\n",
            "w1: [25.45609332] w2: [-23.35371082] bias: [16.29104438] loss: 30.533218772551855\n",
            "Epoch: 4446 / 5000\n",
            "w1: [25.45643778] w2: [-23.35404532] bias: [16.29096508] loss: 30.53319505910487\n",
            "Epoch: 4447 / 5000\n",
            "w1: [25.45678201] w2: [-23.35437944] bias: [16.29088579] loss: 30.533171386604415\n",
            "Epoch: 4448 / 5000\n",
            "w1: [25.45712601] w2: [-23.35471318] bias: [16.2908065] loss: 30.533147754973598\n",
            "Epoch: 4449 / 5000\n",
            "w1: [25.45746979] w2: [-23.35504656] bias: [16.29072722] loss: 30.533124164135675\n",
            "Epoch: 4450 / 5000\n",
            "w1: [25.45781333] w2: [-23.35537956] bias: [16.29064795] loss: 30.53310061401402\n",
            "Epoch: 4451 / 5000\n",
            "w1: [25.45815664] w2: [-23.35571219] bias: [16.29056868] loss: 30.533077104532175\n",
            "Epoch: 4452 / 5000\n",
            "w1: [25.45849973] w2: [-23.35604445] bias: [16.29048942] loss: 30.533053635613822\n",
            "Epoch: 4453 / 5000\n",
            "w1: [25.45884258] w2: [-23.35637634] bias: [16.29041017] loss: 30.533030207182787\n",
            "Epoch: 4454 / 5000\n",
            "w1: [25.45918521] w2: [-23.35670786] bias: [16.29033093] loss: 30.53300681916304\n",
            "Epoch: 4455 / 5000\n",
            "w1: [25.45952761] w2: [-23.357039] bias: [16.29025169] loss: 30.532983471478712\n",
            "Epoch: 4456 / 5000\n",
            "w1: [25.45986978] w2: [-23.35736978] bias: [16.29017246] loss: 30.532960164054042\n",
            "Epoch: 4457 / 5000\n",
            "w1: [25.46021173] w2: [-23.35770019] bias: [16.29009324] loss: 30.53293689681345\n",
            "Epoch: 4458 / 5000\n",
            "w1: [25.46055345] w2: [-23.35803023] bias: [16.29001402] loss: 30.532913669681484\n",
            "Epoch: 4459 / 5000\n",
            "w1: [25.46089494] w2: [-23.35835989] bias: [16.28993481] loss: 30.532890482582832\n",
            "Epoch: 4460 / 5000\n",
            "w1: [25.4612362] w2: [-23.35868919] bias: [16.28985561] loss: 30.53286733544233\n",
            "Epoch: 4461 / 5000\n",
            "w1: [25.46157724] w2: [-23.35901813] bias: [16.28977642] loss: 30.532844228184956\n",
            "Epoch: 4462 / 5000\n",
            "w1: [25.46191805] w2: [-23.35934669] bias: [16.28969723] loss: 30.532821160735832\n",
            "Epoch: 4463 / 5000\n",
            "w1: [25.46225863] w2: [-23.35967489] bias: [16.28961805] loss: 30.53279813302023\n",
            "Epoch: 4464 / 5000\n",
            "w1: [25.46259899] w2: [-23.36000271] bias: [16.28953888] loss: 30.532775144963537\n",
            "Epoch: 4465 / 5000\n",
            "w1: [25.46293912] w2: [-23.36033018] bias: [16.28945972] loss: 30.532752196491316\n",
            "Epoch: 4466 / 5000\n",
            "w1: [25.46327903] w2: [-23.36065727] bias: [16.28938056] loss: 30.532729287529257\n",
            "Epoch: 4467 / 5000\n",
            "w1: [25.46361871] w2: [-23.360984] bias: [16.28930141] loss: 30.53270641800318\n",
            "Epoch: 4468 / 5000\n",
            "w1: [25.46395817] w2: [-23.36131036] bias: [16.28922226] loss: 30.532683587839056\n",
            "Epoch: 4469 / 5000\n",
            "w1: [25.4642974] w2: [-23.36163636] bias: [16.28914313] loss: 30.532660796963015\n",
            "Epoch: 4470 / 5000\n",
            "w1: [25.4646364] w2: [-23.361962] bias: [16.289064] loss: 30.532638045301294\n",
            "Epoch: 4471 / 5000\n",
            "w1: [25.46497518] w2: [-23.36228726] bias: [16.28898488] loss: 30.53261533278029\n",
            "Epoch: 4472 / 5000\n",
            "w1: [25.46531374] w2: [-23.36261217] bias: [16.28890576] loss: 30.532592659326536\n",
            "Epoch: 4473 / 5000\n",
            "w1: [25.46565207] w2: [-23.36293671] bias: [16.28882666] loss: 30.532570024866715\n",
            "Epoch: 4474 / 5000\n",
            "w1: [25.46599018] w2: [-23.36326088] bias: [16.28874756] loss: 30.532547429327618\n",
            "Epoch: 4475 / 5000\n",
            "w1: [25.46632806] w2: [-23.3635847] bias: [16.28866846] loss: 30.53252487263621\n",
            "Epoch: 4476 / 5000\n",
            "w1: [25.46666572] w2: [-23.36390815] bias: [16.28858938] loss: 30.532502354719586\n",
            "Epoch: 4477 / 5000\n",
            "w1: [25.46700316] w2: [-23.36423123] bias: [16.2885103] loss: 30.532479875504958\n",
            "Epoch: 4478 / 5000\n",
            "w1: [25.46734037] w2: [-23.36455396] bias: [16.28843123] loss: 30.532457434919717\n",
            "Epoch: 4479 / 5000\n",
            "w1: [25.46767737] w2: [-23.36487632] bias: [16.28835216] loss: 30.532435032891346\n",
            "Epoch: 4480 / 5000\n",
            "w1: [25.46801413] w2: [-23.36519832] bias: [16.28827311] loss: 30.532412669347504\n",
            "Epoch: 4481 / 5000\n",
            "w1: [25.46835068] w2: [-23.36551996] bias: [16.28819406] loss: 30.532390344215965\n",
            "Epoch: 4482 / 5000\n",
            "w1: [25.468687] w2: [-23.36584124] bias: [16.28811502] loss: 30.53236805742465\n",
            "Epoch: 4483 / 5000\n",
            "w1: [25.4690231] w2: [-23.36616216] bias: [16.28803598] loss: 30.532345808901614\n",
            "Epoch: 4484 / 5000\n",
            "w1: [25.46935898] w2: [-23.36648272] bias: [16.28795695] loss: 30.532323598575047\n",
            "Epoch: 4485 / 5000\n",
            "w1: [25.46969463] w2: [-23.36680292] bias: [16.28787793] loss: 30.532301426373287\n",
            "Epoch: 4486 / 5000\n",
            "w1: [25.47003007] w2: [-23.36712276] bias: [16.28779892] loss: 30.53227929222479\n",
            "Epoch: 4487 / 5000\n",
            "w1: [25.47036528] w2: [-23.36744224] bias: [16.28771992] loss: 30.532257196058154\n",
            "Epoch: 4488 / 5000\n",
            "w1: [25.47070027] w2: [-23.36776136] bias: [16.28764092] loss: 30.532235137802118\n",
            "Epoch: 4489 / 5000\n",
            "w1: [25.47103504] w2: [-23.36808012] bias: [16.28756193] loss: 30.532213117385563\n",
            "Epoch: 4490 / 5000\n",
            "w1: [25.47136959] w2: [-23.36839853] bias: [16.28748294] loss: 30.532191134737495\n",
            "Epoch: 4491 / 5000\n",
            "w1: [25.47170392] w2: [-23.36871658] bias: [16.28740396] loss: 30.532169189787048\n",
            "Epoch: 4492 / 5000\n",
            "w1: [25.47203803] w2: [-23.36903427] bias: [16.287325] loss: 30.532147282463505\n",
            "Epoch: 4493 / 5000\n",
            "w1: [25.47237192] w2: [-23.3693516] bias: [16.28724603] loss: 30.53212541269628\n",
            "Epoch: 4494 / 5000\n",
            "w1: [25.47270558] w2: [-23.36966858] bias: [16.28716708] loss: 30.53210358041492\n",
            "Epoch: 4495 / 5000\n",
            "w1: [25.47303903] w2: [-23.3699852] bias: [16.28708813] loss: 30.5320817855491\n",
            "Epoch: 4496 / 5000\n",
            "w1: [25.47337226] w2: [-23.37030146] bias: [16.28700919] loss: 30.53206002802864\n",
            "Epoch: 4497 / 5000\n",
            "w1: [25.47370527] w2: [-23.37061737] bias: [16.28693026] loss: 30.532038307783477\n",
            "Epoch: 4498 / 5000\n",
            "w1: [25.47403806] w2: [-23.37093292] bias: [16.28685133] loss: 30.532016624743708\n",
            "Epoch: 4499 / 5000\n",
            "w1: [25.47437063] w2: [-23.37124812] bias: [16.28677241] loss: 30.53199497883953\n",
            "Epoch: 4500 / 5000\n",
            "w1: [25.47470298] w2: [-23.37156297] bias: [16.2866935] loss: 30.5319733700013\n",
            "Epoch: 4501 / 5000\n",
            "w1: [25.47503511] w2: [-23.37187746] bias: [16.2866146] loss: 30.53195179815949\n",
            "Epoch: 4502 / 5000\n",
            "w1: [25.47536702] w2: [-23.37219159] bias: [16.2865357] loss: 30.53193026324472\n",
            "Epoch: 4503 / 5000\n",
            "w1: [25.47569872] w2: [-23.37250537] bias: [16.28645681] loss: 30.531908765187723\n",
            "Epoch: 4504 / 5000\n",
            "w1: [25.4760302] w2: [-23.3728188] bias: [16.28637793] loss: 30.531887303919376\n",
            "Epoch: 4505 / 5000\n",
            "w1: [25.47636145] w2: [-23.37313188] bias: [16.28629905] loss: 30.53186587937069\n",
            "Epoch: 4506 / 5000\n",
            "w1: [25.4766925] w2: [-23.3734446] bias: [16.28622019] loss: 30.531844491472803\n",
            "Epoch: 4507 / 5000\n",
            "w1: [25.47702332] w2: [-23.37375697] bias: [16.28614133] loss: 30.531823140156977\n",
            "Epoch: 4508 / 5000\n",
            "w1: [25.47735393] w2: [-23.37406899] bias: [16.28606247] loss: 30.531801825354613\n",
            "Epoch: 4509 / 5000\n",
            "w1: [25.47768432] w2: [-23.37438066] bias: [16.28598363] loss: 30.53178054699724\n",
            "Epoch: 4510 / 5000\n",
            "w1: [25.47801449] w2: [-23.37469198] bias: [16.28590479] loss: 30.531759305016518\n",
            "Epoch: 4511 / 5000\n",
            "w1: [25.47834444] w2: [-23.37500295] bias: [16.28582596] loss: 30.531738099344246\n",
            "Epoch: 4512 / 5000\n",
            "w1: [25.47867418] w2: [-23.37531356] bias: [16.28574714] loss: 30.53171692991232\n",
            "Epoch: 4513 / 5000\n",
            "w1: [25.4790037] w2: [-23.37562383] bias: [16.28566832] loss: 30.531695796652812\n",
            "Epoch: 4514 / 5000\n",
            "w1: [25.47933301] w2: [-23.37593374] bias: [16.28558951] loss: 30.531674699497888\n",
            "Epoch: 4515 / 5000\n",
            "w1: [25.4796621] w2: [-23.37624331] bias: [16.28551071] loss: 30.53165363837985\n",
            "Epoch: 4516 / 5000\n",
            "w1: [25.47999097] w2: [-23.37655253] bias: [16.28543191] loss: 30.531632613231153\n",
            "Epoch: 4517 / 5000\n",
            "w1: [25.48031963] w2: [-23.3768614] bias: [16.28535313] loss: 30.531611623984347\n",
            "Epoch: 4518 / 5000\n",
            "w1: [25.48064807] w2: [-23.37716992] bias: [16.28527435] loss: 30.531590670572122\n",
            "Epoch: 4519 / 5000\n",
            "w1: [25.4809763] w2: [-23.37747809] bias: [16.28519557] loss: 30.53156975292731\n",
            "Epoch: 4520 / 5000\n",
            "w1: [25.48130432] w2: [-23.37778591] bias: [16.28511681] loss: 30.53154887098285\n",
            "Epoch: 4521 / 5000\n",
            "w1: [25.48163211] w2: [-23.37809339] bias: [16.28503805] loss: 30.531528024671815\n",
            "Epoch: 4522 / 5000\n",
            "w1: [25.4819597] w2: [-23.37840052] bias: [16.2849593] loss: 30.531507213927412\n",
            "Epoch: 4523 / 5000\n",
            "w1: [25.48228706] w2: [-23.3787073] bias: [16.28488056] loss: 30.531486438682975\n",
            "Epoch: 4524 / 5000\n",
            "w1: [25.48261422] w2: [-23.37901374] bias: [16.28480182] loss: 30.53146569887195\n",
            "Epoch: 4525 / 5000\n",
            "w1: [25.48294116] w2: [-23.37931983] bias: [16.28472309] loss: 30.53144499442794\n",
            "Epoch: 4526 / 5000\n",
            "w1: [25.48326789] w2: [-23.37962558] bias: [16.28464437] loss: 30.53142432528463\n",
            "Epoch: 4527 / 5000\n",
            "w1: [25.4835944] w2: [-23.37993098] bias: [16.28456566] loss: 30.531403691375868\n",
            "Epoch: 4528 / 5000\n",
            "w1: [25.4839207] w2: [-23.38023603] bias: [16.28448695] loss: 30.53138309263561\n",
            "Epoch: 4529 / 5000\n",
            "w1: [25.48424678] w2: [-23.38054074] bias: [16.28440825] loss: 30.531362528997956\n",
            "Epoch: 4530 / 5000\n",
            "w1: [25.48457265] w2: [-23.38084511] bias: [16.28432956] loss: 30.531342000397107\n",
            "Epoch: 4531 / 5000\n",
            "w1: [25.48489831] w2: [-23.38114913] bias: [16.28425088] loss: 30.531321506767398\n",
            "Epoch: 4532 / 5000\n",
            "w1: [25.48522376] w2: [-23.38145281] bias: [16.2841722] loss: 30.531301048043296\n",
            "Epoch: 4533 / 5000\n",
            "w1: [25.48554899] w2: [-23.38175614] bias: [16.28409353] loss: 30.531280624159383\n",
            "Epoch: 4534 / 5000\n",
            "w1: [25.48587402] w2: [-23.38205913] bias: [16.28401487] loss: 30.531260235050375\n",
            "Epoch: 4535 / 5000\n",
            "w1: [25.48619882] w2: [-23.38236178] bias: [16.28393621] loss: 30.53123988065111\n",
            "Epoch: 4536 / 5000\n",
            "w1: [25.48652342] w2: [-23.38266409] bias: [16.28385757] loss: 30.531219560896535\n",
            "Epoch: 4537 / 5000\n",
            "w1: [25.48684781] w2: [-23.38296605] bias: [16.28377893] loss: 30.531199275721743\n",
            "Epoch: 4538 / 5000\n",
            "w1: [25.48717198] w2: [-23.38326768] bias: [16.28370029] loss: 30.53117902506194\n",
            "Epoch: 4539 / 5000\n",
            "w1: [25.48749594] w2: [-23.38356896] bias: [16.28362167] loss: 30.531158808852457\n",
            "Epoch: 4540 / 5000\n",
            "w1: [25.48781969] w2: [-23.3838699] bias: [16.28354305] loss: 30.53113862702873\n",
            "Epoch: 4541 / 5000\n",
            "w1: [25.48814323] w2: [-23.3841705] bias: [16.28346444] loss: 30.531118479526356\n",
            "Epoch: 4542 / 5000\n",
            "w1: [25.48846656] w2: [-23.38447076] bias: [16.28338584] loss: 30.53109836628102\n",
            "Epoch: 4543 / 5000\n",
            "w1: [25.48878968] w2: [-23.38477068] bias: [16.28330724] loss: 30.531078287228546\n",
            "Epoch: 4544 / 5000\n",
            "w1: [25.48911259] w2: [-23.38507026] bias: [16.28322865] loss: 30.531058242304876\n",
            "Epoch: 4545 / 5000\n",
            "w1: [25.48943529] w2: [-23.3853695] bias: [16.28315007] loss: 30.531038231446068\n",
            "Epoch: 4546 / 5000\n",
            "w1: [25.48975778] w2: [-23.3856684] bias: [16.2830715] loss: 30.531018254588318\n",
            "Epoch: 4547 / 5000\n",
            "w1: [25.49008005] w2: [-23.38596696] bias: [16.28299293] loss: 30.53099831166792\n",
            "Epoch: 4548 / 5000\n",
            "w1: [25.49040212] w2: [-23.38626518] bias: [16.28291437] loss: 30.530978402621308\n",
            "Epoch: 4549 / 5000\n",
            "w1: [25.49072398] w2: [-23.38656307] bias: [16.28283582] loss: 30.530958527385028\n",
            "Epoch: 4550 / 5000\n",
            "w1: [25.49104563] w2: [-23.38686062] bias: [16.28275727] loss: 30.530938685895755\n",
            "Epoch: 4551 / 5000\n",
            "w1: [25.49136707] w2: [-23.38715783] bias: [16.28267874] loss: 30.530918878090276\n",
            "Epoch: 4552 / 5000\n",
            "w1: [25.4916883] w2: [-23.3874547] bias: [16.28260021] loss: 30.530899103905494\n",
            "Epoch: 4553 / 5000\n",
            "w1: [25.49200932] w2: [-23.38775124] bias: [16.28252169] loss: 30.530879363278448\n",
            "Epoch: 4554 / 5000\n",
            "w1: [25.49233013] w2: [-23.38804744] bias: [16.28244317] loss: 30.530859656146283\n",
            "Epoch: 4555 / 5000\n",
            "w1: [25.49265074] w2: [-23.38834331] bias: [16.28236466] loss: 30.530839982446263\n",
            "Epoch: 4556 / 5000\n",
            "w1: [25.49297113] w2: [-23.38863884] bias: [16.28228616] loss: 30.53082034211579\n",
            "Epoch: 4557 / 5000\n",
            "w1: [25.49329132] w2: [-23.38893403] bias: [16.28220767] loss: 30.530800735092356\n",
            "Epoch: 4558 / 5000\n",
            "w1: [25.4936113] w2: [-23.38922889] bias: [16.28212919] loss: 30.530781161313598\n",
            "Epoch: 4559 / 5000\n",
            "w1: [25.49393107] w2: [-23.38952341] bias: [16.28205071] loss: 30.530761620717254\n",
            "Epoch: 4560 / 5000\n",
            "w1: [25.49425064] w2: [-23.3898176] bias: [16.28197224] loss: 30.53074211324119\n",
            "Epoch: 4561 / 5000\n",
            "w1: [25.49456999] w2: [-23.39011146] bias: [16.28189378] loss: 30.530722638823384\n",
            "Epoch: 4562 / 5000\n",
            "w1: [25.49488914] w2: [-23.39040498] bias: [16.28181532] loss: 30.53070319740194\n",
            "Epoch: 4563 / 5000\n",
            "w1: [25.49520809] w2: [-23.39069817] bias: [16.28173687] loss: 30.530683788915063\n",
            "Epoch: 4564 / 5000\n",
            "w1: [25.49552682] w2: [-23.39099102] bias: [16.28165843] loss: 30.5306644133011\n",
            "Epoch: 4565 / 5000\n",
            "w1: [25.49584535] w2: [-23.39128355] bias: [16.28158] loss: 30.530645070498505\n",
            "Epoch: 4566 / 5000\n",
            "w1: [25.49616368] w2: [-23.39157574] bias: [16.28150157] loss: 30.530625760445833\n",
            "Epoch: 4567 / 5000\n",
            "w1: [25.4964818] w2: [-23.39186759] bias: [16.28142315] loss: 30.53060648308178\n",
            "Epoch: 4568 / 5000\n",
            "w1: [25.49679971] w2: [-23.39215912] bias: [16.28134474] loss: 30.530587238345134\n",
            "Epoch: 4569 / 5000\n",
            "w1: [25.49711741] w2: [-23.39245031] bias: [16.28126634] loss: 30.530568026174834\n",
            "Epoch: 4570 / 5000\n",
            "w1: [25.49743491] w2: [-23.39274118] bias: [16.28118794] loss: 30.5305488465099\n",
            "Epoch: 4571 / 5000\n",
            "w1: [25.49775221] w2: [-23.39303171] bias: [16.28110956] loss: 30.530529699289485\n",
            "Epoch: 4572 / 5000\n",
            "w1: [25.49806929] w2: [-23.39332191] bias: [16.28103117] loss: 30.530510584452845\n",
            "Epoch: 4573 / 5000\n",
            "w1: [25.49838618] w2: [-23.39361178] bias: [16.2809528] loss: 30.53049150193938\n",
            "Epoch: 4574 / 5000\n",
            "w1: [25.49870286] w2: [-23.39390132] bias: [16.28087443] loss: 30.53047245168857\n",
            "Epoch: 4575 / 5000\n",
            "w1: [25.49901933] w2: [-23.39419053] bias: [16.28079608] loss: 30.53045343364004\n",
            "Epoch: 4576 / 5000\n",
            "w1: [25.4993356] w2: [-23.39447942] bias: [16.28071772] loss: 30.530434447733512\n",
            "Epoch: 4577 / 5000\n",
            "w1: [25.49965166] w2: [-23.39476797] bias: [16.28063938] loss: 30.53041549390882\n",
            "Epoch: 4578 / 5000\n",
            "w1: [25.49996752] w2: [-23.3950562] bias: [16.28056104] loss: 30.530396572105925\n",
            "Epoch: 4579 / 5000\n",
            "w1: [25.50028318] w2: [-23.39534409] bias: [16.28048271] loss: 30.530377682264888\n",
            "Epoch: 4580 / 5000\n",
            "w1: [25.50059863] w2: [-23.39563166] bias: [16.28040439] loss: 30.530358824325912\n",
            "Epoch: 4581 / 5000\n",
            "w1: [25.50091388] w2: [-23.3959189] bias: [16.28032608] loss: 30.53033999822927\n",
            "Epoch: 4582 / 5000\n",
            "w1: [25.50122893] w2: [-23.39620582] bias: [16.28024777] loss: 30.530321203915396\n",
            "Epoch: 4583 / 5000\n",
            "w1: [25.50154377] w2: [-23.3964924] bias: [16.28016947] loss: 30.530302441324793\n",
            "Epoch: 4584 / 5000\n",
            "w1: [25.50185841] w2: [-23.39677866] bias: [16.28009118] loss: 30.530283710398113\n",
            "Epoch: 4585 / 5000\n",
            "w1: [25.50217284] w2: [-23.3970646] bias: [16.2800129] loss: 30.530265011076093\n",
            "Epoch: 4586 / 5000\n",
            "w1: [25.50248708] w2: [-23.39735021] bias: [16.27993462] loss: 30.530246343299602\n",
            "Epoch: 4587 / 5000\n",
            "w1: [25.50280111] w2: [-23.39763549] bias: [16.27985635] loss: 30.530227707009626\n",
            "Epoch: 4588 / 5000\n",
            "w1: [25.50311494] w2: [-23.39792045] bias: [16.27977809] loss: 30.530209102147232\n",
            "Epoch: 4589 / 5000\n",
            "w1: [25.50342856] w2: [-23.39820508] bias: [16.27969983] loss: 30.530190528653627\n",
            "Epoch: 4590 / 5000\n",
            "w1: [25.50374199] w2: [-23.39848938] bias: [16.27962159] loss: 30.53017198647012\n",
            "Epoch: 4591 / 5000\n",
            "w1: [25.50405521] w2: [-23.39877337] bias: [16.27954335] loss: 30.530153475538143\n",
            "Epoch: 4592 / 5000\n",
            "w1: [25.50436823] w2: [-23.39905702] bias: [16.27946512] loss: 30.530134995799216\n",
            "Epoch: 4593 / 5000\n",
            "w1: [25.50468105] w2: [-23.39934036] bias: [16.27938689] loss: 30.530116547194993\n",
            "Epoch: 4594 / 5000\n",
            "w1: [25.50499366] w2: [-23.39962337] bias: [16.27930867] loss: 30.53009812966722\n",
            "Epoch: 4595 / 5000\n",
            "w1: [25.50530608] w2: [-23.39990606] bias: [16.27923046] loss: 30.530079743157774\n",
            "Epoch: 4596 / 5000\n",
            "w1: [25.50561829] w2: [-23.40018842] bias: [16.27915226] loss: 30.53006138760863\n",
            "Epoch: 4597 / 5000\n",
            "w1: [25.50593031] w2: [-23.40047046] bias: [16.27907407] loss: 30.53004306296187\n",
            "Epoch: 4598 / 5000\n",
            "w1: [25.50624212] w2: [-23.40075218] bias: [16.27899588] loss: 30.530024769159702\n",
            "Epoch: 4599 / 5000\n",
            "w1: [25.50655374] w2: [-23.40103358] bias: [16.2789177] loss: 30.530006506144417\n",
            "Epoch: 4600 / 5000\n",
            "w1: [25.50686515] w2: [-23.40131466] bias: [16.27883953] loss: 30.529988273858443\n",
            "Epoch: 4601 / 5000\n",
            "w1: [25.50717636] w2: [-23.40159541] bias: [16.27876136] loss: 30.529970072244307\n",
            "Epoch: 4602 / 5000\n",
            "w1: [25.50748737] w2: [-23.40187585] bias: [16.27868321] loss: 30.52995190124464\n",
            "Epoch: 4603 / 5000\n",
            "w1: [25.50779819] w2: [-23.40215596] bias: [16.27860506] loss: 30.529933760802187\n",
            "Epoch: 4604 / 5000\n",
            "w1: [25.5081088] w2: [-23.40243575] bias: [16.27852691] loss: 30.529915650859806\n",
            "Epoch: 4605 / 5000\n",
            "w1: [25.50841921] w2: [-23.40271523] bias: [16.27844878] loss: 30.529897571360447\n",
            "Epoch: 4606 / 5000\n",
            "w1: [25.50872943] w2: [-23.40299438] bias: [16.27837065] loss: 30.5298795222472\n",
            "Epoch: 4607 / 5000\n",
            "w1: [25.50903945] w2: [-23.40327321] bias: [16.27829253] loss: 30.529861503463223\n",
            "Epoch: 4608 / 5000\n",
            "w1: [25.50934926] w2: [-23.40355173] bias: [16.27821442] loss: 30.52984351495182\n",
            "Epoch: 4609 / 5000\n",
            "w1: [25.50965888] w2: [-23.40382992] bias: [16.27813632] loss: 30.529825556656373\n",
            "Epoch: 4610 / 5000\n",
            "w1: [25.5099683] w2: [-23.4041078] bias: [16.27805822] loss: 30.529807628520388\n",
            "Epoch: 4611 / 5000\n",
            "w1: [25.51027752] w2: [-23.40438536] bias: [16.27798013] loss: 30.52978973048748\n",
            "Epoch: 4612 / 5000\n",
            "w1: [25.51058654] w2: [-23.4046626] bias: [16.27790205] loss: 30.529771862501356\n",
            "Epoch: 4613 / 5000\n",
            "w1: [25.51089537] w2: [-23.40493952] bias: [16.27782397] loss: 30.52975402450585\n",
            "Epoch: 4614 / 5000\n",
            "w1: [25.511204] w2: [-23.40521613] bias: [16.2777459] loss: 30.52973621644488\n",
            "Epoch: 4615 / 5000\n",
            "w1: [25.51151243] w2: [-23.40549242] bias: [16.27766784] loss: 30.529718438262496\n",
            "Epoch: 4616 / 5000\n",
            "w1: [25.51182066] w2: [-23.40576839] bias: [16.27758979] loss: 30.529700689902832\n",
            "Epoch: 4617 / 5000\n",
            "w1: [25.51212869] w2: [-23.40604405] bias: [16.27751175] loss: 30.529682971310145\n",
            "Epoch: 4618 / 5000\n",
            "w1: [25.51243653] w2: [-23.40631939] bias: [16.27743371] loss: 30.529665282428784\n",
            "Epoch: 4619 / 5000\n",
            "w1: [25.51274417] w2: [-23.40659441] bias: [16.27735568] loss: 30.529647623203218\n",
            "Epoch: 4620 / 5000\n",
            "w1: [25.51305162] w2: [-23.40686912] bias: [16.27727766] loss: 30.529629993578\n",
            "Epoch: 4621 / 5000\n",
            "w1: [25.51335887] w2: [-23.40714352] bias: [16.27719964] loss: 30.52961239349781\n",
            "Epoch: 4622 / 5000\n",
            "w1: [25.51366592] w2: [-23.4074176] bias: [16.27712164] loss: 30.52959482290743\n",
            "Epoch: 4623 / 5000\n",
            "w1: [25.51397277] w2: [-23.40769136] bias: [16.27704364] loss: 30.529577281751738\n",
            "Epoch: 4624 / 5000\n",
            "w1: [25.51427943] w2: [-23.40796481] bias: [16.27696565] loss: 30.52955976997572\n",
            "Epoch: 4625 / 5000\n",
            "w1: [25.51458589] w2: [-23.40823795] bias: [16.27688766] loss: 30.529542287524475\n",
            "Epoch: 4626 / 5000\n",
            "w1: [25.51489216] w2: [-23.40851077] bias: [16.27680968] loss: 30.52952483434318\n",
            "Epoch: 4627 / 5000\n",
            "w1: [25.51519823] w2: [-23.40878328] bias: [16.27673171] loss: 30.529507410377164\n",
            "Epoch: 4628 / 5000\n",
            "w1: [25.51550411] w2: [-23.40905548] bias: [16.27665375] loss: 30.529490015571806\n",
            "Epoch: 4629 / 5000\n",
            "w1: [25.51580979] w2: [-23.40932736] bias: [16.2765758] loss: 30.529472649872623\n",
            "Epoch: 4630 / 5000\n",
            "w1: [25.51611528] w2: [-23.40959894] bias: [16.27649785] loss: 30.52945531322523\n",
            "Epoch: 4631 / 5000\n",
            "w1: [25.51642057] w2: [-23.4098702] bias: [16.27641991] loss: 30.529438005575333\n",
            "Epoch: 4632 / 5000\n",
            "w1: [25.51672567] w2: [-23.41014114] bias: [16.27634198] loss: 30.52942072686876\n",
            "Epoch: 4633 / 5000\n",
            "w1: [25.51703057] w2: [-23.41041178] bias: [16.27626406] loss: 30.52940347705142\n",
            "Epoch: 4634 / 5000\n",
            "w1: [25.51733528] w2: [-23.41068211] bias: [16.27618614] loss: 30.529386256069355\n",
            "Epoch: 4635 / 5000\n",
            "w1: [25.51763979] w2: [-23.41095212] bias: [16.27610823] loss: 30.52936906386868\n",
            "Epoch: 4636 / 5000\n",
            "w1: [25.51794411] w2: [-23.41122182] bias: [16.27603033] loss: 30.529351900395607\n",
            "Epoch: 4637 / 5000\n",
            "w1: [25.51824824] w2: [-23.41149122] bias: [16.27595244] loss: 30.5293347655965\n",
            "Epoch: 4638 / 5000\n",
            "w1: [25.51855217] w2: [-23.4117603] bias: [16.27587455] loss: 30.52931765941777\n",
            "Epoch: 4639 / 5000\n",
            "w1: [25.51885591] w2: [-23.41202908] bias: [16.27579667] loss: 30.52930058180596\n",
            "Epoch: 4640 / 5000\n",
            "w1: [25.51915945] w2: [-23.41229754] bias: [16.2757188] loss: 30.529283532707698\n",
            "Epoch: 4641 / 5000\n",
            "w1: [25.51946281] w2: [-23.4125657] bias: [16.27564094] loss: 30.529266512069732\n",
            "Epoch: 4642 / 5000\n",
            "w1: [25.51976596] w2: [-23.41283355] bias: [16.27556308] loss: 30.529249519838896\n",
            "Epoch: 4643 / 5000\n",
            "w1: [25.52006893] w2: [-23.41310109] bias: [16.27548523] loss: 30.529232555962132\n",
            "Epoch: 4644 / 5000\n",
            "w1: [25.52037171] w2: [-23.41336832] bias: [16.27540739] loss: 30.529215620386477\n",
            "Epoch: 4645 / 5000\n",
            "w1: [25.52067429] w2: [-23.41363524] bias: [16.27532956] loss: 30.52919871305907\n",
            "Epoch: 4646 / 5000\n",
            "w1: [25.52097668] w2: [-23.41390186] bias: [16.27525173] loss: 30.529181833927165\n",
            "Epoch: 4647 / 5000\n",
            "w1: [25.52127887] w2: [-23.41416816] bias: [16.27517391] loss: 30.529164982938088\n",
            "Epoch: 4648 / 5000\n",
            "w1: [25.52158088] w2: [-23.41443417] bias: [16.2750961] loss: 30.5291481600393\n",
            "Epoch: 4649 / 5000\n",
            "w1: [25.52188269] w2: [-23.41469986] bias: [16.2750183] loss: 30.529131365178333\n",
            "Epoch: 4650 / 5000\n",
            "w1: [25.52218431] w2: [-23.41496525] bias: [16.2749405] loss: 30.529114598302826\n",
            "Epoch: 4651 / 5000\n",
            "w1: [25.52248575] w2: [-23.41523033] bias: [16.27486271] loss: 30.52909785936052\n",
            "Epoch: 4652 / 5000\n",
            "w1: [25.52278698] w2: [-23.41549511] bias: [16.27478493] loss: 30.529081148299262\n",
            "Epoch: 4653 / 5000\n",
            "w1: [25.52308803] w2: [-23.41575958] bias: [16.27470716] loss: 30.52906446506699\n",
            "Epoch: 4654 / 5000\n",
            "w1: [25.52338889] w2: [-23.41602375] bias: [16.2746294] loss: 30.52904780961174\n",
            "Epoch: 4655 / 5000\n",
            "w1: [25.52368956] w2: [-23.41628761] bias: [16.27455164] loss: 30.529031181881653\n",
            "Epoch: 4656 / 5000\n",
            "w1: [25.52399003] w2: [-23.41655117] bias: [16.27447389] loss: 30.529014581824963\n",
            "Epoch: 4657 / 5000\n",
            "w1: [25.52429032] w2: [-23.41681442] bias: [16.27439615] loss: 30.52899800939\n",
            "Epoch: 4658 / 5000\n",
            "w1: [25.52459041] w2: [-23.41707737] bias: [16.27431841] loss: 30.528981464525202\n",
            "Epoch: 4659 / 5000\n",
            "w1: [25.52489032] w2: [-23.41734001] bias: [16.27424068] loss: 30.528964947179098\n",
            "Epoch: 4660 / 5000\n",
            "w1: [25.52519004] w2: [-23.41760236] bias: [16.27416296] loss: 30.528948457300324\n",
            "Epoch: 4661 / 5000\n",
            "w1: [25.52548956] w2: [-23.4178644] bias: [16.27408525] loss: 30.528931994837592\n",
            "Epoch: 4662 / 5000\n",
            "w1: [25.5257889] w2: [-23.41812613] bias: [16.27400755] loss: 30.528915559739733\n",
            "Epoch: 4663 / 5000\n",
            "w1: [25.52608804] w2: [-23.41838757] bias: [16.27392985] loss: 30.528899151955663\n",
            "Epoch: 4664 / 5000\n",
            "w1: [25.526387] w2: [-23.4186487] bias: [16.27385216] loss: 30.528882771434414\n",
            "Epoch: 4665 / 5000\n",
            "w1: [25.52668577] w2: [-23.41890953] bias: [16.27377448] loss: 30.528866418125084\n",
            "Epoch: 4666 / 5000\n",
            "w1: [25.52698435] w2: [-23.41917006] bias: [16.27369681] loss: 30.528850091976896\n",
            "Epoch: 4667 / 5000\n",
            "w1: [25.52728274] w2: [-23.41943028] bias: [16.27361914] loss: 30.528833792939146\n",
            "Epoch: 4668 / 5000\n",
            "w1: [25.52758094] w2: [-23.41969021] bias: [16.27354148] loss: 30.528817520961248\n",
            "Epoch: 4669 / 5000\n",
            "w1: [25.52787896] w2: [-23.41994984] bias: [16.27346383] loss: 30.5288012759927\n",
            "Epoch: 4670 / 5000\n",
            "w1: [25.52817678] w2: [-23.42020916] bias: [16.27338619] loss: 30.528785057983093\n",
            "Epoch: 4671 / 5000\n",
            "w1: [25.52847442] w2: [-23.42046819] bias: [16.27330855] loss: 30.528768866882125\n",
            "Epoch: 4672 / 5000\n",
            "w1: [25.52877187] w2: [-23.42072691] bias: [16.27323092] loss: 30.528752702639583\n",
            "Epoch: 4673 / 5000\n",
            "w1: [25.52906913] w2: [-23.42098534] bias: [16.2731533] loss: 30.52873656520535\n",
            "Epoch: 4674 / 5000\n",
            "w1: [25.52936621] w2: [-23.42124346] bias: [16.27307569] loss: 30.5287204545294\n",
            "Epoch: 4675 / 5000\n",
            "w1: [25.52966309] w2: [-23.42150129] bias: [16.27299808] loss: 30.52870437056181\n",
            "Epoch: 4676 / 5000\n",
            "w1: [25.52995979] w2: [-23.42175882] bias: [16.27292048] loss: 30.528688313252754\n",
            "Epoch: 4677 / 5000\n",
            "w1: [25.53025631] w2: [-23.42201605] bias: [16.27284289] loss: 30.52867228255248\n",
            "Epoch: 4678 / 5000\n",
            "w1: [25.53055263] w2: [-23.42227298] bias: [16.27276531] loss: 30.52865627841136\n",
            "Epoch: 4679 / 5000\n",
            "w1: [25.53084877] w2: [-23.42252962] bias: [16.27268774] loss: 30.528640300779838\n",
            "Epoch: 4680 / 5000\n",
            "w1: [25.53114472] w2: [-23.42278596] bias: [16.27261017] loss: 30.52862434960846\n",
            "Epoch: 4681 / 5000\n",
            "w1: [25.53144049] w2: [-23.423042] bias: [16.27253261] loss: 30.528608424847878\n",
            "Epoch: 4682 / 5000\n",
            "w1: [25.53173607] w2: [-23.42329774] bias: [16.27245506] loss: 30.528592526448804\n",
            "Epoch: 4683 / 5000\n",
            "w1: [25.53203146] w2: [-23.42355319] bias: [16.27237751] loss: 30.528576654362077\n",
            "Epoch: 4684 / 5000\n",
            "w1: [25.53232667] w2: [-23.42380834] bias: [16.27229997] loss: 30.528560808538625\n",
            "Epoch: 4685 / 5000\n",
            "w1: [25.53262169] w2: [-23.42406319] bias: [16.27222245] loss: 30.528544988929447\n",
            "Epoch: 4686 / 5000\n",
            "w1: [25.53291653] w2: [-23.42431775] bias: [16.27214492] loss: 30.528529195485664\n",
            "Epoch: 4687 / 5000\n",
            "w1: [25.53321118] w2: [-23.42457201] bias: [16.27206741] loss: 30.528513428158465\n",
            "Epoch: 4688 / 5000\n",
            "w1: [25.53350565] w2: [-23.42482598] bias: [16.2719899] loss: 30.528497686899144\n",
            "Epoch: 4689 / 5000\n",
            "w1: [25.53379993] w2: [-23.42507966] bias: [16.2719124] loss: 30.52848197165909\n",
            "Epoch: 4690 / 5000\n",
            "w1: [25.53409402] w2: [-23.42533304] bias: [16.27183491] loss: 30.528466282389783\n",
            "Epoch: 4691 / 5000\n",
            "w1: [25.53438794] w2: [-23.42558612] bias: [16.27175743] loss: 30.52845061904279\n",
            "Epoch: 4692 / 5000\n",
            "w1: [25.53468166] w2: [-23.42583891] bias: [16.27167995] loss: 30.52843498156977\n",
            "Epoch: 4693 / 5000\n",
            "w1: [25.5349752] w2: [-23.42609141] bias: [16.27160249] loss: 30.528419369922474\n",
            "Epoch: 4694 / 5000\n",
            "w1: [25.53526856] w2: [-23.42634361] bias: [16.27152503] loss: 30.528403784052763\n",
            "Epoch: 4695 / 5000\n",
            "w1: [25.53556174] w2: [-23.42659552] bias: [16.27144757] loss: 30.528388223912554\n",
            "Epoch: 4696 / 5000\n",
            "w1: [25.53585473] w2: [-23.42684714] bias: [16.27137013] loss: 30.528372689453885\n",
            "Epoch: 4697 / 5000\n",
            "w1: [25.53614753] w2: [-23.42709846] bias: [16.27129269] loss: 30.52835718062887\n",
            "Epoch: 4698 / 5000\n",
            "w1: [25.53644015] w2: [-23.4273495] bias: [16.27121526] loss: 30.528341697389735\n",
            "Epoch: 4699 / 5000\n",
            "w1: [25.53673259] w2: [-23.42760024] bias: [16.27113784] loss: 30.528326239688763\n",
            "Epoch: 4700 / 5000\n",
            "w1: [25.53702485] w2: [-23.42785069] bias: [16.27106042] loss: 30.528310807478352\n",
            "Epoch: 4701 / 5000\n",
            "w1: [25.53731692] w2: [-23.42810085] bias: [16.27098302] loss: 30.528295400710988\n",
            "Epoch: 4702 / 5000\n",
            "w1: [25.53760881] w2: [-23.42835071] bias: [16.27090562] loss: 30.52828001933923\n",
            "Epoch: 4703 / 5000\n",
            "w1: [25.53790052] w2: [-23.42860029] bias: [16.27082822] loss: 30.52826466331576\n",
            "Epoch: 4704 / 5000\n",
            "w1: [25.53819204] w2: [-23.42884957] bias: [16.27075084] loss: 30.52824933259332\n",
            "Epoch: 4705 / 5000\n",
            "w1: [25.53848338] w2: [-23.42909857] bias: [16.27067346] loss: 30.528234027124743\n",
            "Epoch: 4706 / 5000\n",
            "w1: [25.53877454] w2: [-23.42934727] bias: [16.2705961] loss: 30.52821874686298\n",
            "Epoch: 4707 / 5000\n",
            "w1: [25.53906552] w2: [-23.42959569] bias: [16.27051874] loss: 30.528203491761037\n",
            "Epoch: 4708 / 5000\n",
            "w1: [25.53935632] w2: [-23.42984381] bias: [16.27044138] loss: 30.52818826177203\n",
            "Epoch: 4709 / 5000\n",
            "w1: [25.53964693] w2: [-23.43009165] bias: [16.27036404] loss: 30.528173056849166\n",
            "Epoch: 4710 / 5000\n",
            "w1: [25.53993736] w2: [-23.4303392] bias: [16.2702867] loss: 30.528157876945716\n",
            "Epoch: 4711 / 5000\n",
            "w1: [25.54022761] w2: [-23.43058646] bias: [16.27020937] loss: 30.528142722015076\n",
            "Epoch: 4712 / 5000\n",
            "w1: [25.54051768] w2: [-23.43083343] bias: [16.27013205] loss: 30.528127592010694\n",
            "Epoch: 4713 / 5000\n",
            "w1: [25.54080757] w2: [-23.43108011] bias: [16.27005473] loss: 30.52811248688614\n",
            "Epoch: 4714 / 5000\n",
            "w1: [25.54109727] w2: [-23.43132651] bias: [16.26997742] loss: 30.528097406595048\n",
            "Epoch: 4715 / 5000\n",
            "w1: [25.5413868] w2: [-23.43157262] bias: [16.26990012] loss: 30.52808235109115\n",
            "Epoch: 4716 / 5000\n",
            "w1: [25.54167614] w2: [-23.43181844] bias: [16.26982283] loss: 30.528067320328265\n",
            "Epoch: 4717 / 5000\n",
            "w1: [25.54196531] w2: [-23.43206397] bias: [16.26974555] loss: 30.5280523142603\n",
            "Epoch: 4718 / 5000\n",
            "w1: [25.54225429] w2: [-23.43230922] bias: [16.26966827] loss: 30.528037332841244\n",
            "Epoch: 4719 / 5000\n",
            "w1: [25.5425431] w2: [-23.43255418] bias: [16.269591] loss: 30.528022376025188\n",
            "Epoch: 4720 / 5000\n",
            "w1: [25.54283172] w2: [-23.43279885] bias: [16.26951374] loss: 30.528007443766285\n",
            "Epoch: 4721 / 5000\n",
            "w1: [25.54312016] w2: [-23.43304324] bias: [16.26943649] loss: 30.527992536018807\n",
            "Epoch: 4722 / 5000\n",
            "w1: [25.54340843] w2: [-23.43328735] bias: [16.26935924] loss: 30.527977652737082\n",
            "Epoch: 4723 / 5000\n",
            "w1: [25.54369651] w2: [-23.43353117] bias: [16.269282] loss: 30.52796279387555\n",
            "Epoch: 4724 / 5000\n",
            "w1: [25.54398442] w2: [-23.4337747] bias: [16.26920477] loss: 30.527947959388726\n",
            "Epoch: 4725 / 5000\n",
            "w1: [25.54427214] w2: [-23.43401795] bias: [16.26912755] loss: 30.527933149231195\n",
            "Epoch: 4726 / 5000\n",
            "w1: [25.54455969] w2: [-23.43426091] bias: [16.26905034] loss: 30.527918363357667\n",
            "Epoch: 4727 / 5000\n",
            "w1: [25.54484706] w2: [-23.43450359] bias: [16.26897313] loss: 30.52790360172291\n",
            "Epoch: 4728 / 5000\n",
            "w1: [25.54513424] w2: [-23.43474599] bias: [16.26889593] loss: 30.527888864281778\n",
            "Epoch: 4729 / 5000\n",
            "w1: [25.54542125] w2: [-23.4349881] bias: [16.26881874] loss: 30.52787415098922\n",
            "Epoch: 4730 / 5000\n",
            "w1: [25.54570809] w2: [-23.43522993] bias: [16.26874155] loss: 30.527859461800272\n",
            "Epoch: 4731 / 5000\n",
            "w1: [25.54599474] w2: [-23.43547148] bias: [16.26866438] loss: 30.52784479667005\n",
            "Epoch: 4732 / 5000\n",
            "w1: [25.54628121] w2: [-23.43571274] bias: [16.26858721] loss: 30.52783015555375\n",
            "Epoch: 4733 / 5000\n",
            "w1: [25.54656751] w2: [-23.43595373] bias: [16.26851005] loss: 30.527815538406664\n",
            "Epoch: 4734 / 5000\n",
            "w1: [25.54685363] w2: [-23.43619443] bias: [16.2684329] loss: 30.527800945184158\n",
            "Epoch: 4735 / 5000\n",
            "w1: [25.54713957] w2: [-23.43643484] bias: [16.26835575] loss: 30.5277863758417\n",
            "Epoch: 4736 / 5000\n",
            "w1: [25.54742534] w2: [-23.43667498] bias: [16.26827861] loss: 30.527771830334828\n",
            "Epoch: 4737 / 5000\n",
            "w1: [25.54771092] w2: [-23.43691483] bias: [16.26820148] loss: 30.52775730861917\n",
            "Epoch: 4738 / 5000\n",
            "w1: [25.54799633] w2: [-23.43715441] bias: [16.26812436] loss: 30.527742810650427\n",
            "Epoch: 4739 / 5000\n",
            "w1: [25.54828156] w2: [-23.4373937] bias: [16.26804724] loss: 30.527728336384403\n",
            "Epoch: 4740 / 5000\n",
            "w1: [25.54856662] w2: [-23.43763271] bias: [16.26797014] loss: 30.52771388577697\n",
            "Epoch: 4741 / 5000\n",
            "w1: [25.5488515] w2: [-23.43787145] bias: [16.26789304] loss: 30.5276994587841\n",
            "Epoch: 4742 / 5000\n",
            "w1: [25.5491362] w2: [-23.4381099] bias: [16.26781595] loss: 30.52768505536182\n",
            "Epoch: 4743 / 5000\n",
            "w1: [25.54942072] w2: [-23.43834807] bias: [16.26773886] loss: 30.527670675466283\n",
            "Epoch: 4744 / 5000\n",
            "w1: [25.54970507] w2: [-23.43858596] bias: [16.26766179] loss: 30.527656319053683\n",
            "Epoch: 4745 / 5000\n",
            "w1: [25.54998925] w2: [-23.43882358] bias: [16.26758472] loss: 30.527641986080326\n",
            "Epoch: 4746 / 5000\n",
            "w1: [25.55027324] w2: [-23.43906091] bias: [16.26750766] loss: 30.527627676502597\n",
            "Epoch: 4747 / 5000\n",
            "w1: [25.55055706] w2: [-23.43929797] bias: [16.26743061] loss: 30.52761339027694\n",
            "Epoch: 4748 / 5000\n",
            "w1: [25.55084071] w2: [-23.43953475] bias: [16.26735356] loss: 30.527599127359913\n",
            "Epoch: 4749 / 5000\n",
            "w1: [25.55112418] w2: [-23.43977125] bias: [16.26727652] loss: 30.52758488770814\n",
            "Epoch: 4750 / 5000\n",
            "w1: [25.55140747] w2: [-23.44000747] bias: [16.26719949] loss: 30.527570671278326\n",
            "Epoch: 4751 / 5000\n",
            "w1: [25.55169059] w2: [-23.44024342] bias: [16.26712247] loss: 30.527556478027268\n",
            "Epoch: 4752 / 5000\n",
            "w1: [25.55197354] w2: [-23.44047909] bias: [16.26704546] loss: 30.527542307911844\n",
            "Epoch: 4753 / 5000\n",
            "w1: [25.5522563] w2: [-23.44071448] bias: [16.26696845] loss: 30.527528160889\n",
            "Epoch: 4754 / 5000\n",
            "w1: [25.5525389] w2: [-23.44094959] bias: [16.26689145] loss: 30.527514036915775\n",
            "Epoch: 4755 / 5000\n",
            "w1: [25.55282132] w2: [-23.44118443] bias: [16.26681446] loss: 30.527499935949294\n",
            "Epoch: 4756 / 5000\n",
            "w1: [25.55310356] w2: [-23.44141899] bias: [16.26673748] loss: 30.527485857946754\n",
            "Epoch: 4757 / 5000\n",
            "w1: [25.55338563] w2: [-23.44165328] bias: [16.2666605] loss: 30.527471802865435\n",
            "Epoch: 4758 / 5000\n",
            "w1: [25.55366753] w2: [-23.44188729] bias: [16.26658353] loss: 30.5274577706627\n",
            "Epoch: 4759 / 5000\n",
            "w1: [25.55394925] w2: [-23.44212102] bias: [16.26650657] loss: 30.527443761296002\n",
            "Epoch: 4760 / 5000\n",
            "w1: [25.5542308] w2: [-23.44235448] bias: [16.26642962] loss: 30.52742977472285\n",
            "Epoch: 4761 / 5000\n",
            "w1: [25.55451218] w2: [-23.44258767] bias: [16.26635268] loss: 30.527415810900866\n",
            "Epoch: 4762 / 5000\n",
            "w1: [25.55479338] w2: [-23.44282058] bias: [16.26627574] loss: 30.52740186978772\n",
            "Epoch: 4763 / 5000\n",
            "w1: [25.55507441] w2: [-23.44305322] bias: [16.26619881] loss: 30.527387951341193\n",
            "Epoch: 4764 / 5000\n",
            "w1: [25.55535526] w2: [-23.44328558] bias: [16.26612189] loss: 30.52737405551912\n",
            "Epoch: 4765 / 5000\n",
            "w1: [25.55563595] w2: [-23.44351767] bias: [16.26604498] loss: 30.527360182279438\n",
            "Epoch: 4766 / 5000\n",
            "w1: [25.55591645] w2: [-23.44374948] bias: [16.26596807] loss: 30.52734633158014\n",
            "Epoch: 4767 / 5000\n",
            "w1: [25.55619679] w2: [-23.44398102] bias: [16.26589117] loss: 30.52733250337933\n",
            "Epoch: 4768 / 5000\n",
            "w1: [25.55647695] w2: [-23.44421229] bias: [16.26581428] loss: 30.527318697635156\n",
            "Epoch: 4769 / 5000\n",
            "w1: [25.55675695] w2: [-23.44444329] bias: [16.2657374] loss: 30.527304914305873\n",
            "Epoch: 4770 / 5000\n",
            "w1: [25.55703676] w2: [-23.44467401] bias: [16.26566053] loss: 30.527291153349804\n",
            "Epoch: 4771 / 5000\n",
            "w1: [25.55731641] w2: [-23.44490447] bias: [16.26558366] loss: 30.527277414725358\n",
            "Epoch: 4772 / 5000\n",
            "w1: [25.55759589] w2: [-23.44513465] bias: [16.2655068] loss: 30.527263698391003\n",
            "Epoch: 4773 / 5000\n",
            "w1: [25.55787519] w2: [-23.44536456] bias: [16.26542995] loss: 30.527250004305316\n",
            "Epoch: 4774 / 5000\n",
            "w1: [25.55815432] w2: [-23.44559419] bias: [16.2653531] loss: 30.52723633242693\n",
            "Epoch: 4775 / 5000\n",
            "w1: [25.55843328] w2: [-23.44582356] bias: [16.26527627] loss: 30.52722268271457\n",
            "Epoch: 4776 / 5000\n",
            "w1: [25.55871207] w2: [-23.44605265] bias: [16.26519944] loss: 30.52720905512702\n",
            "Epoch: 4777 / 5000\n",
            "w1: [25.55899068] w2: [-23.44628148] bias: [16.26512262] loss: 30.527195449623168\n",
            "Epoch: 4778 / 5000\n",
            "w1: [25.55926913] w2: [-23.44651003] bias: [16.26504581] loss: 30.52718186616196\n",
            "Epoch: 4779 / 5000\n",
            "w1: [25.55954741] w2: [-23.44673832] bias: [16.264969] loss: 30.527168304702435\n",
            "Epoch: 4780 / 5000\n",
            "w1: [25.55982551] w2: [-23.44696633] bias: [16.26489221] loss: 30.5271547652037\n",
            "Epoch: 4781 / 5000\n",
            "w1: [25.56010344] w2: [-23.44719408] bias: [16.26481542] loss: 30.52714124762494\n",
            "Epoch: 4782 / 5000\n",
            "w1: [25.56038121] w2: [-23.44742156] bias: [16.26473864] loss: 30.52712775192542\n",
            "Epoch: 4783 / 5000\n",
            "w1: [25.5606588] w2: [-23.44764876] bias: [16.26466186] loss: 30.52711427806448\n",
            "Epoch: 4784 / 5000\n",
            "w1: [25.56093622] w2: [-23.4478757] bias: [16.2645851] loss: 30.52710082600154\n",
            "Epoch: 4785 / 5000\n",
            "w1: [25.56121347] w2: [-23.44810237] bias: [16.26450834] loss: 30.5270873956961\n",
            "Epoch: 4786 / 5000\n",
            "w1: [25.56149056] w2: [-23.44832878] bias: [16.26443159] loss: 30.527073987107734\n",
            "Epoch: 4787 / 5000\n",
            "w1: [25.56176747] w2: [-23.44855491] bias: [16.26435485] loss: 30.527060600196076\n",
            "Epoch: 4788 / 5000\n",
            "w1: [25.56204421] w2: [-23.44878078] bias: [16.26427811] loss: 30.52704723492087\n",
            "Epoch: 4789 / 5000\n",
            "w1: [25.56232078] w2: [-23.44900638] bias: [16.26420138] loss: 30.527033891241924\n",
            "Epoch: 4790 / 5000\n",
            "w1: [25.56259719] w2: [-23.44923171] bias: [16.26412466] loss: 30.5270205691191\n",
            "Epoch: 4791 / 5000\n",
            "w1: [25.56287342] w2: [-23.44945678] bias: [16.26404795] loss: 30.527007268512353\n",
            "Epoch: 4792 / 5000\n",
            "w1: [25.56314949] w2: [-23.44968157] bias: [16.26397125] loss: 30.52699398938173\n",
            "Epoch: 4793 / 5000\n",
            "w1: [25.56342539] w2: [-23.44990611] bias: [16.26389455] loss: 30.526980731687324\n",
            "Epoch: 4794 / 5000\n",
            "w1: [25.56370111] w2: [-23.45013037] bias: [16.26381787] loss: 30.526967495389332\n",
            "Epoch: 4795 / 5000\n",
            "w1: [25.56397667] w2: [-23.45035437] bias: [16.26374119] loss: 30.526954280448006\n",
            "Epoch: 4796 / 5000\n",
            "w1: [25.56425207] w2: [-23.45057811] bias: [16.26366451] loss: 30.526941086823673\n",
            "Epoch: 4797 / 5000\n",
            "w1: [25.56452729] w2: [-23.45080158] bias: [16.26358785] loss: 30.526927914476754\n",
            "Epoch: 4798 / 5000\n",
            "w1: [25.56480234] w2: [-23.45102479] bias: [16.26351119] loss: 30.526914763367724\n",
            "Epoch: 4799 / 5000\n",
            "w1: [25.56507723] w2: [-23.45124773] bias: [16.26343454] loss: 30.526901633457154\n",
            "Epoch: 4800 / 5000\n",
            "w1: [25.56535195] w2: [-23.4514704] bias: [16.2633579] loss: 30.52688852470567\n",
            "Epoch: 4801 / 5000\n",
            "w1: [25.5656265] w2: [-23.45169281] bias: [16.26328127] loss: 30.52687543707398\n",
            "Epoch: 4802 / 5000\n",
            "w1: [25.56590089] w2: [-23.45191496] bias: [16.26320464] loss: 30.52686237052288\n",
            "Epoch: 4803 / 5000\n",
            "w1: [25.5661751] w2: [-23.45213685] bias: [16.26312803] loss: 30.526849325013213\n",
            "Epoch: 4804 / 5000\n",
            "w1: [25.56644915] w2: [-23.45235847] bias: [16.26305142] loss: 30.52683630050592\n",
            "Epoch: 4805 / 5000\n",
            "w1: [25.56672303] w2: [-23.45257982] bias: [16.26297481] loss: 30.52682329696201\n",
            "Epoch: 4806 / 5000\n",
            "w1: [25.56699675] w2: [-23.45280092] bias: [16.26289822] loss: 30.52681031434256\n",
            "Epoch: 4807 / 5000\n",
            "w1: [25.5672703] w2: [-23.45302175] bias: [16.26282163] loss: 30.526797352608728\n",
            "Epoch: 4808 / 5000\n",
            "w1: [25.56754368] w2: [-23.45324232] bias: [16.26274505] loss: 30.52678441172174\n",
            "Epoch: 4809 / 5000\n",
            "w1: [25.56781689] w2: [-23.45346263] bias: [16.26266848] loss: 30.526771491642897\n",
            "Epoch: 4810 / 5000\n",
            "w1: [25.56808994] w2: [-23.45368267] bias: [16.26259192] loss: 30.526758592333582\n",
            "Epoch: 4811 / 5000\n",
            "w1: [25.56836283] w2: [-23.45390246] bias: [16.26251537] loss: 30.526745713755236\n",
            "Epoch: 4812 / 5000\n",
            "w1: [25.56863554] w2: [-23.45412198] bias: [16.26243882] loss: 30.526732855869383\n",
            "Epoch: 4813 / 5000\n",
            "w1: [25.56890809] w2: [-23.45434124] bias: [16.26236228] loss: 30.526720018637622\n",
            "Epoch: 4814 / 5000\n",
            "w1: [25.56918048] w2: [-23.45456024] bias: [16.26228575] loss: 30.52670720202162\n",
            "Epoch: 4815 / 5000\n",
            "w1: [25.5694527] w2: [-23.45477898] bias: [16.26220922] loss: 30.52669440598311\n",
            "Epoch: 4816 / 5000\n",
            "w1: [25.56972475] w2: [-23.45499746] bias: [16.26213271] loss: 30.52668163048392\n",
            "Epoch: 4817 / 5000\n",
            "w1: [25.56999664] w2: [-23.45521568] bias: [16.2620562] loss: 30.52666887548593\n",
            "Epoch: 4818 / 5000\n",
            "w1: [25.57026836] w2: [-23.45543364] bias: [16.2619797] loss: 30.526656140951104\n",
            "Epoch: 4819 / 5000\n",
            "w1: [25.57053992] w2: [-23.45565134] bias: [16.2619032] loss: 30.52664342684146\n",
            "Epoch: 4820 / 5000\n",
            "w1: [25.57081131] w2: [-23.45586878] bias: [16.26182672] loss: 30.526630733119106\n",
            "Epoch: 4821 / 5000\n",
            "w1: [25.57108254] w2: [-23.45608596] bias: [16.26175024] loss: 30.526618059746216\n",
            "Epoch: 4822 / 5000\n",
            "w1: [25.57135361] w2: [-23.45630288] bias: [16.26167377] loss: 30.52660540668505\n",
            "Epoch: 4823 / 5000\n",
            "w1: [25.57162451] w2: [-23.45651954] bias: [16.26159731] loss: 30.526592773897914\n",
            "Epoch: 4824 / 5000\n",
            "w1: [25.57189524] w2: [-23.45673595] bias: [16.26152086] loss: 30.52658016134719\n",
            "Epoch: 4825 / 5000\n",
            "w1: [25.57216581] w2: [-23.4569521] bias: [16.26144441] loss: 30.526567568995354\n",
            "Epoch: 4826 / 5000\n",
            "w1: [25.57243622] w2: [-23.45716799] bias: [16.26136798] loss: 30.52655499680494\n",
            "Epoch: 4827 / 5000\n",
            "w1: [25.57270646] w2: [-23.45738362] bias: [16.26129155] loss: 30.526542444738535\n",
            "Epoch: 4828 / 5000\n",
            "w1: [25.57297654] w2: [-23.45759899] bias: [16.26121512] loss: 30.526529912758832\n",
            "Epoch: 4829 / 5000\n",
            "w1: [25.57324646] w2: [-23.45781411] bias: [16.26113871] loss: 30.526517400828567\n",
            "Epoch: 4830 / 5000\n",
            "w1: [25.57351621] w2: [-23.45802897] bias: [16.2610623] loss: 30.526504908910557\n",
            "Epoch: 4831 / 5000\n",
            "w1: [25.5737858] w2: [-23.45824358] bias: [16.2609859] loss: 30.526492436967693\n",
            "Epoch: 4832 / 5000\n",
            "w1: [25.57405522] w2: [-23.45845792] bias: [16.26090951] loss: 30.526479984962922\n",
            "Epoch: 4833 / 5000\n",
            "w1: [25.57432449] w2: [-23.45867202] bias: [16.26083313] loss: 30.526467552859284\n",
            "Epoch: 4834 / 5000\n",
            "w1: [25.57459359] w2: [-23.45888585] bias: [16.26075676] loss: 30.526455140619866\n",
            "Epoch: 4835 / 5000\n",
            "w1: [25.57486252] w2: [-23.45909943] bias: [16.26068039] loss: 30.526442748207852\n",
            "Epoch: 4836 / 5000\n",
            "w1: [25.5751313] w2: [-23.45931276] bias: [16.26060403] loss: 30.52643037558646\n",
            "Epoch: 4837 / 5000\n",
            "w1: [25.57539991] w2: [-23.45952583] bias: [16.26052768] loss: 30.526418022719014\n",
            "Epoch: 4838 / 5000\n",
            "w1: [25.57566836] w2: [-23.45973864] bias: [16.26045133] loss: 30.52640568956888\n",
            "Epoch: 4839 / 5000\n",
            "w1: [25.57593665] w2: [-23.45995121] bias: [16.260375] loss: 30.526393376099517\n",
            "Epoch: 4840 / 5000\n",
            "w1: [25.57620477] w2: [-23.46016351] bias: [16.26029867] loss: 30.526381082274426\n",
            "Epoch: 4841 / 5000\n",
            "w1: [25.57647273] w2: [-23.46037556] bias: [16.26022235] loss: 30.526368808057207\n",
            "Epoch: 4842 / 5000\n",
            "w1: [25.57674054] w2: [-23.46058736] bias: [16.26014604] loss: 30.526356553411507\n",
            "Epoch: 4843 / 5000\n",
            "w1: [25.57700818] w2: [-23.46079891] bias: [16.26006973] loss: 30.52634431830105\n",
            "Epoch: 4844 / 5000\n",
            "w1: [25.57727565] w2: [-23.4610102] bias: [16.25999344] loss: 30.526332102689626\n",
            "Epoch: 4845 / 5000\n",
            "w1: [25.57754297] w2: [-23.46122124] bias: [16.25991715] loss: 30.526319906541097\n",
            "Epoch: 4846 / 5000\n",
            "w1: [25.57781013] w2: [-23.46143202] bias: [16.25984087] loss: 30.5263077298194\n",
            "Epoch: 4847 / 5000\n",
            "w1: [25.57807712] w2: [-23.46164256] bias: [16.25976459] loss: 30.526295572488525\n",
            "Epoch: 4848 / 5000\n",
            "w1: [25.57834396] w2: [-23.46185284] bias: [16.25968833] loss: 30.526283434512546\n",
            "Epoch: 4849 / 5000\n",
            "w1: [25.57861063] w2: [-23.46206287] bias: [16.25961207] loss: 30.5262713158556\n",
            "Epoch: 4850 / 5000\n",
            "w1: [25.57887714] w2: [-23.46227264] bias: [16.25953582] loss: 30.526259216481872\n",
            "Epoch: 4851 / 5000\n",
            "w1: [25.57914349] w2: [-23.46248217] bias: [16.25945958] loss: 30.526247136355654\n",
            "Epoch: 4852 / 5000\n",
            "w1: [25.57940969] w2: [-23.46269144] bias: [16.25938335] loss: 30.526235075441267\n",
            "Epoch: 4853 / 5000\n",
            "w1: [25.57967572] w2: [-23.46290047] bias: [16.25930712] loss: 30.52622303370313\n",
            "Epoch: 4854 / 5000\n",
            "w1: [25.57994159] w2: [-23.46310924] bias: [16.2592309] loss: 30.52621101110572\n",
            "Epoch: 4855 / 5000\n",
            "w1: [25.5802073] w2: [-23.46331776] bias: [16.25915469] loss: 30.526199007613556\n",
            "Epoch: 4856 / 5000\n",
            "w1: [25.58047285] w2: [-23.46352603] bias: [16.25907849] loss: 30.526187023191273\n",
            "Epoch: 4857 / 5000\n",
            "w1: [25.58073824] w2: [-23.46373405] bias: [16.2590023] loss: 30.52617505780353\n",
            "Epoch: 4858 / 5000\n",
            "w1: [25.58100348] w2: [-23.46394183] bias: [16.25892611] loss: 30.526163111415077\n",
            "Epoch: 4859 / 5000\n",
            "w1: [25.58126855] w2: [-23.46414935] bias: [16.25884993] loss: 30.52615118399073\n",
            "Epoch: 4860 / 5000\n",
            "w1: [25.58153346] w2: [-23.46435662] bias: [16.25877376] loss: 30.526139275495353\n",
            "Epoch: 4861 / 5000\n",
            "w1: [25.58179822] w2: [-23.46456364] bias: [16.2586976] loss: 30.526127385893897\n",
            "Epoch: 4862 / 5000\n",
            "w1: [25.58206281] w2: [-23.46477042] bias: [16.25862145] loss: 30.526115515151368\n",
            "Epoch: 4863 / 5000\n",
            "w1: [25.58232725] w2: [-23.46497694] bias: [16.2585453] loss: 30.526103663232842\n",
            "Epoch: 4864 / 5000\n",
            "w1: [25.58259153] w2: [-23.46518322] bias: [16.25846916] loss: 30.526091830103468\n",
            "Epoch: 4865 / 5000\n",
            "w1: [25.58285565] w2: [-23.46538925] bias: [16.25839303] loss: 30.526080015728446\n",
            "Epoch: 4866 / 5000\n",
            "w1: [25.58311961] w2: [-23.46559503] bias: [16.2583169] loss: 30.526068220073057\n",
            "Epoch: 4867 / 5000\n",
            "w1: [25.58338341] w2: [-23.46580057] bias: [16.25824079] loss: 30.52605644310264\n",
            "Epoch: 4868 / 5000\n",
            "w1: [25.58364706] w2: [-23.46600585] bias: [16.25816468] loss: 30.5260446847826\n",
            "Epoch: 4869 / 5000\n",
            "w1: [25.58391055] w2: [-23.46621089] bias: [16.25808858] loss: 30.526032945078413\n",
            "Epoch: 4870 / 5000\n",
            "w1: [25.58417387] w2: [-23.46641568] bias: [16.25801249] loss: 30.526021223955617\n",
            "Epoch: 4871 / 5000\n",
            "w1: [25.58443705] w2: [-23.46662023] bias: [16.25793641] loss: 30.526009521379805\n",
            "Epoch: 4872 / 5000\n",
            "w1: [25.58470006] w2: [-23.46682453] bias: [16.25786033] loss: 30.52599783731666\n",
            "Epoch: 4873 / 5000\n",
            "w1: [25.58496292] w2: [-23.46702858] bias: [16.25778426] loss: 30.5259861717319\n",
            "Epoch: 4874 / 5000\n",
            "w1: [25.58522562] w2: [-23.46723239] bias: [16.2577082] loss: 30.525974524591327\n",
            "Epoch: 4875 / 5000\n",
            "w1: [25.58548816] w2: [-23.46743595] bias: [16.25763215] loss: 30.52596289586083\n",
            "Epoch: 4876 / 5000\n",
            "w1: [25.58575054] w2: [-23.46763926] bias: [16.25755611] loss: 30.525951285506302\n",
            "Epoch: 4877 / 5000\n",
            "w1: [25.58601277] w2: [-23.46784233] bias: [16.25748007] loss: 30.525939693493754\n",
            "Epoch: 4878 / 5000\n",
            "w1: [25.58627484] w2: [-23.46804516] bias: [16.25740404] loss: 30.525928119789235\n",
            "Epoch: 4879 / 5000\n",
            "w1: [25.58653676] w2: [-23.46824774] bias: [16.25732802] loss: 30.525916564358877\n",
            "Epoch: 4880 / 5000\n",
            "w1: [25.58679852] w2: [-23.46845008] bias: [16.25725201] loss: 30.525905027168854\n",
            "Epoch: 4881 / 5000\n",
            "w1: [25.58706012] w2: [-23.46865217] bias: [16.257176] loss: 30.525893508185433\n",
            "Epoch: 4882 / 5000\n",
            "w1: [25.58732156] w2: [-23.46885402] bias: [16.25710001] loss: 30.52588200737491\n",
            "Epoch: 4883 / 5000\n",
            "w1: [25.58758285] w2: [-23.46905562] bias: [16.25702402] loss: 30.52587052470368\n",
            "Epoch: 4884 / 5000\n",
            "w1: [25.58784399] w2: [-23.46925698] bias: [16.25694804] loss: 30.525859060138178\n",
            "Epoch: 4885 / 5000\n",
            "w1: [25.58810497] w2: [-23.4694581] bias: [16.25687206] loss: 30.5258476136449\n",
            "Epoch: 4886 / 5000\n",
            "w1: [25.58836579] w2: [-23.46965897] bias: [16.2567961] loss: 30.525836185190432\n",
            "Epoch: 4887 / 5000\n",
            "w1: [25.58862645] w2: [-23.4698596] bias: [16.25672014] loss: 30.525824774741398\n",
            "Epoch: 4888 / 5000\n",
            "w1: [25.58888697] w2: [-23.47005999] bias: [16.25664419] loss: 30.525813382264495\n",
            "Epoch: 4889 / 5000\n",
            "w1: [25.58914732] w2: [-23.47026014] bias: [16.25656825] loss: 30.525802007726476\n",
            "Epoch: 4890 / 5000\n",
            "w1: [25.58940752] w2: [-23.47046004] bias: [16.25649231] loss: 30.525790651094184\n",
            "Epoch: 4891 / 5000\n",
            "w1: [25.58966757] w2: [-23.47065971] bias: [16.25641639] loss: 30.525779312334485\n",
            "Epoch: 4892 / 5000\n",
            "w1: [25.58992746] w2: [-23.47085913] bias: [16.25634047] loss: 30.525767991414327\n",
            "Epoch: 4893 / 5000\n",
            "w1: [25.5901872] w2: [-23.47105831] bias: [16.25626456] loss: 30.525756688300735\n",
            "Epoch: 4894 / 5000\n",
            "w1: [25.59044678] w2: [-23.47125724] bias: [16.25618866] loss: 30.525745402960776\n",
            "Epoch: 4895 / 5000\n",
            "w1: [25.59070621] w2: [-23.47145594] bias: [16.25611277] loss: 30.525734135361578\n",
            "Epoch: 4896 / 5000\n",
            "w1: [25.59096548] w2: [-23.4716544] bias: [16.25603688] loss: 30.52572288547035\n",
            "Epoch: 4897 / 5000\n",
            "w1: [25.5912246] w2: [-23.47185261] bias: [16.255961] loss: 30.52571165325435\n",
            "Epoch: 4898 / 5000\n",
            "w1: [25.59148356] w2: [-23.47205059] bias: [16.25588513] loss: 30.5257004386809\n",
            "Epoch: 4899 / 5000\n",
            "w1: [25.59174238] w2: [-23.47224832] bias: [16.25580927] loss: 30.525689241717384\n",
            "Epoch: 4900 / 5000\n",
            "w1: [25.59200103] w2: [-23.47244582] bias: [16.25573341] loss: 30.525678062331252\n",
            "Epoch: 4901 / 5000\n",
            "w1: [25.59225954] w2: [-23.47264308] bias: [16.25565757] loss: 30.525666900490005\n",
            "Epoch: 4902 / 5000\n",
            "w1: [25.59251789] w2: [-23.47284009] bias: [16.25558173] loss: 30.52565575616122\n",
            "Epoch: 4903 / 5000\n",
            "w1: [25.59277608] w2: [-23.47303687] bias: [16.2555059] loss: 30.525644629312524\n",
            "Epoch: 4904 / 5000\n",
            "w1: [25.59303413] w2: [-23.47323341] bias: [16.25543007] loss: 30.52563351991162\n",
            "Epoch: 4905 / 5000\n",
            "w1: [25.59329202] w2: [-23.47342971] bias: [16.25535426] loss: 30.52562242792625\n",
            "Epoch: 4906 / 5000\n",
            "w1: [25.59354976] w2: [-23.47362577] bias: [16.25527845] loss: 30.525611353324233\n",
            "Epoch: 4907 / 5000\n",
            "w1: [25.59380734] w2: [-23.4738216] bias: [16.25520265] loss: 30.525600296073456\n",
            "Epoch: 4908 / 5000\n",
            "w1: [25.59406477] w2: [-23.47401719] bias: [16.25512686] loss: 30.525589256141846\n",
            "Epoch: 4909 / 5000\n",
            "w1: [25.59432205] w2: [-23.47421253] bias: [16.25505108] loss: 30.525578233497402\n",
            "Epoch: 4910 / 5000\n",
            "w1: [25.59457918] w2: [-23.47440765] bias: [16.2549753] loss: 30.525567228108184\n",
            "Epoch: 4911 / 5000\n",
            "w1: [25.59483615] w2: [-23.47460252] bias: [16.25489954] loss: 30.52555623994231\n",
            "Epoch: 4912 / 5000\n",
            "w1: [25.59509298] w2: [-23.47479716] bias: [16.25482378] loss: 30.525545268967967\n",
            "Epoch: 4913 / 5000\n",
            "w1: [25.59534965] w2: [-23.47499156] bias: [16.25474803] loss: 30.525534315153394\n",
            "Epoch: 4914 / 5000\n",
            "w1: [25.59560617] w2: [-23.47518572] bias: [16.25467228] loss: 30.525523378466882\n",
            "Epoch: 4915 / 5000\n",
            "w1: [25.59586253] w2: [-23.47537965] bias: [16.25459655] loss: 30.525512458876815\n",
            "Epoch: 4916 / 5000\n",
            "w1: [25.59611875] w2: [-23.47557334] bias: [16.25452082] loss: 30.52550155635159\n",
            "Epoch: 4917 / 5000\n",
            "w1: [25.59637481] w2: [-23.4757668] bias: [16.2544451] loss: 30.525490670859696\n",
            "Epoch: 4918 / 5000\n",
            "w1: [25.59663073] w2: [-23.47596002] bias: [16.25436939] loss: 30.525479802369674\n",
            "Epoch: 4919 / 5000\n",
            "w1: [25.59688649] w2: [-23.476153] bias: [16.25429369] loss: 30.525468950850126\n",
            "Epoch: 4920 / 5000\n",
            "w1: [25.5971421] w2: [-23.47634575] bias: [16.25421799] loss: 30.52545811626971\n",
            "Epoch: 4921 / 5000\n",
            "w1: [25.59739756] w2: [-23.47653827] bias: [16.2541423] loss: 30.525447298597154\n",
            "Epoch: 4922 / 5000\n",
            "w1: [25.59765286] w2: [-23.47673055] bias: [16.25406662] loss: 30.52543649780122\n",
            "Epoch: 4923 / 5000\n",
            "w1: [25.59790802] w2: [-23.47692259] bias: [16.25399095] loss: 30.525425713850762\n",
            "Epoch: 4924 / 5000\n",
            "w1: [25.59816303] w2: [-23.47711441] bias: [16.25391529] loss: 30.52541494671467\n",
            "Epoch: 4925 / 5000\n",
            "w1: [25.59841789] w2: [-23.47730598] bias: [16.25383963] loss: 30.525404196361904\n",
            "Epoch: 4926 / 5000\n",
            "w1: [25.59867259] w2: [-23.47749733] bias: [16.25376398] loss: 30.525393462761482\n",
            "Epoch: 4927 / 5000\n",
            "w1: [25.59892715] w2: [-23.47768844] bias: [16.25368834] loss: 30.52538274588246\n",
            "Epoch: 4928 / 5000\n",
            "w1: [25.59918155] w2: [-23.47787932] bias: [16.25361271] loss: 30.52537204569399\n",
            "Epoch: 4929 / 5000\n",
            "w1: [25.59943581] w2: [-23.47806996] bias: [16.25353709] loss: 30.52536136216526\n",
            "Epoch: 4930 / 5000\n",
            "w1: [25.59968992] w2: [-23.47826037] bias: [16.25346147] loss: 30.52535069526551\n",
            "Epoch: 4931 / 5000\n",
            "w1: [25.59994387] w2: [-23.47845055] bias: [16.25338586] loss: 30.52534004496405\n",
            "Epoch: 4932 / 5000\n",
            "w1: [25.60019768] w2: [-23.4786405] bias: [16.25331026] loss: 30.525329411230263\n",
            "Epoch: 4933 / 5000\n",
            "w1: [25.60045134] w2: [-23.47883021] bias: [16.25323467] loss: 30.525318794033556\n",
            "Epoch: 4934 / 5000\n",
            "w1: [25.60070485] w2: [-23.4790197] bias: [16.25315908] loss: 30.525308193343417\n",
            "Epoch: 4935 / 5000\n",
            "w1: [25.60095821] w2: [-23.47920895] bias: [16.25308351] loss: 30.525297609129385\n",
            "Epoch: 4936 / 5000\n",
            "w1: [25.60121142] w2: [-23.47939797] bias: [16.25300794] loss: 30.52528704136106\n",
            "Epoch: 4937 / 5000\n",
            "w1: [25.60146448] w2: [-23.47958676] bias: [16.25293238] loss: 30.525276490008093\n",
            "Epoch: 4938 / 5000\n",
            "w1: [25.60171739] w2: [-23.47977531] bias: [16.25285683] loss: 30.525265955040204\n",
            "Epoch: 4939 / 5000\n",
            "w1: [25.60197015] w2: [-23.47996364] bias: [16.25278128] loss: 30.525255436427155\n",
            "Epoch: 4940 / 5000\n",
            "w1: [25.60222277] w2: [-23.48015174] bias: [16.25270574] loss: 30.525244934138783\n",
            "Epoch: 4941 / 5000\n",
            "w1: [25.60247524] w2: [-23.4803396] bias: [16.25263022] loss: 30.525234448144968\n",
            "Epoch: 4942 / 5000\n",
            "w1: [25.60272756] w2: [-23.48052724] bias: [16.25255469] loss: 30.525223978415653\n",
            "Epoch: 4943 / 5000\n",
            "w1: [25.60297973] w2: [-23.48071464] bias: [16.25247918] loss: 30.525213524920844\n",
            "Epoch: 4944 / 5000\n",
            "w1: [25.60323175] w2: [-23.48090182] bias: [16.25240368] loss: 30.52520308763059\n",
            "Epoch: 4945 / 5000\n",
            "w1: [25.60348363] w2: [-23.48108877] bias: [16.25232818] loss: 30.525192666515004\n",
            "Epoch: 4946 / 5000\n",
            "w1: [25.60373535] w2: [-23.48127549] bias: [16.25225269] loss: 30.525182261544263\n",
            "Epoch: 4947 / 5000\n",
            "w1: [25.60398693] w2: [-23.48146198] bias: [16.25217721] loss: 30.52517187268858\n",
            "Epoch: 4948 / 5000\n",
            "w1: [25.60423836] w2: [-23.48164824] bias: [16.25210174] loss: 30.525161499918255\n",
            "Epoch: 4949 / 5000\n",
            "w1: [25.60448965] w2: [-23.48183427] bias: [16.25202627] loss: 30.52515114320362\n",
            "Epoch: 4950 / 5000\n",
            "w1: [25.60474079] w2: [-23.48202007] bias: [16.25195082] loss: 30.525140802515057\n",
            "Epoch: 4951 / 5000\n",
            "w1: [25.60499178] w2: [-23.48220565] bias: [16.25187537] loss: 30.525130477823037\n",
            "Epoch: 4952 / 5000\n",
            "w1: [25.60524262] w2: [-23.48239099] bias: [16.25179993] loss: 30.525120169098063\n",
            "Epoch: 4953 / 5000\n",
            "w1: [25.60549332] w2: [-23.48257611] bias: [16.25172449] loss: 30.52510987631069\n",
            "Epoch: 4954 / 5000\n",
            "w1: [25.60574387] w2: [-23.48276101] bias: [16.25164907] loss: 30.525099599431556\n",
            "Epoch: 4955 / 5000\n",
            "w1: [25.60599427] w2: [-23.48294567] bias: [16.25157365] loss: 30.525089338431314\n",
            "Epoch: 4956 / 5000\n",
            "w1: [25.60624453] w2: [-23.48313011] bias: [16.25149824] loss: 30.525079093280713\n",
            "Epoch: 4957 / 5000\n",
            "w1: [25.60649464] w2: [-23.48331432] bias: [16.25142284] loss: 30.52506886395052\n",
            "Epoch: 4958 / 5000\n",
            "w1: [25.6067446] w2: [-23.48349831] bias: [16.25134745] loss: 30.5250586504116\n",
            "Epoch: 4959 / 5000\n",
            "w1: [25.60699442] w2: [-23.48368207] bias: [16.25127206] loss: 30.525048452634834\n",
            "Epoch: 4960 / 5000\n",
            "w1: [25.6072441] w2: [-23.4838656] bias: [16.25119669] loss: 30.52503827059118\n",
            "Epoch: 4961 / 5000\n",
            "w1: [25.60749362] w2: [-23.48404891] bias: [16.25112132] loss: 30.52502810425165\n",
            "Epoch: 4962 / 5000\n",
            "w1: [25.607743] w2: [-23.48423199] bias: [16.25104596] loss: 30.52501795358729\n",
            "Epoch: 4963 / 5000\n",
            "w1: [25.60799224] w2: [-23.48441485] bias: [16.2509706] loss: 30.52500781856924\n",
            "Epoch: 4964 / 5000\n",
            "w1: [25.60824133] w2: [-23.48459748] bias: [16.25089526] loss: 30.524997699168654\n",
            "Epoch: 4965 / 5000\n",
            "w1: [25.60849027] w2: [-23.48477989] bias: [16.25081992] loss: 30.524987595356762\n",
            "Epoch: 4966 / 5000\n",
            "w1: [25.60873907] w2: [-23.48496207] bias: [16.25074459] loss: 30.524977507104854\n",
            "Epoch: 4967 / 5000\n",
            "w1: [25.60898773] w2: [-23.48514403] bias: [16.25066927] loss: 30.52496743438426\n",
            "Epoch: 4968 / 5000\n",
            "w1: [25.60923624] w2: [-23.48532576] bias: [16.25059396] loss: 30.52495737716637\n",
            "Epoch: 4969 / 5000\n",
            "w1: [25.6094846] w2: [-23.48550727] bias: [16.25051865] loss: 30.52494733542263\n",
            "Epoch: 4970 / 5000\n",
            "w1: [25.60973282] w2: [-23.48568855] bias: [16.25044335] loss: 30.52493730912454\n",
            "Epoch: 4971 / 5000\n",
            "w1: [25.6099809] w2: [-23.48586962] bias: [16.25036806] loss: 30.524927298243643\n",
            "Epoch: 4972 / 5000\n",
            "w1: [25.61022883] w2: [-23.48605045] bias: [16.25029278] loss: 30.524917302751554\n",
            "Epoch: 4973 / 5000\n",
            "w1: [25.61047662] w2: [-23.48623107] bias: [16.25021751] loss: 30.52490732261994\n",
            "Epoch: 4974 / 5000\n",
            "w1: [25.61072426] w2: [-23.48641146] bias: [16.25014224] loss: 30.524897357820503\n",
            "Epoch: 4975 / 5000\n",
            "w1: [25.61097176] w2: [-23.48659163] bias: [16.25006699] loss: 30.524887408325018\n",
            "Epoch: 4976 / 5000\n",
            "w1: [25.61121911] w2: [-23.48677158] bias: [16.24999174] loss: 30.5248774741053\n",
            "Epoch: 4977 / 5000\n",
            "w1: [25.61146632] w2: [-23.4869513] bias: [16.2499165] loss: 30.52486755513323\n",
            "Epoch: 4978 / 5000\n",
            "w1: [25.61171339] w2: [-23.48713081] bias: [16.24984126] loss: 30.52485765138074\n",
            "Epoch: 4979 / 5000\n",
            "w1: [25.61196032] w2: [-23.48731009] bias: [16.24976604] loss: 30.5248477628198\n",
            "Epoch: 4980 / 5000\n",
            "w1: [25.6122071] w2: [-23.48748915] bias: [16.24969082] loss: 30.524837889422447\n",
            "Epoch: 4981 / 5000\n",
            "w1: [25.61245373] w2: [-23.48766798] bias: [16.24961561] loss: 30.524828031160776\n",
            "Epoch: 4982 / 5000\n",
            "w1: [25.61270023] w2: [-23.4878466] bias: [16.24954041] loss: 30.524818188006922\n",
            "Epoch: 4983 / 5000\n",
            "w1: [25.61294658] w2: [-23.488025] bias: [16.24946522] loss: 30.524808359933083\n",
            "Epoch: 4984 / 5000\n",
            "w1: [25.61319279] w2: [-23.48820317] bias: [16.24939003] loss: 30.5247985469115\n",
            "Epoch: 4985 / 5000\n",
            "w1: [25.61343885] w2: [-23.48838113] bias: [16.24931486] loss: 30.524788748914474\n",
            "Epoch: 4986 / 5000\n",
            "w1: [25.61368477] w2: [-23.48855886] bias: [16.24923969] loss: 30.524778965914354\n",
            "Epoch: 4987 / 5000\n",
            "w1: [25.61393055] w2: [-23.48873637] bias: [16.24916453] loss: 30.524769197883547\n",
            "Epoch: 4988 / 5000\n",
            "w1: [25.61417619] w2: [-23.48891367] bias: [16.24908937] loss: 30.524759444794512\n",
            "Epoch: 4989 / 5000\n",
            "w1: [25.61442169] w2: [-23.48909074] bias: [16.24901423] loss: 30.524749706619744\n",
            "Epoch: 4990 / 5000\n",
            "w1: [25.61466704] w2: [-23.4892676] bias: [16.24893909] loss: 30.52473998333182\n",
            "Epoch: 4991 / 5000\n",
            "w1: [25.61491225] w2: [-23.48944423] bias: [16.24886396] loss: 30.524730274903337\n",
            "Epoch: 4992 / 5000\n",
            "w1: [25.61515732] w2: [-23.48962065] bias: [16.24878884] loss: 30.524720581306973\n",
            "Epoch: 4993 / 5000\n",
            "w1: [25.61540225] w2: [-23.48979685] bias: [16.24871373] loss: 30.52471090251544\n",
            "Epoch: 4994 / 5000\n",
            "w1: [25.61564703] w2: [-23.48997283] bias: [16.24863862] loss: 30.524701238501496\n",
            "Epoch: 4995 / 5000\n",
            "w1: [25.61589167] w2: [-23.49014859] bias: [16.24856352] loss: 30.524691589237975\n",
            "Epoch: 4996 / 5000\n",
            "w1: [25.61613618] w2: [-23.49032413] bias: [16.24848844] loss: 30.524681954697737\n",
            "Epoch: 4997 / 5000\n",
            "w1: [25.61638054] w2: [-23.49049946] bias: [16.24841335] loss: 30.52467233485372\n",
            "Epoch: 4998 / 5000\n",
            "w1: [25.61662476] w2: [-23.49067456] bias: [16.24833828] loss: 30.524662729678877\n",
            "Epoch: 4999 / 5000\n",
            "w1: [25.61686884] w2: [-23.49084945] bias: [16.24826322] loss: 30.52465313914625\n",
            "Epoch: 5000 / 5000\n",
            "w1: [25.61711278] w2: [-23.49102413] bias: [16.24818816] loss: 30.524643563228903\n",
            "##### 최종 w1, w2, bias #######\n",
            "[25.61711278] [-23.49102413] [16.24818816]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 계산된 Weight와 Bias를 이용하여 Price 예측\n",
        "* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산."
      ],
      "metadata": {
        "id": "qqE3INCeV3T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "GMbQKQh8V3T2",
        "outputId": "f3552c24-6327-4fed-d0e3-998ef6e56009"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
              "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
              "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
              "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
              "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
              "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
              "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
              "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
              "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
              "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
              "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  \n",
              "0     15.3  396.90   4.98   24.0        28.935533  \n",
              "1     17.8  396.90   9.14   21.6        25.483093  \n",
              "2     17.8  392.83   4.03   34.7        32.545474  \n",
              "3     18.7  394.63   2.94   33.4        32.334142  \n",
              "4     18.7  396.90   5.33   36.2        31.516284  \n",
              "5     18.7  394.12   5.21   28.7        28.074722  \n",
              "6     15.2  395.60  12.43   22.9        21.342942  \n",
              "7     15.2  396.90  19.15   27.1        17.772340  \n",
              "8     15.2  386.63  29.93   16.5         8.129206  \n",
              "9     15.2  386.71  17.10   18.9        18.276548  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-149a81fc-2bed-4a30-b193-8b676fe62cce\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.935533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>25.483093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>32.545474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>32.334142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>31.516284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>28.074722</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.342942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>17.772340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>8.129206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>18.276548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-149a81fc-2bed-4a30-b193-8b676fe62cce')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-149a81fc-2bed-4a30-b193-8b676fe62cce button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-149a81fc-2bed-4a30-b193-8b676fe62cce');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1db0b19f-b193-41bb-b9f7-b570028d6400\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1db0b19f-b193-41bb-b9f7-b570028d6400')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1db0b19f-b193-41bb-b9f7-b570028d6400 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras를 이용하여 보스턴 주택가격 모델 학습 및 예측\n",
        "* Dense Layer를 이용하여 퍼셉트론 구현. units는 1로 설정."
      ],
      "metadata": {
        "id": "PV5xXwEWV3T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dense??"
      ],
      "metadata": {
        "id": "9b1fNgh7eCmS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음.\n",
        "    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용.\n",
        "    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n",
        "])\n",
        "# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행.\n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
        "model.fit(scaled_features, bostonDF['PRICE'].values, epochs=1000)\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uII8BeHVV3T3",
        "outputId": "59a2d5f1-9e97-4298-c2da-86650e95c449"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "16/16 [==============================] - 1s 2ms/step - loss: 542.5001 - mse: 542.5001\n",
            "Epoch 2/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 530.4066 - mse: 530.4066\n",
            "Epoch 3/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 518.4561 - mse: 518.4561\n",
            "Epoch 4/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 506.7931 - mse: 506.7931\n",
            "Epoch 5/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 495.2899 - mse: 495.2899\n",
            "Epoch 6/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 484.1107 - mse: 484.1107\n",
            "Epoch 7/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 473.1477 - mse: 473.1477\n",
            "Epoch 8/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 462.3571 - mse: 462.3571\n",
            "Epoch 9/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 451.8850 - mse: 451.8850\n",
            "Epoch 10/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 441.5587 - mse: 441.5587\n",
            "Epoch 11/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 431.5063 - mse: 431.5063\n",
            "Epoch 12/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 421.7782 - mse: 421.7782\n",
            "Epoch 13/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 412.1423 - mse: 412.1423\n",
            "Epoch 14/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 402.6486 - mse: 402.6486\n",
            "Epoch 15/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 393.6215 - mse: 393.6215\n",
            "Epoch 16/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 384.5878 - mse: 384.5878\n",
            "Epoch 17/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 375.7790 - mse: 375.7790\n",
            "Epoch 18/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 367.3608 - mse: 367.3608\n",
            "Epoch 19/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 358.9787 - mse: 358.9787\n",
            "Epoch 20/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 350.7897 - mse: 350.7897\n",
            "Epoch 21/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 342.7961 - mse: 342.7961\n",
            "Epoch 22/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 335.0458 - mse: 335.0458\n",
            "Epoch 23/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 327.5223 - mse: 327.5223\n",
            "Epoch 24/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 320.0805 - mse: 320.0805\n",
            "Epoch 25/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 312.8489 - mse: 312.8489\n",
            "Epoch 26/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 305.8834 - mse: 305.8834\n",
            "Epoch 27/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 299.0002 - mse: 299.0002\n",
            "Epoch 28/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 292.2929 - mse: 292.2929\n",
            "Epoch 29/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 285.8287 - mse: 285.8287\n",
            "Epoch 30/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 279.4427 - mse: 279.4427\n",
            "Epoch 31/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 273.2681 - mse: 273.2681\n",
            "Epoch 32/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 267.2914 - mse: 267.2914\n",
            "Epoch 33/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 261.3810 - mse: 261.3810\n",
            "Epoch 34/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 255.7495 - mse: 255.7495\n",
            "Epoch 35/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 250.1431 - mse: 250.1431\n",
            "Epoch 36/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 244.7789 - mse: 244.7789\n",
            "Epoch 37/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 239.4635 - mse: 239.4635\n",
            "Epoch 38/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 234.3850 - mse: 234.3850\n",
            "Epoch 39/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 229.4477 - mse: 229.4477\n",
            "Epoch 40/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 224.5621 - mse: 224.5621\n",
            "Epoch 41/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 219.8701 - mse: 219.8701\n",
            "Epoch 42/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 215.2538 - mse: 215.2538\n",
            "Epoch 43/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 210.8766 - mse: 210.8766\n",
            "Epoch 44/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 206.5596 - mse: 206.5596\n",
            "Epoch 45/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 202.4023 - mse: 202.4023\n",
            "Epoch 46/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 198.2924 - mse: 198.2924\n",
            "Epoch 47/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 194.3404 - mse: 194.3404\n",
            "Epoch 48/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 190.5505 - mse: 190.5505\n",
            "Epoch 49/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 186.8211 - mse: 186.8211\n",
            "Epoch 50/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 183.2547 - mse: 183.2547\n",
            "Epoch 51/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 179.7780 - mse: 179.7780\n",
            "Epoch 52/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 176.3537 - mse: 176.3537\n",
            "Epoch 53/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 173.1119 - mse: 173.1119\n",
            "Epoch 54/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 169.8960 - mse: 169.8960\n",
            "Epoch 55/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 166.8345 - mse: 166.8345\n",
            "Epoch 56/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 163.8677 - mse: 163.8677\n",
            "Epoch 57/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 160.9019 - mse: 160.9019\n",
            "Epoch 58/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 158.1852 - mse: 158.1852\n",
            "Epoch 59/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 155.4437 - mse: 155.4437\n",
            "Epoch 60/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 152.8143 - mse: 152.8143\n",
            "Epoch 61/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 150.2944 - mse: 150.2944\n",
            "Epoch 62/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 147.8307 - mse: 147.8307\n",
            "Epoch 63/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 145.4622 - mse: 145.4622\n",
            "Epoch 64/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 143.1539 - mse: 143.1539\n",
            "Epoch 65/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 140.9258 - mse: 140.9258\n",
            "Epoch 66/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 138.8016 - mse: 138.8016\n",
            "Epoch 67/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 136.7183 - mse: 136.7183\n",
            "Epoch 68/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 134.7057 - mse: 134.7057\n",
            "Epoch 69/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 132.7275 - mse: 132.7275\n",
            "Epoch 70/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 130.9335 - mse: 130.9335\n",
            "Epoch 71/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 129.0520 - mse: 129.0520\n",
            "Epoch 72/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 127.2802 - mse: 127.2802\n",
            "Epoch 73/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 125.5933 - mse: 125.5933\n",
            "Epoch 74/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 124.0033 - mse: 124.0033\n",
            "Epoch 75/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 122.3456 - mse: 122.3456\n",
            "Epoch 76/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 120.8223 - mse: 120.8223\n",
            "Epoch 77/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 119.3824 - mse: 119.3824\n",
            "Epoch 78/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 117.9198 - mse: 117.9198\n",
            "Epoch 79/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 116.5242 - mse: 116.5242\n",
            "Epoch 80/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 115.2207 - mse: 115.2207\n",
            "Epoch 81/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 113.8997 - mse: 113.8997\n",
            "Epoch 82/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 112.6544 - mse: 112.6544\n",
            "Epoch 83/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 111.4354 - mse: 111.4354\n",
            "Epoch 84/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 110.2690 - mse: 110.2690\n",
            "Epoch 85/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 109.1287 - mse: 109.1287\n",
            "Epoch 86/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 108.0733 - mse: 108.0733\n",
            "Epoch 87/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 107.0242 - mse: 107.0242\n",
            "Epoch 88/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 105.9394 - mse: 105.9394\n",
            "Epoch 89/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 104.9673 - mse: 104.9673\n",
            "Epoch 90/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 104.0147 - mse: 104.0147\n",
            "Epoch 91/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 103.0565 - mse: 103.0565\n",
            "Epoch 92/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 102.1894 - mse: 102.1894\n",
            "Epoch 93/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 101.3028 - mse: 101.3028\n",
            "Epoch 94/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 100.4538 - mse: 100.4538\n",
            "Epoch 95/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 99.6087 - mse: 99.6087\n",
            "Epoch 96/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 98.8070 - mse: 98.8070\n",
            "Epoch 97/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 98.0319 - mse: 98.0319\n",
            "Epoch 98/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 97.2729 - mse: 97.2729\n",
            "Epoch 99/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 96.5289 - mse: 96.5289\n",
            "Epoch 100/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 95.8126 - mse: 95.8126\n",
            "Epoch 101/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 95.1001 - mse: 95.1001\n",
            "Epoch 102/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 94.4412 - mse: 94.4412\n",
            "Epoch 103/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 93.7690 - mse: 93.7690\n",
            "Epoch 104/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 93.1173 - mse: 93.1173\n",
            "Epoch 105/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 92.4823 - mse: 92.4823\n",
            "Epoch 106/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 91.8673 - mse: 91.8673\n",
            "Epoch 107/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 91.2482 - mse: 91.2482\n",
            "Epoch 108/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 90.6833 - mse: 90.6833\n",
            "Epoch 109/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 90.0864 - mse: 90.0864\n",
            "Epoch 110/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 89.5337 - mse: 89.5337\n",
            "Epoch 111/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 88.9731 - mse: 88.9731\n",
            "Epoch 112/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 88.4314 - mse: 88.4314\n",
            "Epoch 113/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 87.8819 - mse: 87.8819\n",
            "Epoch 114/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 87.3673 - mse: 87.3673\n",
            "Epoch 115/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 86.8558 - mse: 86.8558\n",
            "Epoch 116/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 86.3414 - mse: 86.3414\n",
            "Epoch 117/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 85.8454 - mse: 85.8454\n",
            "Epoch 118/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 85.3618 - mse: 85.3618\n",
            "Epoch 119/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 84.8637 - mse: 84.8637\n",
            "Epoch 120/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 84.3864 - mse: 84.3864\n",
            "Epoch 121/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 83.9152 - mse: 83.9152\n",
            "Epoch 122/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 83.4524 - mse: 83.4524\n",
            "Epoch 123/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 82.9868 - mse: 82.9868\n",
            "Epoch 124/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 82.5384 - mse: 82.5384\n",
            "Epoch 125/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 82.0896 - mse: 82.0896\n",
            "Epoch 126/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 81.6503 - mse: 81.6503\n",
            "Epoch 127/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 81.1897 - mse: 81.1897\n",
            "Epoch 128/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 80.7739 - mse: 80.7739\n",
            "Epoch 129/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 80.3360 - mse: 80.3360\n",
            "Epoch 130/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 79.9188 - mse: 79.9188\n",
            "Epoch 131/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 79.4835 - mse: 79.4835\n",
            "Epoch 132/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 79.0729 - mse: 79.0729\n",
            "Epoch 133/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 78.6638 - mse: 78.6638\n",
            "Epoch 134/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 78.2538 - mse: 78.2538\n",
            "Epoch 135/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 77.8379 - mse: 77.8379\n",
            "Epoch 136/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 77.4441 - mse: 77.4441\n",
            "Epoch 137/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 77.0339 - mse: 77.0339\n",
            "Epoch 138/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 76.6470 - mse: 76.6470\n",
            "Epoch 139/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 76.2521 - mse: 76.2521\n",
            "Epoch 140/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 75.8541 - mse: 75.8541\n",
            "Epoch 141/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 75.4631 - mse: 75.4631\n",
            "Epoch 142/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 75.0806 - mse: 75.0806\n",
            "Epoch 143/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 74.6919 - mse: 74.6919\n",
            "Epoch 144/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 74.3115 - mse: 74.3115\n",
            "Epoch 145/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 73.9381 - mse: 73.9381\n",
            "Epoch 146/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 73.5704 - mse: 73.5704\n",
            "Epoch 147/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 73.1781 - mse: 73.1781\n",
            "Epoch 148/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 72.8193 - mse: 72.8193\n",
            "Epoch 149/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 72.4349 - mse: 72.4349\n",
            "Epoch 150/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 72.0861 - mse: 72.0861\n",
            "Epoch 151/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 71.7101 - mse: 71.7101\n",
            "Epoch 152/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 71.3492 - mse: 71.3492\n",
            "Epoch 153/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 70.9865 - mse: 70.9865\n",
            "Epoch 154/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 70.6213 - mse: 70.6213\n",
            "Epoch 155/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 70.2678 - mse: 70.2678\n",
            "Epoch 156/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 69.9118 - mse: 69.9118\n",
            "Epoch 157/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 69.5502 - mse: 69.5502\n",
            "Epoch 158/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 69.2107 - mse: 69.2107\n",
            "Epoch 159/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 68.8509 - mse: 68.8509\n",
            "Epoch 160/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 68.5014 - mse: 68.5014\n",
            "Epoch 161/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 68.1608 - mse: 68.1608\n",
            "Epoch 162/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 67.8134 - mse: 67.8134\n",
            "Epoch 163/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 67.4688 - mse: 67.4688\n",
            "Epoch 164/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 67.1339 - mse: 67.1339\n",
            "Epoch 165/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 66.7919 - mse: 66.7919\n",
            "Epoch 166/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 66.4551 - mse: 66.4551\n",
            "Epoch 167/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 66.1190 - mse: 66.1190\n",
            "Epoch 168/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 65.7876 - mse: 65.7876\n",
            "Epoch 169/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 65.4581 - mse: 65.4581\n",
            "Epoch 170/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 65.1255 - mse: 65.1255\n",
            "Epoch 171/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 64.7999 - mse: 64.7999\n",
            "Epoch 172/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 64.4718 - mse: 64.4718\n",
            "Epoch 173/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 64.1402 - mse: 64.1402\n",
            "Epoch 174/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 63.8259 - mse: 63.8259\n",
            "Epoch 175/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 63.5059 - mse: 63.5059\n",
            "Epoch 176/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 63.1795 - mse: 63.1795\n",
            "Epoch 177/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.8646 - mse: 62.8646\n",
            "Epoch 178/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.5475 - mse: 62.5475\n",
            "Epoch 179/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 62.2476 - mse: 62.2476\n",
            "Epoch 180/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 61.9238 - mse: 61.9238\n",
            "Epoch 181/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 61.6116 - mse: 61.6116\n",
            "Epoch 182/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 61.3083 - mse: 61.3083\n",
            "Epoch 183/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 60.9972 - mse: 60.9972\n",
            "Epoch 184/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 60.7021 - mse: 60.7021\n",
            "Epoch 185/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 60.3875 - mse: 60.3875\n",
            "Epoch 186/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 60.0937 - mse: 60.0937\n",
            "Epoch 187/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 59.8050 - mse: 59.8050\n",
            "Epoch 188/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 59.4942 - mse: 59.4942\n",
            "Epoch 189/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 59.2046 - mse: 59.2046\n",
            "Epoch 190/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 58.9102 - mse: 58.9102\n",
            "Epoch 191/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 58.6180 - mse: 58.6180\n",
            "Epoch 192/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 58.3279 - mse: 58.3279\n",
            "Epoch 193/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 58.0423 - mse: 58.0423\n",
            "Epoch 194/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 57.7680 - mse: 57.7680\n",
            "Epoch 195/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 57.4753 - mse: 57.4753\n",
            "Epoch 196/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 57.1864 - mse: 57.1864\n",
            "Epoch 197/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.9074 - mse: 56.9074\n",
            "Epoch 198/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.6421 - mse: 56.6421\n",
            "Epoch 199/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.3597 - mse: 56.3597\n",
            "Epoch 200/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 56.0785 - mse: 56.0785\n",
            "Epoch 201/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.8056 - mse: 55.8056\n",
            "Epoch 202/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.5404 - mse: 55.5404\n",
            "Epoch 203/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 55.2677 - mse: 55.2677\n",
            "Epoch 204/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.9941 - mse: 54.9941\n",
            "Epoch 205/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.7464 - mse: 54.7464\n",
            "Epoch 206/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.4651 - mse: 54.4651\n",
            "Epoch 207/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 54.2041 - mse: 54.2041\n",
            "Epoch 208/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.9382 - mse: 53.9382\n",
            "Epoch 209/1000\n",
            "16/16 [==============================] - 0s 1ms/step - loss: 53.6836 - mse: 53.6836\n",
            "Epoch 210/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.4259 - mse: 53.4259\n",
            "Epoch 211/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 53.1796 - mse: 53.1796\n",
            "Epoch 212/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 52.9198 - mse: 52.9198\n",
            "Epoch 213/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 52.6753 - mse: 52.6753\n",
            "Epoch 214/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 52.4180 - mse: 52.4180\n",
            "Epoch 215/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 52.1749 - mse: 52.1749\n",
            "Epoch 216/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 51.9258 - mse: 51.9258\n",
            "Epoch 217/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 51.6932 - mse: 51.6932\n",
            "Epoch 218/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 51.4403 - mse: 51.4403\n",
            "Epoch 219/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 51.1947 - mse: 51.1947\n",
            "Epoch 220/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.9699 - mse: 50.9699\n",
            "Epoch 221/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.7231 - mse: 50.7231\n",
            "Epoch 222/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.4879 - mse: 50.4879\n",
            "Epoch 223/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.2585 - mse: 50.2585\n",
            "Epoch 224/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 50.0287 - mse: 50.0287\n",
            "Epoch 225/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.7974 - mse: 49.7974\n",
            "Epoch 226/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.5641 - mse: 49.5641\n",
            "Epoch 227/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.3391 - mse: 49.3391\n",
            "Epoch 228/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 49.1146 - mse: 49.1146\n",
            "Epoch 229/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.8898 - mse: 48.8898\n",
            "Epoch 230/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.6730 - mse: 48.6730\n",
            "Epoch 231/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.4567 - mse: 48.4567\n",
            "Epoch 232/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.2317 - mse: 48.2317\n",
            "Epoch 233/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 48.0172 - mse: 48.0172\n",
            "Epoch 234/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.8033 - mse: 47.8033\n",
            "Epoch 235/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.5890 - mse: 47.5890\n",
            "Epoch 236/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.3806 - mse: 47.3806\n",
            "Epoch 237/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 47.1760 - mse: 47.1760\n",
            "Epoch 238/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.9626 - mse: 46.9626\n",
            "Epoch 239/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.7559 - mse: 46.7559\n",
            "Epoch 240/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.5644 - mse: 46.5644\n",
            "Epoch 241/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.3503 - mse: 46.3503\n",
            "Epoch 242/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 46.1484 - mse: 46.1484\n",
            "Epoch 243/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.9599 - mse: 45.9599\n",
            "Epoch 244/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.7525 - mse: 45.7525\n",
            "Epoch 245/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.5655 - mse: 45.5655\n",
            "Epoch 246/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.3597 - mse: 45.3597\n",
            "Epoch 247/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 45.1736 - mse: 45.1736\n",
            "Epoch 248/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.9818 - mse: 44.9818\n",
            "Epoch 249/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.7945 - mse: 44.7945\n",
            "Epoch 250/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.6010 - mse: 44.6010\n",
            "Epoch 251/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.4217 - mse: 44.4217\n",
            "Epoch 252/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.2320 - mse: 44.2320\n",
            "Epoch 253/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 44.0564 - mse: 44.0564\n",
            "Epoch 254/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.8781 - mse: 43.8781\n",
            "Epoch 255/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.6985 - mse: 43.6985\n",
            "Epoch 256/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.5261 - mse: 43.5261\n",
            "Epoch 257/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.3429 - mse: 43.3429\n",
            "Epoch 258/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 43.1725 - mse: 43.1725\n",
            "Epoch 259/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.9941 - mse: 42.9941\n",
            "Epoch 260/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.8331 - mse: 42.8331\n",
            "Epoch 261/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.6566 - mse: 42.6566\n",
            "Epoch 262/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.4920 - mse: 42.4920\n",
            "Epoch 263/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.3335 - mse: 42.3335\n",
            "Epoch 264/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 42.1559 - mse: 42.1559\n",
            "Epoch 265/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.9983 - mse: 41.9983\n",
            "Epoch 266/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.8435 - mse: 41.8435\n",
            "Epoch 267/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.6824 - mse: 41.6824\n",
            "Epoch 268/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.5279 - mse: 41.5279\n",
            "Epoch 269/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.3636 - mse: 41.3636\n",
            "Epoch 270/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.2152 - mse: 41.2152\n",
            "Epoch 271/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 41.0610 - mse: 41.0610\n",
            "Epoch 272/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.9098 - mse: 40.9098\n",
            "Epoch 273/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.7720 - mse: 40.7720\n",
            "Epoch 274/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.6130 - mse: 40.6130\n",
            "Epoch 275/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.4655 - mse: 40.4655\n",
            "Epoch 276/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.3210 - mse: 40.3210\n",
            "Epoch 277/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.1819 - mse: 40.1819\n",
            "Epoch 278/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 40.0310 - mse: 40.0310\n",
            "Epoch 279/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.8984 - mse: 39.8984\n",
            "Epoch 280/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.7487 - mse: 39.7487\n",
            "Epoch 281/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.6153 - mse: 39.6153\n",
            "Epoch 282/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.4761 - mse: 39.4761\n",
            "Epoch 283/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.3523 - mse: 39.3523\n",
            "Epoch 284/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.2058 - mse: 39.2058\n",
            "Epoch 285/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 39.0764 - mse: 39.0764\n",
            "Epoch 286/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.9532 - mse: 38.9532\n",
            "Epoch 287/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.8243 - mse: 38.8243\n",
            "Epoch 288/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.6926 - mse: 38.6926\n",
            "Epoch 289/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.5556 - mse: 38.5556\n",
            "Epoch 290/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.4370 - mse: 38.4370\n",
            "Epoch 291/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.3105 - mse: 38.3105\n",
            "Epoch 292/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.1883 - mse: 38.1883\n",
            "Epoch 293/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 38.0684 - mse: 38.0684\n",
            "Epoch 294/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.9433 - mse: 37.9433\n",
            "Epoch 295/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.8391 - mse: 37.8391\n",
            "Epoch 296/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.7160 - mse: 37.7160\n",
            "Epoch 297/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.6012 - mse: 37.6012\n",
            "Epoch 298/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.4832 - mse: 37.4832\n",
            "Epoch 299/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.3752 - mse: 37.3752\n",
            "Epoch 300/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.2542 - mse: 37.2542\n",
            "Epoch 301/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.1465 - mse: 37.1465\n",
            "Epoch 302/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 37.0400 - mse: 37.0400\n",
            "Epoch 303/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.9319 - mse: 36.9319\n",
            "Epoch 304/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.8237 - mse: 36.8237\n",
            "Epoch 305/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.7279 - mse: 36.7279\n",
            "Epoch 306/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.6171 - mse: 36.6171\n",
            "Epoch 307/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.5137 - mse: 36.5137\n",
            "Epoch 308/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.4143 - mse: 36.4143\n",
            "Epoch 309/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.3133 - mse: 36.3133\n",
            "Epoch 310/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.2114 - mse: 36.2114\n",
            "Epoch 311/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.1151 - mse: 36.1151\n",
            "Epoch 312/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 36.0153 - mse: 36.0153\n",
            "Epoch 313/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.9295 - mse: 35.9295\n",
            "Epoch 314/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.8358 - mse: 35.8358\n",
            "Epoch 315/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.7336 - mse: 35.7336\n",
            "Epoch 316/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.6454 - mse: 35.6454\n",
            "Epoch 317/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.5537 - mse: 35.5537\n",
            "Epoch 318/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.4693 - mse: 35.4693\n",
            "Epoch 319/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.3785 - mse: 35.3785\n",
            "Epoch 320/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.2877 - mse: 35.2877\n",
            "Epoch 321/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.2078 - mse: 35.2078\n",
            "Epoch 322/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.1263 - mse: 35.1263\n",
            "Epoch 323/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 35.0353 - mse: 35.0353\n",
            "Epoch 324/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.9680 - mse: 34.9680\n",
            "Epoch 325/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.8700 - mse: 34.8700\n",
            "Epoch 326/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.7954 - mse: 34.7954\n",
            "Epoch 327/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.7232 - mse: 34.7232\n",
            "Epoch 328/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.6361 - mse: 34.6361\n",
            "Epoch 329/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.5693 - mse: 34.5693\n",
            "Epoch 330/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.4807 - mse: 34.4807\n",
            "Epoch 331/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.4155 - mse: 34.4155\n",
            "Epoch 332/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.3363 - mse: 34.3363\n",
            "Epoch 333/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.2653 - mse: 34.2653\n",
            "Epoch 334/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.1963 - mse: 34.1963\n",
            "Epoch 335/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.1311 - mse: 34.1311\n",
            "Epoch 336/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 34.0500 - mse: 34.0500\n",
            "Epoch 337/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.9847 - mse: 33.9847\n",
            "Epoch 338/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.9198 - mse: 33.9198\n",
            "Epoch 339/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.8567 - mse: 33.8567\n",
            "Epoch 340/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.7867 - mse: 33.7867\n",
            "Epoch 341/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.7210 - mse: 33.7210\n",
            "Epoch 342/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.6605 - mse: 33.6605\n",
            "Epoch 343/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.6033 - mse: 33.6033\n",
            "Epoch 344/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.5395 - mse: 33.5395\n",
            "Epoch 345/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.4749 - mse: 33.4749\n",
            "Epoch 346/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.4252 - mse: 33.4252\n",
            "Epoch 347/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.3556 - mse: 33.3556\n",
            "Epoch 348/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.2958 - mse: 33.2958\n",
            "Epoch 349/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.2431 - mse: 33.2431\n",
            "Epoch 350/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.1871 - mse: 33.1871\n",
            "Epoch 351/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.1361 - mse: 33.1361\n",
            "Epoch 352/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.0760 - mse: 33.0760\n",
            "Epoch 353/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 33.0222 - mse: 33.0222\n",
            "Epoch 354/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.9743 - mse: 32.9743\n",
            "Epoch 355/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.9204 - mse: 32.9204\n",
            "Epoch 356/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.8739 - mse: 32.8739\n",
            "Epoch 357/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.8199 - mse: 32.8199\n",
            "Epoch 358/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.7747 - mse: 32.7747\n",
            "Epoch 359/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.7224 - mse: 32.7224\n",
            "Epoch 360/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.6859 - mse: 32.6859\n",
            "Epoch 361/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.6293 - mse: 32.6293\n",
            "Epoch 362/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.5907 - mse: 32.5907\n",
            "Epoch 363/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.5456 - mse: 32.5456\n",
            "Epoch 364/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.4989 - mse: 32.4989\n",
            "Epoch 365/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.4524 - mse: 32.4524\n",
            "Epoch 366/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.4189 - mse: 32.4189\n",
            "Epoch 367/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.3713 - mse: 32.3713\n",
            "Epoch 368/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.3291 - mse: 32.3291\n",
            "Epoch 369/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.2939 - mse: 32.2939\n",
            "Epoch 370/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.2481 - mse: 32.2481\n",
            "Epoch 371/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.2194 - mse: 32.2194\n",
            "Epoch 372/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.1766 - mse: 32.1766\n",
            "Epoch 373/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.1419 - mse: 32.1419\n",
            "Epoch 374/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.1024 - mse: 32.1024\n",
            "Epoch 375/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.0628 - mse: 32.0628\n",
            "Epoch 376/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.0345 - mse: 32.0345\n",
            "Epoch 377/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 32.0010 - mse: 32.0010\n",
            "Epoch 378/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.9660 - mse: 31.9660\n",
            "Epoch 379/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.9308 - mse: 31.9308\n",
            "Epoch 380/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.8934 - mse: 31.8934\n",
            "Epoch 381/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.8601 - mse: 31.8601\n",
            "Epoch 382/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.8341 - mse: 31.8341\n",
            "Epoch 383/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.8009 - mse: 31.8009\n",
            "Epoch 384/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.7749 - mse: 31.7749\n",
            "Epoch 385/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.7446 - mse: 31.7446\n",
            "Epoch 386/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.7135 - mse: 31.7135\n",
            "Epoch 387/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.6839 - mse: 31.6839\n",
            "Epoch 388/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.6595 - mse: 31.6595\n",
            "Epoch 389/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.6268 - mse: 31.6268\n",
            "Epoch 390/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.5996 - mse: 31.5996\n",
            "Epoch 391/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.5737 - mse: 31.5737\n",
            "Epoch 392/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.5552 - mse: 31.5552\n",
            "Epoch 393/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.5278 - mse: 31.5278\n",
            "Epoch 394/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.5064 - mse: 31.5064\n",
            "Epoch 395/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4830 - mse: 31.4830\n",
            "Epoch 396/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4518 - mse: 31.4518\n",
            "Epoch 397/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4304 - mse: 31.4304\n",
            "Epoch 398/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.4063 - mse: 31.4063\n",
            "Epoch 399/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.3873 - mse: 31.3873\n",
            "Epoch 400/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.3681 - mse: 31.3681\n",
            "Epoch 401/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.3608 - mse: 31.3608\n",
            "Epoch 402/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.3306 - mse: 31.3306\n",
            "Epoch 403/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.3031 - mse: 31.3031\n",
            "Epoch 404/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2833 - mse: 31.2833\n",
            "Epoch 405/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2660 - mse: 31.2660\n",
            "Epoch 406/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2445 - mse: 31.2445\n",
            "Epoch 407/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2251 - mse: 31.2251\n",
            "Epoch 408/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2257 - mse: 31.2257\n",
            "Epoch 409/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.2044 - mse: 31.2044\n",
            "Epoch 410/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.1737 - mse: 31.1737\n",
            "Epoch 411/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.1598 - mse: 31.1598\n",
            "Epoch 412/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.1534 - mse: 31.1534\n",
            "Epoch 413/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.1246 - mse: 31.1246\n",
            "Epoch 414/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.1070 - mse: 31.1070\n",
            "Epoch 415/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0935 - mse: 31.0935\n",
            "Epoch 416/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0813 - mse: 31.0813\n",
            "Epoch 417/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0635 - mse: 31.0635\n",
            "Epoch 418/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0497 - mse: 31.0497\n",
            "Epoch 419/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0360 - mse: 31.0360\n",
            "Epoch 420/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0290 - mse: 31.0290\n",
            "Epoch 421/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 31.0179 - mse: 31.0179\n",
            "Epoch 422/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9922 - mse: 30.9922\n",
            "Epoch 423/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9846 - mse: 30.9846\n",
            "Epoch 424/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9713 - mse: 30.9713\n",
            "Epoch 425/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9582 - mse: 30.9582\n",
            "Epoch 426/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9473 - mse: 30.9473\n",
            "Epoch 427/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9381 - mse: 30.9381\n",
            "Epoch 428/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9223 - mse: 30.9223\n",
            "Epoch 429/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9168 - mse: 30.9168\n",
            "Epoch 430/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.9034 - mse: 30.9034\n",
            "Epoch 431/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8915 - mse: 30.8915\n",
            "Epoch 432/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8864 - mse: 30.8864\n",
            "Epoch 433/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8703 - mse: 30.8703\n",
            "Epoch 434/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8634 - mse: 30.8634\n",
            "Epoch 435/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8521 - mse: 30.8521\n",
            "Epoch 436/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8478 - mse: 30.8478\n",
            "Epoch 437/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8391 - mse: 30.8391\n",
            "Epoch 438/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.8322 - mse: 30.8322\n",
            "Epoch 439/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8270 - mse: 30.8270\n",
            "Epoch 440/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8106 - mse: 30.8106\n",
            "Epoch 441/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.8018 - mse: 30.8018\n",
            "Epoch 442/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7972 - mse: 30.7972\n",
            "Epoch 443/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7916 - mse: 30.7916\n",
            "Epoch 444/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7811 - mse: 30.7811\n",
            "Epoch 445/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7725 - mse: 30.7725\n",
            "Epoch 446/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7692 - mse: 30.7692\n",
            "Epoch 447/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7582 - mse: 30.7582\n",
            "Epoch 448/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7518 - mse: 30.7518\n",
            "Epoch 449/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7450 - mse: 30.7450\n",
            "Epoch 450/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7398 - mse: 30.7398\n",
            "Epoch 451/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7347 - mse: 30.7347\n",
            "Epoch 452/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7296 - mse: 30.7296\n",
            "Epoch 453/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7274 - mse: 30.7274\n",
            "Epoch 454/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.7199 - mse: 30.7199\n",
            "Epoch 455/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7163 - mse: 30.7163\n",
            "Epoch 456/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7135 - mse: 30.7135\n",
            "Epoch 457/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.7033 - mse: 30.7033\n",
            "Epoch 458/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6970 - mse: 30.6970\n",
            "Epoch 459/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6959 - mse: 30.6959\n",
            "Epoch 460/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6894 - mse: 30.6894\n",
            "Epoch 461/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.6784 - mse: 30.6784\n",
            "Epoch 462/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6783 - mse: 30.6783\n",
            "Epoch 463/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6762 - mse: 30.6762\n",
            "Epoch 464/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6964 - mse: 30.6964\n",
            "Epoch 465/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6753 - mse: 30.6753\n",
            "Epoch 466/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6636 - mse: 30.6636\n",
            "Epoch 467/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6575 - mse: 30.6575\n",
            "Epoch 468/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6576 - mse: 30.6576\n",
            "Epoch 469/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6515 - mse: 30.6515\n",
            "Epoch 470/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6485 - mse: 30.6485\n",
            "Epoch 471/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6434 - mse: 30.6434\n",
            "Epoch 472/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6440 - mse: 30.6440\n",
            "Epoch 473/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6433 - mse: 30.6433\n",
            "Epoch 474/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6383 - mse: 30.6383\n",
            "Epoch 475/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6298 - mse: 30.6298\n",
            "Epoch 476/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6261 - mse: 30.6261\n",
            "Epoch 477/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6280 - mse: 30.6280\n",
            "Epoch 478/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6260 - mse: 30.6260\n",
            "Epoch 479/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.6189 - mse: 30.6189\n",
            "Epoch 480/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6181 - mse: 30.6181\n",
            "Epoch 481/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6144 - mse: 30.6144\n",
            "Epoch 482/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6168 - mse: 30.6168\n",
            "Epoch 483/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6135 - mse: 30.6135\n",
            "Epoch 484/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6166 - mse: 30.6166\n",
            "Epoch 485/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6184 - mse: 30.6184\n",
            "Epoch 486/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6047 - mse: 30.6047\n",
            "Epoch 487/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5989 - mse: 30.5989\n",
            "Epoch 488/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.6127 - mse: 30.6127\n",
            "Epoch 489/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5968 - mse: 30.5968\n",
            "Epoch 490/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5992 - mse: 30.5992\n",
            "Epoch 491/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5899 - mse: 30.5899\n",
            "Epoch 492/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5959 - mse: 30.5959\n",
            "Epoch 493/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5894 - mse: 30.5894\n",
            "Epoch 494/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5871 - mse: 30.5871\n",
            "Epoch 495/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5967 - mse: 30.5967\n",
            "Epoch 496/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5869 - mse: 30.5869\n",
            "Epoch 497/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5844 - mse: 30.5844\n",
            "Epoch 498/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5804 - mse: 30.5804\n",
            "Epoch 499/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5806 - mse: 30.5806\n",
            "Epoch 500/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5801 - mse: 30.5801\n",
            "Epoch 501/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5872 - mse: 30.5872\n",
            "Epoch 502/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5807 - mse: 30.5807\n",
            "Epoch 503/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5785 - mse: 30.5785\n",
            "Epoch 504/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5859 - mse: 30.5859\n",
            "Epoch 505/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5805 - mse: 30.5805\n",
            "Epoch 506/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5687 - mse: 30.5687\n",
            "Epoch 507/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5689 - mse: 30.5689\n",
            "Epoch 508/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5677 - mse: 30.5677\n",
            "Epoch 509/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5739 - mse: 30.5739\n",
            "Epoch 510/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5674 - mse: 30.5674\n",
            "Epoch 511/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5668 - mse: 30.5668\n",
            "Epoch 512/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5664 - mse: 30.5664\n",
            "Epoch 513/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5702 - mse: 30.5702\n",
            "Epoch 514/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5643 - mse: 30.5643\n",
            "Epoch 515/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5619 - mse: 30.5619\n",
            "Epoch 516/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5576 - mse: 30.5576\n",
            "Epoch 517/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5581 - mse: 30.5581\n",
            "Epoch 518/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5619 - mse: 30.5619\n",
            "Epoch 519/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5611 - mse: 30.5611\n",
            "Epoch 520/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5646 - mse: 30.5646\n",
            "Epoch 521/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5618 - mse: 30.5618\n",
            "Epoch 522/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5545 - mse: 30.5545\n",
            "Epoch 523/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5579 - mse: 30.5579\n",
            "Epoch 524/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5553 - mse: 30.5553\n",
            "Epoch 525/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5520 - mse: 30.5520\n",
            "Epoch 526/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5574 - mse: 30.5574\n",
            "Epoch 527/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5605 - mse: 30.5605\n",
            "Epoch 528/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5579 - mse: 30.5579\n",
            "Epoch 529/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5522 - mse: 30.5522\n",
            "Epoch 530/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5540 - mse: 30.5540\n",
            "Epoch 531/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5511 - mse: 30.5511\n",
            "Epoch 532/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5509 - mse: 30.5509\n",
            "Epoch 533/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5591 - mse: 30.5591\n",
            "Epoch 534/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5463 - mse: 30.5463\n",
            "Epoch 535/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5520 - mse: 30.5520\n",
            "Epoch 536/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5483 - mse: 30.5483\n",
            "Epoch 537/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5552 - mse: 30.5552\n",
            "Epoch 538/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5493 - mse: 30.5493\n",
            "Epoch 539/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5461 - mse: 30.5461\n",
            "Epoch 540/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5539 - mse: 30.5539\n",
            "Epoch 541/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5639 - mse: 30.5639\n",
            "Epoch 542/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5457 - mse: 30.5457\n",
            "Epoch 543/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5454 - mse: 30.5454\n",
            "Epoch 544/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5440 - mse: 30.5440\n",
            "Epoch 545/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5462 - mse: 30.5462\n",
            "Epoch 546/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5531 - mse: 30.5531\n",
            "Epoch 547/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5426 - mse: 30.5426\n",
            "Epoch 548/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5399 - mse: 30.5399\n",
            "Epoch 549/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5443 - mse: 30.5443\n",
            "Epoch 550/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5454 - mse: 30.5454\n",
            "Epoch 551/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5420 - mse: 30.5420\n",
            "Epoch 552/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5428 - mse: 30.5428\n",
            "Epoch 553/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5392 - mse: 30.5392\n",
            "Epoch 554/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5573 - mse: 30.5573\n",
            "Epoch 555/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5555 - mse: 30.5555\n",
            "Epoch 556/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5411 - mse: 30.5411\n",
            "Epoch 557/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5423 - mse: 30.5423\n",
            "Epoch 558/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5384 - mse: 30.5384\n",
            "Epoch 559/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5388 - mse: 30.5388\n",
            "Epoch 560/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5559 - mse: 30.5559\n",
            "Epoch 561/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5385 - mse: 30.5385\n",
            "Epoch 562/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5409 - mse: 30.5409\n",
            "Epoch 563/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5431 - mse: 30.5431\n",
            "Epoch 564/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5464 - mse: 30.5464\n",
            "Epoch 565/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5404 - mse: 30.5404\n",
            "Epoch 566/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5395 - mse: 30.5395\n",
            "Epoch 567/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5485 - mse: 30.5485\n",
            "Epoch 568/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5469 - mse: 30.5469\n",
            "Epoch 569/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5368 - mse: 30.5368\n",
            "Epoch 570/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5414 - mse: 30.5414\n",
            "Epoch 571/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5506 - mse: 30.5506\n",
            "Epoch 572/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5361 - mse: 30.5361\n",
            "Epoch 573/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5805 - mse: 30.5805\n",
            "Epoch 574/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5365 - mse: 30.5365\n",
            "Epoch 575/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5460 - mse: 30.5460\n",
            "Epoch 576/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5398 - mse: 30.5398\n",
            "Epoch 577/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\n",
            "Epoch 578/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5343 - mse: 30.5343\n",
            "Epoch 579/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5370 - mse: 30.5370\n",
            "Epoch 580/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5386 - mse: 30.5386\n",
            "Epoch 581/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5451 - mse: 30.5451\n",
            "Epoch 582/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5413 - mse: 30.5413\n",
            "Epoch 583/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5376 - mse: 30.5376\n",
            "Epoch 584/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5418 - mse: 30.5418\n",
            "Epoch 585/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5367 - mse: 30.5367\n",
            "Epoch 586/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5338 - mse: 30.5338\n",
            "Epoch 587/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5372 - mse: 30.5372\n",
            "Epoch 588/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5379 - mse: 30.5379\n",
            "Epoch 589/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5356 - mse: 30.5356\n",
            "Epoch 590/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5324 - mse: 30.5324\n",
            "Epoch 591/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5364 - mse: 30.5364\n",
            "Epoch 592/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5364 - mse: 30.5364\n",
            "Epoch 593/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5437 - mse: 30.5437\n",
            "Epoch 594/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5437 - mse: 30.5437\n",
            "Epoch 595/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5392 - mse: 30.5392\n",
            "Epoch 596/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5363 - mse: 30.5363\n",
            "Epoch 597/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5356 - mse: 30.5356\n",
            "Epoch 598/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5326 - mse: 30.5326\n",
            "Epoch 599/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5303 - mse: 30.5303\n",
            "Epoch 600/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5365 - mse: 30.5365\n",
            "Epoch 601/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5351 - mse: 30.5351\n",
            "Epoch 602/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5334 - mse: 30.5334\n",
            "Epoch 603/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5321 - mse: 30.5321\n",
            "Epoch 604/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5399 - mse: 30.5399\n",
            "Epoch 605/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5357 - mse: 30.5357\n",
            "Epoch 606/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5332 - mse: 30.5332\n",
            "Epoch 607/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5381 - mse: 30.5381\n",
            "Epoch 608/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5343 - mse: 30.5343\n",
            "Epoch 609/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5372 - mse: 30.5372\n",
            "Epoch 610/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5322 - mse: 30.5322\n",
            "Epoch 611/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5326 - mse: 30.5326\n",
            "Epoch 612/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\n",
            "Epoch 613/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5383 - mse: 30.5383\n",
            "Epoch 614/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5326 - mse: 30.5326\n",
            "Epoch 615/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5308 - mse: 30.5308\n",
            "Epoch 616/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5317 - mse: 30.5317\n",
            "Epoch 617/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5329 - mse: 30.5329\n",
            "Epoch 618/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\n",
            "Epoch 619/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5367 - mse: 30.5367\n",
            "Epoch 620/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\n",
            "Epoch 621/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 622/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5375 - mse: 30.5375\n",
            "Epoch 623/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5387 - mse: 30.5387\n",
            "Epoch 624/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5341 - mse: 30.5341\n",
            "Epoch 625/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5348 - mse: 30.5348\n",
            "Epoch 626/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 627/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5335 - mse: 30.5335\n",
            "Epoch 628/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5337 - mse: 30.5337\n",
            "Epoch 629/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\n",
            "Epoch 630/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5315 - mse: 30.5315\n",
            "Epoch 631/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5383 - mse: 30.5383\n",
            "Epoch 632/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5465 - mse: 30.5465\n",
            "Epoch 633/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5311 - mse: 30.5311\n",
            "Epoch 634/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\n",
            "Epoch 635/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5312 - mse: 30.5312\n",
            "Epoch 636/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5337 - mse: 30.5337\n",
            "Epoch 637/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5332 - mse: 30.5332\n",
            "Epoch 638/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 639/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5413 - mse: 30.5413\n",
            "Epoch 640/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5330 - mse: 30.5330\n",
            "Epoch 641/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5299 - mse: 30.5299\n",
            "Epoch 642/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\n",
            "Epoch 643/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\n",
            "Epoch 644/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\n",
            "Epoch 645/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5369 - mse: 30.5369\n",
            "Epoch 646/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5361 - mse: 30.5361\n",
            "Epoch 647/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5334 - mse: 30.5334\n",
            "Epoch 648/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5314 - mse: 30.5314\n",
            "Epoch 649/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5365 - mse: 30.5365\n",
            "Epoch 650/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5396 - mse: 30.5396\n",
            "Epoch 651/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 652/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5278 - mse: 30.5278\n",
            "Epoch 653/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5361 - mse: 30.5361\n",
            "Epoch 654/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5300 - mse: 30.5300\n",
            "Epoch 655/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5341 - mse: 30.5341\n",
            "Epoch 656/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5354 - mse: 30.5354\n",
            "Epoch 657/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\n",
            "Epoch 658/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 659/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5279 - mse: 30.5279\n",
            "Epoch 660/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5295 - mse: 30.5295\n",
            "Epoch 661/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5343 - mse: 30.5343\n",
            "Epoch 662/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5348 - mse: 30.5348\n",
            "Epoch 663/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5335 - mse: 30.5335\n",
            "Epoch 664/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5319 - mse: 30.5319\n",
            "Epoch 665/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 666/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5404 - mse: 30.5404\n",
            "Epoch 667/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5313 - mse: 30.5313\n",
            "Epoch 668/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5346 - mse: 30.5346\n",
            "Epoch 669/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5350 - mse: 30.5350\n",
            "Epoch 670/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5306 - mse: 30.5306\n",
            "Epoch 671/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5304 - mse: 30.5304\n",
            "Epoch 672/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 673/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5374 - mse: 30.5374\n",
            "Epoch 674/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5279 - mse: 30.5279\n",
            "Epoch 675/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5389 - mse: 30.5389\n",
            "Epoch 676/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\n",
            "Epoch 677/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5282 - mse: 30.5282\n",
            "Epoch 678/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5296 - mse: 30.5296\n",
            "Epoch 679/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5268 - mse: 30.5268\n",
            "Epoch 680/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 681/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5266 - mse: 30.5266\n",
            "Epoch 682/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5316 - mse: 30.5316\n",
            "Epoch 683/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5289 - mse: 30.5289\n",
            "Epoch 684/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5274 - mse: 30.5274\n",
            "Epoch 685/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 686/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5333 - mse: 30.5333\n",
            "Epoch 687/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5281 - mse: 30.5281\n",
            "Epoch 688/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5385 - mse: 30.5385\n",
            "Epoch 689/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5334 - mse: 30.5334\n",
            "Epoch 690/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5256 - mse: 30.5256\n",
            "Epoch 691/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 30.5327 - mse: 30.5327\n",
            "Epoch 692/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5300 - mse: 30.5300\n",
            "Epoch 693/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5283 - mse: 30.5283\n",
            "Epoch 694/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\n",
            "Epoch 695/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5316 - mse: 30.5316\n",
            "Epoch 696/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5366 - mse: 30.5366\n",
            "Epoch 697/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5311 - mse: 30.5311\n",
            "Epoch 698/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5288 - mse: 30.5288\n",
            "Epoch 699/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\n",
            "Epoch 700/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5406 - mse: 30.5406\n",
            "Epoch 701/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5290 - mse: 30.5290\n",
            "Epoch 702/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5450 - mse: 30.5450\n",
            "Epoch 703/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5321 - mse: 30.5321\n",
            "Epoch 704/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5342 - mse: 30.5342\n",
            "Epoch 705/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5261 - mse: 30.5261\n",
            "Epoch 706/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5314 - mse: 30.5314\n",
            "Epoch 707/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 30.5354 - mse: 30.5354\n",
            "Epoch 708/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 709/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5298 - mse: 30.5298\n",
            "Epoch 710/1000\n",
            "16/16 [==============================] - 0s 7ms/step - loss: 30.5319 - mse: 30.5319\n",
            "Epoch 711/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5329 - mse: 30.5329\n",
            "Epoch 712/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5300 - mse: 30.5300\n",
            "Epoch 713/1000\n",
            "16/16 [==============================] - 0s 9ms/step - loss: 30.5311 - mse: 30.5311\n",
            "Epoch 714/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 715/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5310 - mse: 30.5310\n",
            "Epoch 716/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5391 - mse: 30.5391\n",
            "Epoch 717/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5286 - mse: 30.5286\n",
            "Epoch 718/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5294 - mse: 30.5294\n",
            "Epoch 719/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5390 - mse: 30.5390\n",
            "Epoch 720/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 721/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5379 - mse: 30.5379\n",
            "Epoch 722/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5279 - mse: 30.5279\n",
            "Epoch 723/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5315 - mse: 30.5315\n",
            "Epoch 724/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 725/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5273 - mse: 30.5273\n",
            "Epoch 726/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5262 - mse: 30.5262\n",
            "Epoch 727/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5269 - mse: 30.5269\n",
            "Epoch 728/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5410 - mse: 30.5410\n",
            "Epoch 729/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5465 - mse: 30.5465\n",
            "Epoch 730/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5445 - mse: 30.5445\n",
            "Epoch 731/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5337 - mse: 30.5337\n",
            "Epoch 732/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5290 - mse: 30.5290\n",
            "Epoch 733/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5314 - mse: 30.5314\n",
            "Epoch 734/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5384 - mse: 30.5384\n",
            "Epoch 735/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\n",
            "Epoch 736/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\n",
            "Epoch 737/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5420 - mse: 30.5420\n",
            "Epoch 738/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5276 - mse: 30.5276\n",
            "Epoch 739/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5282 - mse: 30.5282\n",
            "Epoch 740/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5369 - mse: 30.5369\n",
            "Epoch 741/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 742/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5349 - mse: 30.5349\n",
            "Epoch 743/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5275 - mse: 30.5275\n",
            "Epoch 744/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 745/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\n",
            "Epoch 746/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5310 - mse: 30.5310\n",
            "Epoch 747/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5280 - mse: 30.5280\n",
            "Epoch 748/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5323 - mse: 30.5323\n",
            "Epoch 749/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 750/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5299 - mse: 30.5299\n",
            "Epoch 751/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5261 - mse: 30.5261\n",
            "Epoch 752/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\n",
            "Epoch 753/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5368 - mse: 30.5368\n",
            "Epoch 754/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5419 - mse: 30.5419\n",
            "Epoch 755/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 756/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5302 - mse: 30.5302\n",
            "Epoch 757/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5411 - mse: 30.5411\n",
            "Epoch 758/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5267 - mse: 30.5267\n",
            "Epoch 759/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 760/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 761/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5294 - mse: 30.5294\n",
            "Epoch 762/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5303 - mse: 30.5303\n",
            "Epoch 763/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5234 - mse: 30.5234\n",
            "Epoch 764/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5271 - mse: 30.5271\n",
            "Epoch 765/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5296 - mse: 30.5296\n",
            "Epoch 766/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5440 - mse: 30.5440\n",
            "Epoch 767/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5302 - mse: 30.5302\n",
            "Epoch 768/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5282 - mse: 30.5282\n",
            "Epoch 769/1000\n",
            "16/16 [==============================] - 0s 6ms/step - loss: 30.5256 - mse: 30.5256\n",
            "Epoch 770/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5247 - mse: 30.5247\n",
            "Epoch 771/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5333 - mse: 30.5333\n",
            "Epoch 772/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5258 - mse: 30.5258\n",
            "Epoch 773/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5315 - mse: 30.5315\n",
            "Epoch 774/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5349 - mse: 30.5349\n",
            "Epoch 775/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5251 - mse: 30.5251\n",
            "Epoch 776/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 777/1000\n",
            "16/16 [==============================] - 0s 4ms/step - loss: 30.5259 - mse: 30.5259\n",
            "Epoch 778/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 779/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5249 - mse: 30.5249\n",
            "Epoch 780/1000\n",
            "16/16 [==============================] - 0s 5ms/step - loss: 30.5246 - mse: 30.5246\n",
            "Epoch 781/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 782/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5309 - mse: 30.5309\n",
            "Epoch 783/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5279 - mse: 30.5279\n",
            "Epoch 784/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\n",
            "Epoch 785/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5286 - mse: 30.5286\n",
            "Epoch 786/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5265 - mse: 30.5265\n",
            "Epoch 787/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5260 - mse: 30.5260\n",
            "Epoch 788/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\n",
            "Epoch 789/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5231 - mse: 30.5231\n",
            "Epoch 790/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 791/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5253 - mse: 30.5253\n",
            "Epoch 792/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5248 - mse: 30.5248\n",
            "Epoch 793/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5241 - mse: 30.5241\n",
            "Epoch 794/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5239 - mse: 30.5239\n",
            "Epoch 795/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5240 - mse: 30.5240\n",
            "Epoch 796/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 797/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5440 - mse: 30.5440\n",
            "Epoch 798/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\n",
            "Epoch 799/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 800/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5324 - mse: 30.5324\n",
            "Epoch 801/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\n",
            "Epoch 802/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5237 - mse: 30.5237\n",
            "Epoch 803/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5402 - mse: 30.5402\n",
            "Epoch 804/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5251 - mse: 30.5251\n",
            "Epoch 805/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5260 - mse: 30.5260\n",
            "Epoch 806/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 807/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5297 - mse: 30.5297\n",
            "Epoch 808/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5341 - mse: 30.5341\n",
            "Epoch 809/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5333 - mse: 30.5333\n",
            "Epoch 810/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5277 - mse: 30.5277\n",
            "Epoch 811/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5289 - mse: 30.5289\n",
            "Epoch 812/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 813/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5327 - mse: 30.5327\n",
            "Epoch 814/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5217 - mse: 30.5217\n",
            "Epoch 815/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5267 - mse: 30.5267\n",
            "Epoch 816/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5263 - mse: 30.5263\n",
            "Epoch 817/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5346 - mse: 30.5346\n",
            "Epoch 818/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\n",
            "Epoch 819/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5307 - mse: 30.5307\n",
            "Epoch 820/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5275 - mse: 30.5275\n",
            "Epoch 821/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5280 - mse: 30.5280\n",
            "Epoch 822/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5232 - mse: 30.5232\n",
            "Epoch 823/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5351 - mse: 30.5351\n",
            "Epoch 824/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5234 - mse: 30.5234\n",
            "Epoch 825/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5268 - mse: 30.5268\n",
            "Epoch 826/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5230 - mse: 30.5230\n",
            "Epoch 827/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5220 - mse: 30.5220\n",
            "Epoch 828/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5217 - mse: 30.5217\n",
            "Epoch 829/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 830/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 831/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\n",
            "Epoch 832/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5276 - mse: 30.5276\n",
            "Epoch 833/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5232 - mse: 30.5232\n",
            "Epoch 834/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5240 - mse: 30.5240\n",
            "Epoch 835/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5265 - mse: 30.5265\n",
            "Epoch 836/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5235 - mse: 30.5235\n",
            "Epoch 837/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5454 - mse: 30.5454\n",
            "Epoch 838/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5319 - mse: 30.5319\n",
            "Epoch 839/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5316 - mse: 30.5316\n",
            "Epoch 840/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5263 - mse: 30.5263\n",
            "Epoch 841/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5221 - mse: 30.5221\n",
            "Epoch 842/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5438 - mse: 30.5438\n",
            "Epoch 843/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5206 - mse: 30.5206\n",
            "Epoch 844/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5311 - mse: 30.5311\n",
            "Epoch 845/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5294 - mse: 30.5294\n",
            "Epoch 846/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5228 - mse: 30.5228\n",
            "Epoch 847/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5255 - mse: 30.5255\n",
            "Epoch 848/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5279 - mse: 30.5279\n",
            "Epoch 849/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5237 - mse: 30.5237\n",
            "Epoch 850/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5234 - mse: 30.5234\n",
            "Epoch 851/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5255 - mse: 30.5255\n",
            "Epoch 852/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5224 - mse: 30.5224\n",
            "Epoch 853/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5263 - mse: 30.5263\n",
            "Epoch 854/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 855/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5261 - mse: 30.5261\n",
            "Epoch 856/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5256 - mse: 30.5256\n",
            "Epoch 857/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5259 - mse: 30.5259\n",
            "Epoch 858/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5240 - mse: 30.5240\n",
            "Epoch 859/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5225 - mse: 30.5225\n",
            "Epoch 860/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5254 - mse: 30.5254\n",
            "Epoch 861/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5258 - mse: 30.5258\n",
            "Epoch 862/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5240 - mse: 30.5240\n",
            "Epoch 863/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5275 - mse: 30.5275\n",
            "Epoch 864/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5260 - mse: 30.5260\n",
            "Epoch 865/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5275 - mse: 30.5275\n",
            "Epoch 866/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5257 - mse: 30.5257\n",
            "Epoch 867/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5238 - mse: 30.5238\n",
            "Epoch 868/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5223 - mse: 30.5223\n",
            "Epoch 869/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5286 - mse: 30.5286\n",
            "Epoch 870/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5212 - mse: 30.5212\n",
            "Epoch 871/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5215 - mse: 30.5215\n",
            "Epoch 872/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5229 - mse: 30.5229\n",
            "Epoch 873/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5284 - mse: 30.5284\n",
            "Epoch 874/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5232 - mse: 30.5232\n",
            "Epoch 875/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5225 - mse: 30.5225\n",
            "Epoch 876/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\n",
            "Epoch 877/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5216 - mse: 30.5216\n",
            "Epoch 878/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5283 - mse: 30.5283\n",
            "Epoch 879/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5207 - mse: 30.5207\n",
            "Epoch 880/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5250 - mse: 30.5250\n",
            "Epoch 881/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 882/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5368 - mse: 30.5368\n",
            "Epoch 883/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5201 - mse: 30.5201\n",
            "Epoch 884/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5262 - mse: 30.5262\n",
            "Epoch 885/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5226 - mse: 30.5226\n",
            "Epoch 886/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5228 - mse: 30.5228\n",
            "Epoch 887/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5223 - mse: 30.5223\n",
            "Epoch 888/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 889/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\n",
            "Epoch 890/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5308 - mse: 30.5308\n",
            "Epoch 891/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 892/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5254 - mse: 30.5254\n",
            "Epoch 893/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 894/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5365 - mse: 30.5365\n",
            "Epoch 895/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5194 - mse: 30.5194\n",
            "Epoch 896/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5220 - mse: 30.5220\n",
            "Epoch 897/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5246 - mse: 30.5246\n",
            "Epoch 898/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\n",
            "Epoch 899/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5224 - mse: 30.5224\n",
            "Epoch 900/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5218 - mse: 30.5218\n",
            "Epoch 901/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5290 - mse: 30.5290\n",
            "Epoch 902/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5292 - mse: 30.5292\n",
            "Epoch 903/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5305 - mse: 30.5305\n",
            "Epoch 904/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5269 - mse: 30.5269\n",
            "Epoch 905/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5245 - mse: 30.5245\n",
            "Epoch 906/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5198 - mse: 30.5198\n",
            "Epoch 907/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5264 - mse: 30.5264\n",
            "Epoch 908/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5229 - mse: 30.5229\n",
            "Epoch 909/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5218 - mse: 30.5218\n",
            "Epoch 910/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 911/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 912/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5233 - mse: 30.5233\n",
            "Epoch 913/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5198 - mse: 30.5198\n",
            "Epoch 914/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5236 - mse: 30.5236\n",
            "Epoch 915/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5232 - mse: 30.5232\n",
            "Epoch 916/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5317 - mse: 30.5317\n",
            "Epoch 917/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5258 - mse: 30.5258\n",
            "Epoch 918/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5250 - mse: 30.5250\n",
            "Epoch 919/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5310 - mse: 30.5310\n",
            "Epoch 920/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5228 - mse: 30.5228\n",
            "Epoch 921/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\n",
            "Epoch 922/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5193 - mse: 30.5193\n",
            "Epoch 923/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5285 - mse: 30.5285\n",
            "Epoch 924/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\n",
            "Epoch 925/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 926/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5246 - mse: 30.5246\n",
            "Epoch 927/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5203 - mse: 30.5203\n",
            "Epoch 928/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5320 - mse: 30.5320\n",
            "Epoch 929/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5215 - mse: 30.5215\n",
            "Epoch 930/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5214 - mse: 30.5214\n",
            "Epoch 931/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5203 - mse: 30.5203\n",
            "Epoch 932/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5202 - mse: 30.5202\n",
            "Epoch 933/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 934/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5259 - mse: 30.5259\n",
            "Epoch 935/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 936/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5299 - mse: 30.5299\n",
            "Epoch 937/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5266 - mse: 30.5266\n",
            "Epoch 938/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\n",
            "Epoch 939/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5242 - mse: 30.5242\n",
            "Epoch 940/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5226 - mse: 30.5226\n",
            "Epoch 941/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5243 - mse: 30.5243\n",
            "Epoch 942/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5225 - mse: 30.5225\n",
            "Epoch 943/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\n",
            "Epoch 944/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5224 - mse: 30.5224\n",
            "Epoch 945/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5249 - mse: 30.5249\n",
            "Epoch 946/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5207 - mse: 30.5207\n",
            "Epoch 947/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5207 - mse: 30.5207\n",
            "Epoch 948/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5225 - mse: 30.5225\n",
            "Epoch 949/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5273 - mse: 30.5273\n",
            "Epoch 950/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\n",
            "Epoch 951/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\n",
            "Epoch 952/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5247 - mse: 30.5247\n",
            "Epoch 953/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5206 - mse: 30.5206\n",
            "Epoch 954/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5206 - mse: 30.5206\n",
            "Epoch 955/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 956/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5291 - mse: 30.5291\n",
            "Epoch 957/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5204 - mse: 30.5204\n",
            "Epoch 958/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5199 - mse: 30.5199\n",
            "Epoch 959/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5387 - mse: 30.5387\n",
            "Epoch 960/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5255 - mse: 30.5255\n",
            "Epoch 961/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5258 - mse: 30.5258\n",
            "Epoch 962/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5189 - mse: 30.5189\n",
            "Epoch 963/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5360 - mse: 30.5360\n",
            "Epoch 964/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5457 - mse: 30.5457\n",
            "Epoch 965/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5289 - mse: 30.5289\n",
            "Epoch 966/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5194 - mse: 30.5194\n",
            "Epoch 967/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\n",
            "Epoch 968/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5212 - mse: 30.5212\n",
            "Epoch 969/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5351 - mse: 30.5351\n",
            "Epoch 970/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5298 - mse: 30.5298\n",
            "Epoch 971/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5239 - mse: 30.5239\n",
            "Epoch 972/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5209 - mse: 30.5209\n",
            "Epoch 973/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5214 - mse: 30.5214\n",
            "Epoch 974/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5328 - mse: 30.5328\n",
            "Epoch 975/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5217 - mse: 30.5217\n",
            "Epoch 976/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5247 - mse: 30.5247\n",
            "Epoch 977/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5269 - mse: 30.5269\n",
            "Epoch 978/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5252 - mse: 30.5252\n",
            "Epoch 979/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5205 - mse: 30.5205\n",
            "Epoch 980/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5238 - mse: 30.5238\n",
            "Epoch 981/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5275 - mse: 30.5275\n",
            "Epoch 982/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5219 - mse: 30.5219\n",
            "Epoch 983/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5244 - mse: 30.5244\n",
            "Epoch 984/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5211 - mse: 30.5211\n",
            "Epoch 985/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5209 - mse: 30.5209\n",
            "Epoch 986/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5293 - mse: 30.5293\n",
            "Epoch 987/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5295 - mse: 30.5295\n",
            "Epoch 988/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5271 - mse: 30.5271\n",
            "Epoch 989/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5202 - mse: 30.5202\n",
            "Epoch 990/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5253 - mse: 30.5253\n",
            "Epoch 991/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5240 - mse: 30.5240\n",
            "Epoch 992/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5193 - mse: 30.5193\n",
            "Epoch 993/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5246 - mse: 30.5246\n",
            "Epoch 994/1000\n",
            "16/16 [==============================] - 0s 3ms/step - loss: 30.5272 - mse: 30.5272\n",
            "Epoch 995/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5204 - mse: 30.5204\n",
            "Epoch 996/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5251 - mse: 30.5251\n",
            "Epoch 997/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5270 - mse: 30.5270\n",
            "Epoch 998/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5222 - mse: 30.5222\n",
            "Epoch 999/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5238 - mse: 30.5238\n",
            "Epoch 1000/1000\n",
            "16/16 [==============================] - 0s 2ms/step - loss: 30.5228 - mse: 30.5228\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7969703078b0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras로 학습된 모델을 이용하여 주택 가격 예측 수행."
      ],
      "metadata": {
        "id": "ntWPfMLdV3T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(scaled_features)\n",
        "bostonDF['KERAS_PREDICTED_PRICE'] = predicted\n",
        "bostonDF.head(10)\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "zpVmGlMRV3T4",
        "outputId": "2e9c106d-a490-41a0-d5ef-aa02fc3aaa27"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
              "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
              "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
              "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
              "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
              "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
              "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
              "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
              "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
              "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
              "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
              "\n",
              "   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  KERAS_PREDICTED_PRICE  \n",
              "0     15.3  396.90   4.98   24.0        28.935533              28.973110  \n",
              "1     17.8  396.90   9.14   21.6        25.483093              25.497845  \n",
              "2     17.8  392.83   4.03   34.7        32.545474              32.630135  \n",
              "3     18.7  394.63   2.94   33.4        32.334142              32.408226  \n",
              "4     18.7  396.90   5.33   36.2        31.516284              31.594551  \n",
              "5     18.7  394.12   5.21   28.7        28.074722              28.101097  \n",
              "6     15.2  395.60  12.43   22.9        21.342942              21.318724  \n",
              "7     15.2  396.90  19.15   27.1        17.772340              17.741028  \n",
              "8     15.2  386.63  29.93   16.5         8.129206               8.028409  \n",
              "9     15.2  386.71  17.10   18.9        18.276548              18.238716  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bbe17f9d-2c77-4b34-bc6c-b1a7be3f45b2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CRIM</th>\n",
              "      <th>ZN</th>\n",
              "      <th>INDUS</th>\n",
              "      <th>CHAS</th>\n",
              "      <th>NOX</th>\n",
              "      <th>RM</th>\n",
              "      <th>AGE</th>\n",
              "      <th>DIS</th>\n",
              "      <th>RAD</th>\n",
              "      <th>TAX</th>\n",
              "      <th>PTRATIO</th>\n",
              "      <th>B</th>\n",
              "      <th>LSTAT</th>\n",
              "      <th>PRICE</th>\n",
              "      <th>PREDICTED_PRICE</th>\n",
              "      <th>KERAS_PREDICTED_PRICE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>2.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.538</td>\n",
              "      <td>6.575</td>\n",
              "      <td>65.2</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>24.0</td>\n",
              "      <td>28.935533</td>\n",
              "      <td>28.973110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>6.421</td>\n",
              "      <td>78.9</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>21.6</td>\n",
              "      <td>25.483093</td>\n",
              "      <td>25.497845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.469</td>\n",
              "      <td>7.185</td>\n",
              "      <td>61.1</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>2.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>34.7</td>\n",
              "      <td>32.545474</td>\n",
              "      <td>32.630135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.998</td>\n",
              "      <td>45.8</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>33.4</td>\n",
              "      <td>32.334142</td>\n",
              "      <td>32.408226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>7.147</td>\n",
              "      <td>54.2</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>36.2</td>\n",
              "      <td>31.516284</td>\n",
              "      <td>31.594551</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.02985</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.18</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458</td>\n",
              "      <td>6.430</td>\n",
              "      <td>58.7</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>3.0</td>\n",
              "      <td>222.0</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.12</td>\n",
              "      <td>5.21</td>\n",
              "      <td>28.7</td>\n",
              "      <td>28.074722</td>\n",
              "      <td>28.101097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.08829</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.012</td>\n",
              "      <td>66.6</td>\n",
              "      <td>5.5605</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>395.60</td>\n",
              "      <td>12.43</td>\n",
              "      <td>22.9</td>\n",
              "      <td>21.342942</td>\n",
              "      <td>21.318724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.14455</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.172</td>\n",
              "      <td>96.1</td>\n",
              "      <td>5.9505</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>396.90</td>\n",
              "      <td>19.15</td>\n",
              "      <td>27.1</td>\n",
              "      <td>17.772340</td>\n",
              "      <td>17.741028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.21124</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>5.631</td>\n",
              "      <td>100.0</td>\n",
              "      <td>6.0821</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.63</td>\n",
              "      <td>29.93</td>\n",
              "      <td>16.5</td>\n",
              "      <td>8.129206</td>\n",
              "      <td>8.028409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.17004</td>\n",
              "      <td>12.5</td>\n",
              "      <td>7.87</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.524</td>\n",
              "      <td>6.004</td>\n",
              "      <td>85.9</td>\n",
              "      <td>6.5921</td>\n",
              "      <td>5.0</td>\n",
              "      <td>311.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>386.71</td>\n",
              "      <td>17.10</td>\n",
              "      <td>18.9</td>\n",
              "      <td>18.276548</td>\n",
              "      <td>18.238716</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbe17f9d-2c77-4b34-bc6c-b1a7be3f45b2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bbe17f9d-2c77-4b34-bc6c-b1a7be3f45b2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bbe17f9d-2c77-4b34-bc6c-b1a7be3f45b2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a28415a-8788-4c91-b083-8bfc8d224035\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a28415a-8788-4c91-b083-8bfc8d224035')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a28415a-8788-4c91-b083-8bfc8d224035 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "QdloVJIkV3T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n",
        "* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n",
        "* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용"
      ],
      "metadata": {
        "id": "xlLl15FcV3T4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "bostonDF['PRICE'] = boston.target\n",
        "print(bostonDF.shape)\n",
        "bostonDF.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Z7HWV8jCV3T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD 기반으로 Weight/Bias update 값 구하기"
      ],
      "metadata": {
        "id": "J6p4gWqxV3T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n",
        "\n",
        "    # 데이터 건수\n",
        "    N = target_sgd.shape[0]\n",
        "    # 예측 값.\n",
        "    predicted_sgd = w1 * rm_sgd + w2*lstat_sgd + bias\n",
        "    # 실제값과 예측값의 차이\n",
        "    diff_sgd = target_sgd - predicted_sgd\n",
        "    # bias 를 array 기반으로 구하기 위해서 설정.\n",
        "    bias_factors = np.ones((N,))\n",
        "\n",
        "    # weight와 bias를 얼마나 update할 것인지를 계산.\n",
        "    w1_update = -(2/N)*learning_rate*(np.dot(rm_sgd.T, diff_sgd))\n",
        "    w2_update = -(2/N)*learning_rate*(np.dot(lstat_sgd.T, diff_sgd))\n",
        "    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_sgd))\n",
        "\n",
        "    # Mean Squared Error값을 계산.\n",
        "    #mse_loss = np.mean(np.square(diff))\n",
        "\n",
        "    # weight와 bias가 update되어야 할 값 반환\n",
        "    return bias_update, w1_update, w2_update"
      ],
      "metadata": {
        "trusted": true,
        "id": "HD6LdizLV3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SGD 수행하기"
      ],
      "metadata": {
        "id": "p3rmzPI7V3T5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(bostonDF['PRICE'].values.shape)\n",
        "print(np.random.choice(bostonDF['PRICE'].values.shape[0], 1))\n",
        "print(np.random.choice(506, 1))"
      ],
      "metadata": {
        "trusted": true,
        "id": "18zC0uqqV3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RM, LSTAT feature array와 PRICE target array를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용.\n",
        "def st_gradient_descent(features, target, iter_epochs=1000, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정.\n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "\n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "\n",
        "\n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행.\n",
        "    for i in range(iter_epochs):\n",
        "        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. 추출할 데이터의 인덱스를 random.choice() 로 선택.\n",
        "        stochastic_index = np.random.choice(target.shape[0], 1)\n",
        "        rm_sgd = rm[stochastic_index]\n",
        "        lstat_sgd = lstat[stochastic_index]\n",
        "        target_sgd = target[stochastic_index]\n",
        "        # SGD 기반으로 Weight/Bias의 Update를 구함.\n",
        "        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate)\n",
        "\n",
        "        # SGD로 구한 weight/bias의 update 적용.\n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if verbose:\n",
        "            print('Epoch:', i+1,'/', iter_epochs)\n",
        "            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n",
        "            predicted = w1 * rm + w2*lstat + bias\n",
        "            diff = target - predicted\n",
        "            mse_loss = np.mean(np.square(diff))\n",
        "            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "trusted": true,
        "id": "GYenHJK8V3T5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(bostonDF[['RM', 'LSTAT']])\n",
        "\n",
        "w1, w2, bias = st_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "trusted": true,
        "id": "xbAtgWFpV3T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_SGD'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HiqlJgvKV3T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행"
      ],
      "metadata": {
        "id": "kHj2UDPzV3T6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n",
        "\n",
        "    # 데이터 건수\n",
        "    N = target_batch.shape[0]\n",
        "    # 예측 값.\n",
        "    predicted_batch = w1 * rm_batch+ w2 * lstat_batch + bias\n",
        "    # 실제값과 예측값의 차이\n",
        "    diff_batch = target_batch - predicted_batch\n",
        "    # bias 를 array 기반으로 구하기 위해서 설정.\n",
        "    bias_factors = np.ones((N,))\n",
        "\n",
        "    # weight와 bias를 얼마나 update할 것인지를 계산.\n",
        "    w1_update = -(2/N)*learning_rate*(np.dot(rm_batch.T, diff_batch))\n",
        "    w2_update = -(2/N)*learning_rate*(np.dot(lstat_batch.T, diff_batch))\n",
        "    bias_update = -(2/N)*learning_rate*(np.dot(bias_factors.T, diff_batch))\n",
        "\n",
        "    # Mean Squared Error값을 계산.\n",
        "    #mse_loss = np.mean(np.square(diff))\n",
        "\n",
        "    # weight와 bias가 update되어야 할 값 반환\n",
        "    return bias_update, w1_update, w2_update"
      ],
      "metadata": {
        "trusted": true,
        "id": "KPACA3qlV3T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_indexes = np.random.choice(506, 30)\n",
        "print(batch_indexes)\n",
        "\n",
        "bostonDF['RM'].values[batch_indexes]\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "xzez1B2EV3T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음.\n",
        "def batch_random_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정.\n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "\n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "\n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행.\n",
        "    for i in range(iter_epochs):\n",
        "        # batch_size 갯수만큼 데이터를 임의로 선택.\n",
        "        batch_indexes = np.random.choice(target.shape[0], batch_size)\n",
        "        rm_batch = rm[batch_indexes]\n",
        "        lstat_batch = lstat[batch_indexes]\n",
        "        target_batch = target[batch_indexes]\n",
        "        # Batch GD 기반으로 Weight/Bias의 Update를 구함.\n",
        "        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n",
        "\n",
        "        # Batch GD로 구한 weight/bias의 update 적용.\n",
        "        w1 = w1 - w1_update\n",
        "        w2 = w2 - w2_update\n",
        "        bias = bias - bias_update\n",
        "        if verbose:\n",
        "            print('Epoch:', i+1,'/', iter_epochs)\n",
        "            # Loss는 전체 학습 데이터 기반으로 구해야 함.\n",
        "            predicted = w1 * rm + w2*lstat + bias\n",
        "            diff = target - predicted\n",
        "            mse_loss = np.mean(np.square(diff))\n",
        "            print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "trusted": true,
        "id": "-K89Zm25V3T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias = batch_random_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "trusted": true,
        "id": "AsBLRsdHV3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_BATCH_RANDOM'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Is1IVkQpV3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행"
      ],
      "metadata": {
        "id": "P3kEEVipV3T7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_step in range(0, 506, 30):\n",
        "    print(batch_step)"
      ],
      "metadata": {
        "trusted": true,
        "id": "I_99j-pXV3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bostonDF['PRICE'].values[480:510]"
      ],
      "metadata": {
        "trusted": true,
        "id": "PV1fglXFV3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음.\n",
        "def batch_gradient_descent(features, target, iter_epochs=1000, batch_size=30, verbose=True):\n",
        "    # w1, w2는 numpy array 연산을 위해 1차원 array로 변환하되 초기 값은 0으로 설정\n",
        "    # bias도 1차원 array로 변환하되 초기 값은 1로 설정.\n",
        "    np.random.seed = 2021\n",
        "    w1 = np.zeros((1,))\n",
        "    w2 = np.zeros((1,))\n",
        "    bias = np.zeros((1, ))\n",
        "    print('최초 w1, w2, bias:', w1, w2, bias)\n",
        "\n",
        "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
        "    learning_rate = 0.01\n",
        "    rm = features[:, 0]\n",
        "    lstat = features[:, 1]\n",
        "\n",
        "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행.\n",
        "    for i in range(iter_epochs):\n",
        "        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n",
        "        for batch_step in range(0, target.shape[0], batch_size):\n",
        "            # batch_size만큼 순차적인 데이터를 가져옴.\n",
        "            rm_batch = rm[batch_step:batch_step + batch_size]\n",
        "            lstat_batch = lstat[batch_step:batch_step + batch_size]\n",
        "            target_batch = target[batch_step:batch_step + batch_size]\n",
        "\n",
        "            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate)\n",
        "\n",
        "            # Batch GD로 구한 weight/bias의 update 적용.\n",
        "            w1 = w1 - w1_update\n",
        "            w2 = w2 - w2_update\n",
        "            bias = bias - bias_update\n",
        "\n",
        "            if verbose:\n",
        "                print('Epoch:', i+1,'/', iter_epochs, 'batch step:', batch_step)\n",
        "                # Loss는 전체 학습 데이터 기반으로 구해야 함.\n",
        "                predicted = w1 * rm + w2*lstat + bias\n",
        "                diff = target - predicted\n",
        "                mse_loss = np.mean(np.square(diff))\n",
        "                print('w1:', w1, 'w2:', w2, 'bias:', bias, 'loss:', mse_loss)\n",
        "\n",
        "    return w1, w2, bias"
      ],
      "metadata": {
        "trusted": true,
        "id": "F7LGIGozV3T7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w1, w2, bias = batch_gradient_descent(scaled_features, bostonDF['PRICE'].values, iter_epochs=5000, batch_size=30, verbose=True)\n",
        "print('##### 최종 w1, w2, bias #######')\n",
        "print(w1, w2, bias)"
      ],
      "metadata": {
        "trusted": true,
        "id": "hG7UqIfcV3T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = scaled_features[:, 0]*w1 + scaled_features[:, 1]*w2 + bias\n",
        "bostonDF['PREDICTED_PRICE_BATCH'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "p8mmGVBGV3T8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mini BATCH GD를 Keras로 수행\n",
        "* Keras는 기본적으로 Mini Batch GD를 수행"
      ],
      "metadata": {
        "id": "AXlPJy8yV3UG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    # 단 하나의 units 설정. input_shape는 2차원, 회귀이므로 activation은 설정하지 않음.\n",
        "    # weight와 bias 초기화는 kernel_inbitializer와 bias_initializer를 이용.\n",
        "    Dense(1, input_shape=(2, ), activation=None, kernel_initializer='zeros', bias_initializer='ones')\n",
        "])\n",
        "# Adam optimizer를 이용하고 Loss 함수는 Mean Squared Error, 성능 측정 역시 MSE를 이용하여 학습 수행.\n",
        "model.compile(optimizer=Adam(learning_rate=0.01), loss='mse', metrics=['mse'])\n",
        "\n",
        "# Keras는 반드시 Batch GD를 적용함. batch_size가 None이면 32를 할당.\n",
        "model.fit(scaled_features, bostonDF['PRICE'].values, batch_size=30, epochs=1000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CycZO0T7V3UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = model.predict(scaled_features)\n",
        "bostonDF['KERAS_PREDICTED_PRICE_BATCH'] = predicted\n",
        "bostonDF.head(10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "JWaEo5QWV3UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jDlJmrfdV3UI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}